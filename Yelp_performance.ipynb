{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-qcbi5fUd42"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEeevkbCM6q9",
        "outputId": "13555628-5b7e-4667-a4bf-d8e619ccfbfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 217 ms, sys: 54.6 ms, total: 272 ms\n",
            "Wall time: 331 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "df1= pd.read_csv(\"/content/sample_data/metadata\",delimiter='\\t',encoding='utf-8',names=['User-id','restaurant-id','rating','label','date'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFt2kxM8XkTM"
      },
      "outputs": [],
      "source": [
        "df_review= pd.read_csv(\"/content/sample_data/reviewContent\",delimiter='\\t',names=['User-id','restaurant-id','date','review'],header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fogUjOJdt3Y",
        "outputId": "739a614d-b356-43b6-a582-0c8ee87b0e9e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(608458, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df_review.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ii5GtdsmXA7p"
      },
      "outputs": [],
      "source": [
        "df_review_merge=pd.merge(df1,df_review,how='inner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "pWRvPelmDzW2",
        "outputId": "2b4a154d-d0ed-4f35-e1f5-57b9b2083104"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   User-id  restaurant-id  rating  label        date  \\\n",
              "0     5044              0     1.0     -1  2014-11-16   \n",
              "1     5045              0     1.0     -1  2014-09-08   \n",
              "2     5046              0     3.0     -1  2013-10-06   \n",
              "3     5047              0     5.0     -1  2014-11-30   \n",
              "4     5048              0     5.0     -1  2014-08-28   \n",
              "\n",
              "                                              review  \n",
              "0  Drinks were bad, the hot chocolate was watered...  \n",
              "1  This was the worst experience I've ever had a ...  \n",
              "2  This is located on the site of the old Spruce ...  \n",
              "3  I enjoyed coffee and breakfast twice at Toast ...  \n",
              "4  I love Toast! The food choices are fantastic -...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e2330e24-cfde-4ec6-95ba-349793c2c053\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User-id</th>\n",
              "      <th>restaurant-id</th>\n",
              "      <th>rating</th>\n",
              "      <th>label</th>\n",
              "      <th>date</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5044</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>2014-11-16</td>\n",
              "      <td>Drinks were bad, the hot chocolate was watered...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5045</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>2014-09-08</td>\n",
              "      <td>This was the worst experience I've ever had a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5046</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>2013-10-06</td>\n",
              "      <td>This is located on the site of the old Spruce ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5047</td>\n",
              "      <td>0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>2014-11-30</td>\n",
              "      <td>I enjoyed coffee and breakfast twice at Toast ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5048</td>\n",
              "      <td>0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>2014-08-28</td>\n",
              "      <td>I love Toast! The food choices are fantastic -...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e2330e24-cfde-4ec6-95ba-349793c2c053')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e2330e24-cfde-4ec6-95ba-349793c2c053 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e2330e24-cfde-4ec6-95ba-349793c2c053');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df_review_merge.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_review_merge['review'] = [i.replace(\"&amp;amp;\", '').replace(\"\\'\",'') for i in df_review_merge['review']]"
      ],
      "metadata": {
        "id": "iWpZlmCAsvF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67-8mEUJXhvo",
        "outputId": "a67ae53d-4a7b-406b-ec2e-e724f2986f21"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(608458, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "df_review_merge.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wI0cjJjjfxD6",
        "outputId": "942cd0f7-3d89-439e-a4c8-2a1662c23758"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 1    528019\n",
              "-1     80439\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df_review_merge.label.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yx3kvU-rrRBo"
      },
      "outputs": [],
      "source": [
        "df_review_merge['label']=df_review_merge['label'].replace(-1,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "84kNINwJD3hR",
        "outputId": "9cf28cb9-5932-4d64-92d9-eb704277eb0f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   User-id  restaurant-id  rating  label        date  \\\n",
              "0     5044              0     1.0      0  2014-11-16   \n",
              "1     5045              0     1.0      0  2014-09-08   \n",
              "2     5046              0     3.0      0  2013-10-06   \n",
              "3     5047              0     5.0      0  2014-11-30   \n",
              "4     5048              0     5.0      0  2014-08-28   \n",
              "\n",
              "                                              review  \n",
              "0  Drinks were bad, the hot chocolate was watered...  \n",
              "1  This was the worst experience Ive ever had a c...  \n",
              "2  This is located on the site of the old Spruce ...  \n",
              "3  I enjoyed coffee and breakfast twice at Toast ...  \n",
              "4  I love Toast! The food choices are fantastic -...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0ad49d04-4252-4879-b35f-783bf2313dea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User-id</th>\n",
              "      <th>restaurant-id</th>\n",
              "      <th>rating</th>\n",
              "      <th>label</th>\n",
              "      <th>date</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5044</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2014-11-16</td>\n",
              "      <td>Drinks were bad, the hot chocolate was watered...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5045</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2014-09-08</td>\n",
              "      <td>This was the worst experience Ive ever had a c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5046</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2013-10-06</td>\n",
              "      <td>This is located on the site of the old Spruce ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5047</td>\n",
              "      <td>0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2014-11-30</td>\n",
              "      <td>I enjoyed coffee and breakfast twice at Toast ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5048</td>\n",
              "      <td>0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2014-08-28</td>\n",
              "      <td>I love Toast! The food choices are fantastic -...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0ad49d04-4252-4879-b35f-783bf2313dea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0ad49d04-4252-4879-b35f-783bf2313dea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0ad49d04-4252-4879-b35f-783bf2313dea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df_review_merge.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXizd8W7ixiM",
        "outputId": "7762741c-c922-4277-9a87-5c13cb31b618"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#removing character references amp that is not useful for the model\n",
        "df_review_merge['review'] = [i.replace(\"&amp;amp;\", '').replace(\"\\'\",'') for i in df_review_merge['review']]"
      ],
      "metadata": {
        "id": "moxBPJaW2jO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata"
      ],
      "metadata": {
        "id": "sRhp8i1d4l38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjLl8CmCTlAM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6237f28f-deca-4008-908d-9d3496c31d12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4min 51s, sys: 1.37 s, total: 4min 53s\n",
            "Wall time: 4min 52s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "def clean_text(txt):\n",
        "    \"\"\"\"\"\n",
        "    cleans the input text in the following steps\n",
        "    1- replace contractions\n",
        "    2- removing punctuation\n",
        "    3- spliting into words\n",
        "    4- removing stopwords\n",
        "    5- removing leftover punctuations\n",
        "    \"\"\"\"\"\n",
        "    contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "    def _get_contractions(contraction_dict):\n",
        "        contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
        "        return contraction_dict, contraction_re\n",
        "\n",
        "    def replace_contractions(text):\n",
        "        contractions, contractions_re = _get_contractions(contraction_dict)\n",
        "        def replace(match):\n",
        "            return contractions[match.group(0)]\n",
        "        return contractions_re.sub(replace, text)\n",
        "\n",
        "    # replace contractions and convert to lowercase\n",
        "    txt = txt.lower()\n",
        "    #txt = replace_contractions(txt)\n",
        "    \n",
        "    #remove punctuations\n",
        "    txt  = \"\".join([char for char in txt if char not in string.punctuation])\n",
        "    txt = re.sub('[0-9]+', '', txt)\n",
        "    txt= re.sub(r\"([.,!?])\", r\" \\1 \",txt)\n",
        "    txt = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", txt)\n",
        "    txt = re.sub(r'[^\\x00-\\x7f]',r'', txt)#remove non ascii\n",
        "    txt = unicodedata.normalize('NFKD', txt).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    txt = re.sub(' +', ' ', txt).strip() #remove leading and trailing white spaces\n",
        "    re.sub(\"(.)\\\\1{2,}\", \"\\\\1\",txt) #removing duplicate values like youuuuu to you\n",
        "  \n",
        "    \n",
        "    # split into words\n",
        "    words = word_tokenize(txt)\n",
        "    \n",
        "    # remove stopwords\n",
        "    #stop_words = set(stopwords.words('english'))\n",
        "   # words = [w for w in words if not w in stop_words]\n",
        "    \n",
        "    # removing leftover punctuations\n",
        "   # words = [word for word in words if word.isalpha()]\n",
        "\n",
        "   # lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "  #  def lemmatize_text(txt):\n",
        "  #      return [lemmatizer.lemmatize(w) for w in words]\n",
        "\n",
        "    cleaned_text = ' '.join(words)\n",
        "    return cleaned_text\n",
        "    \n",
        "df_review_merge['review'] = df_review_merge['review'].apply(lambda txt: clean_text(txt))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnEvINiDDgfD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "36d3bf1e-ee78-48a7-cc43-01fa8d342523"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'drinks were bad the hot chocolate was watered down and the latte had a burnt taste to it the food was also poor quality but the service was the worst part their cashier was very rude'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "df_review_merge['review'].iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3K-n8LScYCc",
        "outputId": "6f5cb9dd-433d-4abe-f4d6-787ac7a2bde1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    528019\n",
              "0     80439\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "df_review_merge.label.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWf5TBSKXz6z"
      },
      "outputs": [],
      "source": [
        "sentences=df_review_merge.review.values\n",
        "target=df_review_merge.label.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9dxoP0tzYgS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "257d6c43-1bb5-4f7a-8353-c4235be42cbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUXn5wkP0KiH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae03a829-bde3-48a2-fe3d-301048486844"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.2"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "1.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEiLuSHJYBGM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59722905-6feb-46c8-d73b-619ffd26a395"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers==3.0.0 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (21.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (0.0.53)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (0.8.0rc4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (3.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (4.64.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (0.1.97)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.0) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==3.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSSvqX0QYLRH"
      },
      "outputs": [],
      "source": [
        "#!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_LKzDsEkCK-",
        "outputId": "2da257b5-ca11-4124-ae6e-0c44cae3a673"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.97)\n"
          ]
        }
      ],
      "source": [
        "!pip install Sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYmqqbSRrV_S"
      },
      "outputs": [],
      "source": [
        "#!pip install -q torch==1.4.0 -f https://download.pytorch.org/whl/cu101/torch_stable.html\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaConfig,AutoConfig\n",
        "\n",
        "configuration = RobertaConfig.from_pretrained('roberta-base',output_attentions=False,output_hidden_states=False,num_labels=2)\n",
        "configuration.hidden_dropout_prob = 0.2\n",
        "configuration.attention_probs_dropout_prob = 0.1\n"
      ],
      "metadata": {
        "id": "BLnLYIznl6Si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-B0OUEA4jxWg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a28e5cf8-70be-470b-920c-1c8df08c9b06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "WARNING:transformers.modeling_utils:Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "#Loading pre trained models\n",
        "from transformers import (\n",
        "    BertForSequenceClassification,\n",
        "#     TFBertForSequenceClassification, \n",
        "                          BertTokenizer,\n",
        "#                           TFRobertaForSequenceClassification,\n",
        "                          RobertaForSequenceClassification,\n",
        "                          RobertaTokenizer,\n",
        "                          XLNetForSequenceClassification,\n",
        "                          XLNetTokenizer,\n",
        "                         AdamW)\n",
        "roberta_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", # 12-layer, 768-hidden, 12-heads, 125M parameters RoBERTa using the BERT-base architecture\n",
        "                                                                  # num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                                                                                    # You can increase this for multi-class tasks.   \n",
        "                                                                 #  output_attentions = False, # Whether the model returns attentions weights.\n",
        "                                                                 # output_hidden_states = False, #, # Whether the model returns all hidden-states.\n",
        "                                                                   config =configuration\n",
        "                                                                )\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "roberta_model.cuda()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aujKyQnlk4gF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4c030df-63a1-4f74-8824-8f352ff02ced"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Original:  drinks were bad the hot chocolate was watered down and the latte had a burnt taste to it the food was also poor quality but the service was the worst part their cashier was very rude\n",
            "Tokenized RoBERT:  ['dr', 'inks', 'were', 'bad', 'the', 'hot', 'chocolate', 'was', 'watered', 'down', 'and', 'the', 'lat', 'te', 'had', 'a', 'burnt', 'taste', 'to', 'it', 'the', 'food', 'was', 'also', 'poor', 'quality', 'but', 'the', 'service', 'was', 'the', 'worst', 'part', 'their', 'cash', 'ier', 'was', 'very', 'rude']\n",
            "Token IDs RoBERTa:  [10232, 12935, 58, 1099, 5, 2131, 7548, 21, 36408, 159, 8, 5, 16619, 859, 56, 10, 18698, 5840, 7, 24, 5, 689, 21, 67, 2129, 1318, 53, 5, 544, 21, 5, 2373, 233, 49, 1055, 906, 21, 182, 21820]\n"
          ]
        }
      ],
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the text split into tokens.\n",
        "print('Tokenized RoBERT: ', roberta_tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the text mapped to token ids.\n",
        "print('Token IDs RoBERTa: ', roberta_tokenizer.convert_tokens_to_ids(roberta_tokenizer.tokenize(sentences[0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbzP1QUuWUMi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5fc5668-795e-464d-aa6c-b549ecf168cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RoBERTa: ['Will', 'Your', 'H', 'ometown', 'Be', 'Taking', 'In', 'Obama', '', '', 's', 'Refugees', '?', 'Here', '', '', 's', 'The', 'List', 'Of', 'Cities', 'Where', 'They', '', '', 're', 'Being', 'Trans', 'pl', 'anted', '', '', '', '']\n"
          ]
        }
      ],
      "source": [
        "sequence = \"\"\"Will Your Hometown Be Taking In Obamas Refugees? Heres The List Of Cities Where Theyre Being Transplanted \"\"\"\n",
        "roberta_tokenized_sequence = roberta_tokenizer.tokenize(sequence)\n",
        "print(\"RoBERTa:\", roberta_tokenized_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0b0Cs2zaWjfb"
      },
      "outputs": [],
      "source": [
        "token_lens = []\n",
        "\n",
        "for txt in sentences:\n",
        "    tokens = roberta_tokenizer.encode(txt,truncation=True, max_length=512)\n",
        "    token_lens.append(len(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "aCbxpk7ojP0j",
        "outputId": "f0a536a0-8eff-443f-82ea-bb93f4160e98"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.histplot(token_lens)\n",
        "plt.xlim([0, 600]);\n",
        "plt.xlabel('Token count');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZamXIfgk-Du",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46d5f940-650c-4b1c-bd09-e25a338cdcb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  drinks were bad the hot chocolate was watered down and the latte had a burnt taste to it the food was also poor quality but the service was the worst part their cashier was very rude\n",
            "Token IDs: tensor([    0, 10232, 12935,    58,  1099,     5,  2131,  7548,    21, 36408,\n",
            "          159,     8,     5, 16619,   859,    56,    10, 18698,  5840,     7,\n",
            "           24,     5,   689,    21,    67,  2129,  1318,    53,     5,   544,\n",
            "           21,     5,  2373,   233,    49,  1055,   906,    21,   182, 21820,\n",
            "            2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1])\n",
            "labels: tensor([0, 0, 0,  ..., 1, 1, 1])\n"
          ]
        }
      ],
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "roberta_input_ids = []\n",
        "roberta_attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    roberta_encoded_dict = roberta_tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 256,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',\n",
        "                        truncation=True    # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    roberta_input_ids.append(roberta_encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    roberta_attention_masks.append(roberta_encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "roberta_input_ids = torch.cat(roberta_input_ids, dim=0)\n",
        "roberta_attention_masks = torch.cat(roberta_attention_masks, dim=0)\n",
        "labels = torch.tensor(df_review_merge.label.values)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', roberta_input_ids[0])\n",
        "print('labels:', labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWmN6cjTyePb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cd5b841-1a9b-4ed5-ca7f-ae7ee3e07226"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "486,766 training samples\n",
            "121,692 validation samples\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(roberta_input_ids, roberta_attention_masks, labels)\n",
        "\n",
        "# Create a 80-20 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtETllfvyjmy",
        "outputId": "095b0757-2888-483f-9140-61da1b03274d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "486766"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNe3YowrynHY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0928ef8-b35e-444d-da9c-a2ef3233698b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({1: 422485, 0: 64281})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "import collections as c\n",
        "train_classes = [df_review_merge.label[i] for i in train_dataset.indices]\n",
        "c.Counter(train_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0bdj-ntyril",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fe803a5-ebcf-43e9-e694-b67f62507a80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 64281 422485]\n",
            "[1.55566964e-05 2.36694794e-06]\n",
            "tensor([2.3669e-06, 2.3669e-06, 2.3669e-06,  ..., 2.3669e-06, 2.3669e-06,\n",
            "        2.3669e-06], dtype=torch.float64)\n",
            "227551\n",
            "<torch.utils.data.sampler.WeightedRandomSampler object at 0x7f309e47e2d0>\n"
          ]
        }
      ],
      "source": [
        "#Using weighted random sampler for class imbalance\n",
        "import numpy as np \n",
        "\n",
        "y_train_indices = train_dataset.indices\n",
        "\n",
        "y_train = [target[i] for i in y_train_indices]\n",
        "\n",
        "class_sample_count = np.array(\n",
        "    [len(np.where(y_train == t)[0]) for t in np.unique(y_train)])\n",
        "print(class_sample_count)\n",
        "weight = 1. / (class_sample_count)\n",
        "print(weight)\n",
        "samples_weight = np.array([weight[t] for t in y_train])\n",
        "samples_weight = torch.from_numpy(samples_weight)\n",
        "print(samples_weight)\n",
        "sampler = torch.utils.data.sampler.WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
        "print(next(iter(sampler)))\n",
        "print(sampler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJcpIWH1WJOW"
      },
      "outputs": [],
      "source": [
        "labels_unique,counts=np.unique(y_train,return_counts=True)\n",
        "counts\n",
        "class_weights=[sum(counts)/c for c in counts]\n",
        "class_weights\n",
        "example_weights=[class_weights[e] for e in y_train]\n",
        "#len(torch.example_weights)\n",
        "#sampler = torch.utils.data.sampler.WeightedRandomSampler(example_weights.type('torch.DoubleTensor'), len(example_weights))\n",
        "#sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wKaZFPZ8Yix",
        "outputId": "90d9ce30-e297-4a59-db79-620b822ecd6a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "np.unique(df_review_merge.label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9qKHsARntz7"
      },
      "outputs": [],
      "source": [
        "#Run if we are using weighted random sampler\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32 or 64.\n",
        "batch_size = 64\n",
        "gradient_accumulations=10\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = sampler, # Select batches randomly\n",
        "            batch_size = batch_size, # Trains with this batch size.\n",
        "            num_workers=2,\n",
        "            pin_memory = True\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size, # Evaluate with this batch size.\n",
        "            num_workers =2,\n",
        "            pin_memory = True\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpqN1Rsdi6cL"
      },
      "outputs": [],
      "source": [
        "#Run if we are using only random sampler for train data \n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32 or 64.\n",
        "batch_size = 64\n",
        "gradient_accumulations=10\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size, # Trains with this batch size.\n",
        "            num_workers=2,\n",
        "            pin_memory = True\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size, # Evaluate with this batch size.\n",
        "            num_workers =2,\n",
        "            pin_memory = True\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r35PDVsBdAN7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69467d00-81c3-415d-c689-962dbb009453"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels batch shape: <built-in method values of Tensor object at 0x7f2f826bc290>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([33, 31])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "#Testing the label counts in training data after applying weighted random sampler\n",
        "import collections as c\n",
        "train_labels = next(iter(train_dataloader))\n",
        "#print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels[0].values}\")\n",
        "#img = train_features[0].squeeze()\n",
        "label = train_labels[2]\n",
        "torch.bincount(label)\n",
        "#print(label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84LiNyT63LxF",
        "outputId": "8d9f6b62-499d-4b33-d092-9a78b9317bc9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7606"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "len(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDw6uSrMw6eI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b3c9207-98a4-4054-87da-c1bd180eceb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(422485.)\n",
            "tensor(64281.)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1521)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "#2nd method using pos weight for imbalance subsitute in loss function\n",
        "y_train=torch.Tensor(y_train)\n",
        "num_positives = torch.sum(y_train, dim=0)\n",
        "print(num_positives)\n",
        "num_negatives = len(train_dataset) - num_positives\n",
        "print(num_negatives)\n",
        "pos_weight  = num_negatives / num_positives\n",
        "pos_weight\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGMurnwan6GB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb2ff4e7-3a3e-4038-bb7b-365fa501d07b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Roberta model has 203 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "roberta.embeddings.word_embeddings.weight               (50265, 768)\n",
            "roberta.embeddings.position_embeddings.weight             (514, 768)\n",
            "roberta.embeddings.token_type_embeddings.weight             (1, 768)\n",
            "roberta.embeddings.LayerNorm.weight                           (768,)\n",
            "roberta.embeddings.LayerNorm.bias                             (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "roberta.encoder.layer.0.attention.self.query.weight       (768, 768)\n",
            "roberta.encoder.layer.0.attention.self.query.bias             (768,)\n",
            "roberta.encoder.layer.0.attention.self.key.weight         (768, 768)\n",
            "roberta.encoder.layer.0.attention.self.key.bias               (768,)\n",
            "roberta.encoder.layer.0.attention.self.value.weight       (768, 768)\n",
            "roberta.encoder.layer.0.attention.self.value.bias             (768,)\n",
            "roberta.encoder.layer.0.attention.output.dense.weight     (768, 768)\n",
            "roberta.encoder.layer.0.attention.output.dense.bias           (768,)\n",
            "roberta.encoder.layer.0.attention.output.LayerNorm.weight       (768,)\n",
            "roberta.encoder.layer.0.attention.output.LayerNorm.bias       (768,)\n",
            "roberta.encoder.layer.0.intermediate.dense.weight        (3072, 768)\n",
            "roberta.encoder.layer.0.intermediate.dense.bias              (3072,)\n",
            "roberta.encoder.layer.0.output.dense.weight              (768, 3072)\n",
            "roberta.encoder.layer.0.output.dense.bias                     (768,)\n",
            "roberta.encoder.layer.0.output.LayerNorm.weight               (768,)\n",
            "roberta.encoder.layer.0.output.LayerNorm.bias                 (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "classifier.dense.weight                                   (768, 768)\n",
            "classifier.dense.bias                                         (768,)\n",
            "classifier.out_proj.weight                                  (2, 768)\n",
            "classifier.out_proj.bias                                        (2,)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(roberta_model.named_parameters())\n",
        "\n",
        "print('The Roberta model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5U8SUYo5n9XY"
      },
      "outputs": [],
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in roberta_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in roberta_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5, eps = 1e-8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XR50r-utoALX"
      },
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "epochs = 3\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbeXUTPooC5s"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txhcOtv3RTyp"
      },
      "outputs": [],
      "source": [
        "from torch.cuda.amp import GradScaler, autocast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2u9XFEsoGnB"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHC5I3hdaAF4"
      },
      "outputs": [],
      "source": [
        "#Loss function with weight\n",
        "def loss_fn1(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)(outputs, targets.float())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcgOGvJIdad7"
      },
      "outputs": [],
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets.float())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCWlp9jZoL5v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2258f28c-6631-4f56-8b45-fe5e524318c6"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of  7,606.    Elapsed: 0:00:58.\n",
            "  Batch    80  of  7,606.    Elapsed: 0:01:55.\n",
            "  Batch   120  of  7,606.    Elapsed: 0:02:52.\n",
            "  Batch   160  of  7,606.    Elapsed: 0:03:49.\n",
            "  Batch   200  of  7,606.    Elapsed: 0:04:46.\n",
            "  Batch   240  of  7,606.    Elapsed: 0:05:43.\n",
            "  Batch   280  of  7,606.    Elapsed: 0:06:40.\n",
            "  Batch   320  of  7,606.    Elapsed: 0:07:37.\n",
            "  Batch   360  of  7,606.    Elapsed: 0:08:34.\n",
            "  Batch   400  of  7,606.    Elapsed: 0:09:31.\n",
            "  Batch   440  of  7,606.    Elapsed: 0:10:28.\n",
            "  Batch   480  of  7,606.    Elapsed: 0:11:25.\n",
            "  Batch   520  of  7,606.    Elapsed: 0:12:22.\n",
            "  Batch   560  of  7,606.    Elapsed: 0:13:19.\n",
            "  Batch   600  of  7,606.    Elapsed: 0:14:16.\n",
            "  Batch   640  of  7,606.    Elapsed: 0:15:13.\n",
            "  Batch   680  of  7,606.    Elapsed: 0:16:10.\n",
            "  Batch   720  of  7,606.    Elapsed: 0:17:07.\n",
            "  Batch   760  of  7,606.    Elapsed: 0:18:04.\n",
            "  Batch   800  of  7,606.    Elapsed: 0:19:01.\n",
            "  Batch   840  of  7,606.    Elapsed: 0:19:58.\n",
            "  Batch   880  of  7,606.    Elapsed: 0:20:55.\n",
            "  Batch   920  of  7,606.    Elapsed: 0:21:52.\n",
            "  Batch   960  of  7,606.    Elapsed: 0:22:49.\n",
            "  Batch 1,000  of  7,606.    Elapsed: 0:23:46.\n",
            "  Batch 1,040  of  7,606.    Elapsed: 0:24:43.\n",
            "  Batch 1,080  of  7,606.    Elapsed: 0:25:40.\n",
            "  Batch 1,120  of  7,606.    Elapsed: 0:26:37.\n",
            "  Batch 1,160  of  7,606.    Elapsed: 0:27:35.\n",
            "  Batch 1,200  of  7,606.    Elapsed: 0:28:32.\n",
            "  Batch 1,240  of  7,606.    Elapsed: 0:29:29.\n",
            "  Batch 1,280  of  7,606.    Elapsed: 0:30:26.\n",
            "  Batch 1,320  of  7,606.    Elapsed: 0:31:23.\n",
            "  Batch 1,360  of  7,606.    Elapsed: 0:32:20.\n",
            "  Batch 1,400  of  7,606.    Elapsed: 0:33:17.\n",
            "  Batch 1,440  of  7,606.    Elapsed: 0:34:14.\n",
            "  Batch 1,480  of  7,606.    Elapsed: 0:35:11.\n",
            "  Batch 1,520  of  7,606.    Elapsed: 0:36:08.\n",
            "  Batch 1,560  of  7,606.    Elapsed: 0:37:05.\n",
            "  Batch 1,600  of  7,606.    Elapsed: 0:38:02.\n",
            "  Batch 1,640  of  7,606.    Elapsed: 0:38:59.\n",
            "  Batch 1,680  of  7,606.    Elapsed: 0:39:56.\n",
            "  Batch 1,720  of  7,606.    Elapsed: 0:40:53.\n",
            "  Batch 1,760  of  7,606.    Elapsed: 0:41:50.\n",
            "  Batch 1,800  of  7,606.    Elapsed: 0:42:47.\n",
            "  Batch 1,840  of  7,606.    Elapsed: 0:43:44.\n",
            "  Batch 1,880  of  7,606.    Elapsed: 0:44:41.\n",
            "  Batch 1,920  of  7,606.    Elapsed: 0:45:38.\n",
            "  Batch 1,960  of  7,606.    Elapsed: 0:46:35.\n",
            "  Batch 2,000  of  7,606.    Elapsed: 0:47:32.\n",
            "  Batch 2,040  of  7,606.    Elapsed: 0:48:29.\n",
            "  Batch 2,080  of  7,606.    Elapsed: 0:49:26.\n",
            "  Batch 2,120  of  7,606.    Elapsed: 0:50:23.\n",
            "  Batch 2,160  of  7,606.    Elapsed: 0:51:20.\n",
            "  Batch 2,200  of  7,606.    Elapsed: 0:52:17.\n",
            "  Batch 2,240  of  7,606.    Elapsed: 0:53:14.\n",
            "  Batch 2,280  of  7,606.    Elapsed: 0:54:11.\n",
            "  Batch 2,320  of  7,606.    Elapsed: 0:55:08.\n",
            "  Batch 2,360  of  7,606.    Elapsed: 0:56:05.\n",
            "  Batch 2,400  of  7,606.    Elapsed: 0:57:02.\n",
            "  Batch 2,440  of  7,606.    Elapsed: 0:57:59.\n",
            "  Batch 2,480  of  7,606.    Elapsed: 0:58:56.\n",
            "  Batch 2,520  of  7,606.    Elapsed: 0:59:53.\n",
            "  Batch 2,560  of  7,606.    Elapsed: 1:00:50.\n",
            "  Batch 2,600  of  7,606.    Elapsed: 1:01:47.\n",
            "  Batch 2,640  of  7,606.    Elapsed: 1:02:44.\n",
            "  Batch 2,680  of  7,606.    Elapsed: 1:03:41.\n",
            "  Batch 2,720  of  7,606.    Elapsed: 1:04:38.\n",
            "  Batch 2,760  of  7,606.    Elapsed: 1:05:35.\n",
            "  Batch 2,800  of  7,606.    Elapsed: 1:06:33.\n",
            "  Batch 2,840  of  7,606.    Elapsed: 1:07:30.\n",
            "  Batch 2,880  of  7,606.    Elapsed: 1:08:27.\n",
            "  Batch 2,920  of  7,606.    Elapsed: 1:09:24.\n",
            "  Batch 2,960  of  7,606.    Elapsed: 1:10:21.\n",
            "  Batch 3,000  of  7,606.    Elapsed: 1:11:18.\n",
            "  Batch 3,040  of  7,606.    Elapsed: 1:12:15.\n",
            "  Batch 3,080  of  7,606.    Elapsed: 1:13:12.\n",
            "  Batch 3,120  of  7,606.    Elapsed: 1:14:09.\n",
            "  Batch 3,160  of  7,606.    Elapsed: 1:15:06.\n",
            "  Batch 3,200  of  7,606.    Elapsed: 1:16:03.\n",
            "  Batch 3,240  of  7,606.    Elapsed: 1:17:00.\n",
            "  Batch 3,280  of  7,606.    Elapsed: 1:17:57.\n",
            "  Batch 3,320  of  7,606.    Elapsed: 1:18:54.\n",
            "  Batch 3,360  of  7,606.    Elapsed: 1:19:51.\n",
            "  Batch 3,400  of  7,606.    Elapsed: 1:20:48.\n",
            "  Batch 3,440  of  7,606.    Elapsed: 1:21:45.\n",
            "  Batch 3,480  of  7,606.    Elapsed: 1:22:42.\n",
            "  Batch 3,520  of  7,606.    Elapsed: 1:23:39.\n",
            "  Batch 3,560  of  7,606.    Elapsed: 1:24:36.\n",
            "  Batch 3,600  of  7,606.    Elapsed: 1:25:33.\n",
            "  Batch 3,640  of  7,606.    Elapsed: 1:26:30.\n",
            "  Batch 3,680  of  7,606.    Elapsed: 1:27:27.\n",
            "  Batch 3,720  of  7,606.    Elapsed: 1:28:24.\n",
            "  Batch 3,760  of  7,606.    Elapsed: 1:29:21.\n",
            "  Batch 3,800  of  7,606.    Elapsed: 1:30:18.\n",
            "  Batch 3,840  of  7,606.    Elapsed: 1:31:15.\n",
            "  Batch 3,880  of  7,606.    Elapsed: 1:32:12.\n",
            "  Batch 3,920  of  7,606.    Elapsed: 1:33:09.\n",
            "  Batch 3,960  of  7,606.    Elapsed: 1:34:06.\n",
            "  Batch 4,000  of  7,606.    Elapsed: 1:35:03.\n",
            "  Batch 4,040  of  7,606.    Elapsed: 1:36:00.\n",
            "  Batch 4,080  of  7,606.    Elapsed: 1:36:57.\n",
            "  Batch 4,120  of  7,606.    Elapsed: 1:37:54.\n",
            "  Batch 4,160  of  7,606.    Elapsed: 1:38:51.\n",
            "  Batch 4,200  of  7,606.    Elapsed: 1:39:48.\n",
            "  Batch 4,240  of  7,606.    Elapsed: 1:40:45.\n",
            "  Batch 4,280  of  7,606.    Elapsed: 1:41:42.\n",
            "  Batch 4,320  of  7,606.    Elapsed: 1:42:40.\n",
            "  Batch 4,360  of  7,606.    Elapsed: 1:43:37.\n",
            "  Batch 4,400  of  7,606.    Elapsed: 1:44:34.\n",
            "  Batch 4,440  of  7,606.    Elapsed: 1:45:31.\n",
            "  Batch 4,480  of  7,606.    Elapsed: 1:46:28.\n",
            "  Batch 4,520  of  7,606.    Elapsed: 1:47:25.\n",
            "  Batch 4,560  of  7,606.    Elapsed: 1:48:22.\n",
            "  Batch 4,600  of  7,606.    Elapsed: 1:49:19.\n",
            "  Batch 4,640  of  7,606.    Elapsed: 1:50:16.\n",
            "  Batch 4,680  of  7,606.    Elapsed: 1:51:13.\n",
            "  Batch 4,720  of  7,606.    Elapsed: 1:52:10.\n",
            "  Batch 4,760  of  7,606.    Elapsed: 1:53:07.\n",
            "  Batch 4,800  of  7,606.    Elapsed: 1:54:04.\n",
            "  Batch 4,840  of  7,606.    Elapsed: 1:55:01.\n",
            "  Batch 4,880  of  7,606.    Elapsed: 1:55:58.\n",
            "  Batch 4,920  of  7,606.    Elapsed: 1:56:55.\n",
            "  Batch 4,960  of  7,606.    Elapsed: 1:57:52.\n",
            "  Batch 5,000  of  7,606.    Elapsed: 1:58:49.\n",
            "  Batch 5,040  of  7,606.    Elapsed: 1:59:46.\n",
            "  Batch 5,080  of  7,606.    Elapsed: 2:00:43.\n",
            "  Batch 5,120  of  7,606.    Elapsed: 2:01:40.\n",
            "  Batch 5,160  of  7,606.    Elapsed: 2:02:37.\n",
            "  Batch 5,200  of  7,606.    Elapsed: 2:03:34.\n",
            "  Batch 5,240  of  7,606.    Elapsed: 2:04:31.\n",
            "  Batch 5,280  of  7,606.    Elapsed: 2:05:28.\n",
            "  Batch 5,320  of  7,606.    Elapsed: 2:06:25.\n",
            "  Batch 5,360  of  7,606.    Elapsed: 2:07:22.\n",
            "  Batch 5,400  of  7,606.    Elapsed: 2:08:19.\n",
            "  Batch 5,440  of  7,606.    Elapsed: 2:09:16.\n",
            "  Batch 5,480  of  7,606.    Elapsed: 2:10:13.\n",
            "  Batch 5,520  of  7,606.    Elapsed: 2:11:10.\n",
            "  Batch 5,560  of  7,606.    Elapsed: 2:12:08.\n",
            "  Batch 5,600  of  7,606.    Elapsed: 2:13:05.\n",
            "  Batch 5,640  of  7,606.    Elapsed: 2:14:02.\n",
            "  Batch 5,680  of  7,606.    Elapsed: 2:14:59.\n",
            "  Batch 5,720  of  7,606.    Elapsed: 2:15:56.\n",
            "  Batch 5,760  of  7,606.    Elapsed: 2:16:53.\n",
            "  Batch 5,800  of  7,606.    Elapsed: 2:17:50.\n",
            "  Batch 5,840  of  7,606.    Elapsed: 2:18:47.\n",
            "  Batch 5,880  of  7,606.    Elapsed: 2:19:44.\n",
            "  Batch 5,920  of  7,606.    Elapsed: 2:20:41.\n",
            "  Batch 5,960  of  7,606.    Elapsed: 2:21:38.\n",
            "  Batch 6,000  of  7,606.    Elapsed: 2:22:35.\n",
            "  Batch 6,040  of  7,606.    Elapsed: 2:23:32.\n",
            "  Batch 6,080  of  7,606.    Elapsed: 2:24:29.\n",
            "  Batch 6,120  of  7,606.    Elapsed: 2:25:26.\n",
            "  Batch 6,160  of  7,606.    Elapsed: 2:26:23.\n",
            "  Batch 6,200  of  7,606.    Elapsed: 2:27:20.\n",
            "  Batch 6,240  of  7,606.    Elapsed: 2:28:17.\n",
            "  Batch 6,280  of  7,606.    Elapsed: 2:29:14.\n",
            "  Batch 6,320  of  7,606.    Elapsed: 2:30:11.\n",
            "  Batch 6,360  of  7,606.    Elapsed: 2:31:08.\n",
            "  Batch 6,400  of  7,606.    Elapsed: 2:32:05.\n",
            "  Batch 6,440  of  7,606.    Elapsed: 2:33:02.\n",
            "  Batch 6,480  of  7,606.    Elapsed: 2:33:59.\n",
            "  Batch 6,520  of  7,606.    Elapsed: 2:34:56.\n",
            "  Batch 6,560  of  7,606.    Elapsed: 2:35:53.\n",
            "  Batch 6,600  of  7,606.    Elapsed: 2:36:50.\n",
            "  Batch 6,640  of  7,606.    Elapsed: 2:37:47.\n",
            "  Batch 6,680  of  7,606.    Elapsed: 2:38:44.\n",
            "  Batch 6,720  of  7,606.    Elapsed: 2:39:41.\n",
            "  Batch 6,760  of  7,606.    Elapsed: 2:40:39.\n",
            "  Batch 6,800  of  7,606.    Elapsed: 2:41:36.\n",
            "  Batch 6,840  of  7,606.    Elapsed: 2:42:33.\n",
            "  Batch 6,880  of  7,606.    Elapsed: 2:43:30.\n",
            "  Batch 6,920  of  7,606.    Elapsed: 2:44:27.\n",
            "  Batch 6,960  of  7,606.    Elapsed: 2:45:24.\n",
            "  Batch 7,000  of  7,606.    Elapsed: 2:46:21.\n",
            "  Batch 7,040  of  7,606.    Elapsed: 2:47:18.\n",
            "  Batch 7,080  of  7,606.    Elapsed: 2:48:15.\n",
            "  Batch 7,120  of  7,606.    Elapsed: 2:49:12.\n",
            "  Batch 7,160  of  7,606.    Elapsed: 2:50:09.\n",
            "  Batch 7,200  of  7,606.    Elapsed: 2:51:06.\n",
            "  Batch 7,240  of  7,606.    Elapsed: 2:52:03.\n",
            "  Batch 7,280  of  7,606.    Elapsed: 2:53:00.\n",
            "  Batch 7,320  of  7,606.    Elapsed: 2:53:57.\n",
            "  Batch 7,360  of  7,606.    Elapsed: 2:54:54.\n",
            "  Batch 7,400  of  7,606.    Elapsed: 2:55:51.\n",
            "  Batch 7,440  of  7,606.    Elapsed: 2:56:48.\n",
            "  Batch 7,480  of  7,606.    Elapsed: 2:57:45.\n",
            "  Batch 7,520  of  7,606.    Elapsed: 2:58:42.\n",
            "  Batch 7,560  of  7,606.    Elapsed: 2:59:39.\n",
            "  Batch 7,600  of  7,606.    Elapsed: 3:00:36.\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 3:00:44\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.57\n",
            "  F1 score: 0.51\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.82      0.33     16158\n",
            "           1       0.95      0.52      0.68    105534\n",
            "\n",
            "    accuracy                           0.56    121692\n",
            "   macro avg       0.58      0.67      0.50    121692\n",
            "weighted avg       0.85      0.56      0.63    121692\n",
            "\n",
            "[[13329  2829]\n",
            " [50251 55283]]\n",
            "Mathews Correlation coefficient score for this epoch is : 0.24\n",
            "Model validation score: f1=0.676 auc=0.948\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Validation Loss: 0.70\n",
            "  Validation took: 0:15:47\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of  7,606.    Elapsed: 0:00:57.\n",
            "  Batch    80  of  7,606.    Elapsed: 0:01:54.\n",
            "  Batch   120  of  7,606.    Elapsed: 0:02:51.\n",
            "  Batch   160  of  7,606.    Elapsed: 0:03:48.\n",
            "  Batch   200  of  7,606.    Elapsed: 0:04:45.\n",
            "  Batch   240  of  7,606.    Elapsed: 0:05:43.\n",
            "  Batch   280  of  7,606.    Elapsed: 0:06:40.\n",
            "  Batch   320  of  7,606.    Elapsed: 0:07:37.\n",
            "  Batch   360  of  7,606.    Elapsed: 0:08:34.\n",
            "  Batch   400  of  7,606.    Elapsed: 0:09:31.\n",
            "  Batch   440  of  7,606.    Elapsed: 0:10:28.\n",
            "  Batch   480  of  7,606.    Elapsed: 0:11:25.\n",
            "  Batch   520  of  7,606.    Elapsed: 0:12:22.\n",
            "  Batch   560  of  7,606.    Elapsed: 0:13:19.\n",
            "  Batch   600  of  7,606.    Elapsed: 0:14:16.\n",
            "  Batch   640  of  7,606.    Elapsed: 0:15:13.\n",
            "  Batch   680  of  7,606.    Elapsed: 0:16:10.\n",
            "  Batch   720  of  7,606.    Elapsed: 0:17:07.\n",
            "  Batch   760  of  7,606.    Elapsed: 0:18:04.\n",
            "  Batch   800  of  7,606.    Elapsed: 0:19:01.\n",
            "  Batch   840  of  7,606.    Elapsed: 0:19:58.\n",
            "  Batch   880  of  7,606.    Elapsed: 0:20:55.\n",
            "  Batch   920  of  7,606.    Elapsed: 0:21:52.\n",
            "  Batch   960  of  7,606.    Elapsed: 0:22:49.\n",
            "  Batch 1,000  of  7,606.    Elapsed: 0:23:46.\n",
            "  Batch 1,040  of  7,606.    Elapsed: 0:24:43.\n",
            "  Batch 1,080  of  7,606.    Elapsed: 0:25:40.\n",
            "  Batch 1,120  of  7,606.    Elapsed: 0:26:37.\n",
            "  Batch 1,160  of  7,606.    Elapsed: 0:27:34.\n",
            "  Batch 1,200  of  7,606.    Elapsed: 0:28:31.\n",
            "  Batch 1,240  of  7,606.    Elapsed: 0:29:28.\n",
            "  Batch 1,280  of  7,606.    Elapsed: 0:30:25.\n",
            "  Batch 1,320  of  7,606.    Elapsed: 0:31:22.\n",
            "  Batch 1,360  of  7,606.    Elapsed: 0:32:19.\n",
            "  Batch 1,400  of  7,606.    Elapsed: 0:33:16.\n",
            "  Batch 1,440  of  7,606.    Elapsed: 0:34:13.\n",
            "  Batch 1,480  of  7,606.    Elapsed: 0:35:10.\n",
            "  Batch 1,520  of  7,606.    Elapsed: 0:36:07.\n",
            "  Batch 1,560  of  7,606.    Elapsed: 0:37:04.\n",
            "  Batch 1,600  of  7,606.    Elapsed: 0:38:02.\n",
            "  Batch 1,640  of  7,606.    Elapsed: 0:38:59.\n",
            "  Batch 1,680  of  7,606.    Elapsed: 0:39:56.\n",
            "  Batch 1,720  of  7,606.    Elapsed: 0:40:53.\n",
            "  Batch 1,760  of  7,606.    Elapsed: 0:41:50.\n",
            "  Batch 1,800  of  7,606.    Elapsed: 0:42:47.\n",
            "  Batch 1,840  of  7,606.    Elapsed: 0:43:44.\n",
            "  Batch 1,880  of  7,606.    Elapsed: 0:44:41.\n",
            "  Batch 1,920  of  7,606.    Elapsed: 0:45:38.\n",
            "  Batch 1,960  of  7,606.    Elapsed: 0:46:35.\n",
            "  Batch 2,000  of  7,606.    Elapsed: 0:47:32.\n",
            "  Batch 2,040  of  7,606.    Elapsed: 0:48:29.\n",
            "  Batch 2,080  of  7,606.    Elapsed: 0:49:26.\n",
            "  Batch 2,120  of  7,606.    Elapsed: 0:50:23.\n",
            "  Batch 2,160  of  7,606.    Elapsed: 0:51:20.\n",
            "  Batch 2,200  of  7,606.    Elapsed: 0:52:17.\n",
            "  Batch 2,240  of  7,606.    Elapsed: 0:53:14.\n",
            "  Batch 2,280  of  7,606.    Elapsed: 0:54:11.\n",
            "  Batch 2,320  of  7,606.    Elapsed: 0:55:08.\n",
            "  Batch 2,360  of  7,606.    Elapsed: 0:56:05.\n",
            "  Batch 2,400  of  7,606.    Elapsed: 0:57:02.\n",
            "  Batch 2,440  of  7,606.    Elapsed: 0:57:59.\n",
            "  Batch 2,480  of  7,606.    Elapsed: 0:58:56.\n",
            "  Batch 2,520  of  7,606.    Elapsed: 0:59:53.\n",
            "  Batch 2,560  of  7,606.    Elapsed: 1:00:50.\n",
            "  Batch 2,600  of  7,606.    Elapsed: 1:01:47.\n",
            "  Batch 2,640  of  7,606.    Elapsed: 1:02:44.\n",
            "  Batch 2,680  of  7,606.    Elapsed: 1:03:41.\n",
            "  Batch 2,720  of  7,606.    Elapsed: 1:04:38.\n",
            "  Batch 2,760  of  7,606.    Elapsed: 1:05:35.\n",
            "  Batch 2,800  of  7,606.    Elapsed: 1:06:32.\n",
            "  Batch 2,840  of  7,606.    Elapsed: 1:07:30.\n",
            "  Batch 2,880  of  7,606.    Elapsed: 1:08:27.\n",
            "  Batch 2,920  of  7,606.    Elapsed: 1:09:24.\n",
            "  Batch 2,960  of  7,606.    Elapsed: 1:10:21.\n",
            "  Batch 3,000  of  7,606.    Elapsed: 1:11:18.\n",
            "  Batch 3,040  of  7,606.    Elapsed: 1:12:15.\n",
            "  Batch 3,080  of  7,606.    Elapsed: 1:13:12.\n",
            "  Batch 3,120  of  7,606.    Elapsed: 1:14:09.\n",
            "  Batch 3,160  of  7,606.    Elapsed: 1:15:06.\n",
            "  Batch 3,200  of  7,606.    Elapsed: 1:16:03.\n",
            "  Batch 3,240  of  7,606.    Elapsed: 1:17:00.\n",
            "  Batch 3,280  of  7,606.    Elapsed: 1:17:57.\n",
            "  Batch 3,320  of  7,606.    Elapsed: 1:18:54.\n",
            "  Batch 3,360  of  7,606.    Elapsed: 1:19:51.\n",
            "  Batch 3,400  of  7,606.    Elapsed: 1:20:48.\n",
            "  Batch 3,440  of  7,606.    Elapsed: 1:21:45.\n",
            "  Batch 3,480  of  7,606.    Elapsed: 1:22:42.\n",
            "  Batch 3,520  of  7,606.    Elapsed: 1:23:39.\n",
            "  Batch 3,560  of  7,606.    Elapsed: 1:24:36.\n",
            "  Batch 3,600  of  7,606.    Elapsed: 1:25:33.\n",
            "  Batch 3,640  of  7,606.    Elapsed: 1:26:30.\n",
            "  Batch 3,680  of  7,606.    Elapsed: 1:27:27.\n",
            "  Batch 3,720  of  7,606.    Elapsed: 1:28:24.\n",
            "  Batch 3,760  of  7,606.    Elapsed: 1:29:21.\n",
            "  Batch 3,800  of  7,606.    Elapsed: 1:30:18.\n",
            "  Batch 3,840  of  7,606.    Elapsed: 1:31:15.\n",
            "  Batch 3,880  of  7,606.    Elapsed: 1:32:12.\n",
            "  Batch 3,920  of  7,606.    Elapsed: 1:33:09.\n",
            "  Batch 3,960  of  7,606.    Elapsed: 1:34:06.\n",
            "  Batch 4,000  of  7,606.    Elapsed: 1:35:03.\n",
            "  Batch 4,040  of  7,606.    Elapsed: 1:36:00.\n",
            "  Batch 4,080  of  7,606.    Elapsed: 1:36:57.\n",
            "  Batch 4,120  of  7,606.    Elapsed: 1:37:54.\n",
            "  Batch 4,160  of  7,606.    Elapsed: 1:38:52.\n",
            "  Batch 4,200  of  7,606.    Elapsed: 1:39:49.\n",
            "  Batch 4,240  of  7,606.    Elapsed: 1:40:46.\n",
            "  Batch 4,280  of  7,606.    Elapsed: 1:41:43.\n",
            "  Batch 4,320  of  7,606.    Elapsed: 1:42:40.\n",
            "  Batch 4,360  of  7,606.    Elapsed: 1:43:37.\n",
            "  Batch 4,400  of  7,606.    Elapsed: 1:44:34.\n",
            "  Batch 4,440  of  7,606.    Elapsed: 1:45:31.\n",
            "  Batch 4,480  of  7,606.    Elapsed: 1:46:28.\n",
            "  Batch 4,520  of  7,606.    Elapsed: 1:47:25.\n",
            "  Batch 4,560  of  7,606.    Elapsed: 1:48:22.\n",
            "  Batch 4,600  of  7,606.    Elapsed: 1:49:19.\n",
            "  Batch 4,640  of  7,606.    Elapsed: 1:50:16.\n",
            "  Batch 4,680  of  7,606.    Elapsed: 1:51:13.\n",
            "  Batch 4,720  of  7,606.    Elapsed: 1:52:10.\n",
            "  Batch 4,760  of  7,606.    Elapsed: 1:53:07.\n",
            "  Batch 4,800  of  7,606.    Elapsed: 1:54:04.\n",
            "  Batch 4,840  of  7,606.    Elapsed: 1:55:01.\n",
            "  Batch 4,880  of  7,606.    Elapsed: 1:55:58.\n",
            "  Batch 4,920  of  7,606.    Elapsed: 1:56:55.\n",
            "  Batch 4,960  of  7,606.    Elapsed: 1:57:52.\n",
            "  Batch 5,000  of  7,606.    Elapsed: 1:58:49.\n",
            "  Batch 5,040  of  7,606.    Elapsed: 1:59:46.\n",
            "  Batch 5,080  of  7,606.    Elapsed: 2:00:43.\n",
            "  Batch 5,120  of  7,606.    Elapsed: 2:01:40.\n",
            "  Batch 5,160  of  7,606.    Elapsed: 2:02:37.\n",
            "  Batch 5,200  of  7,606.    Elapsed: 2:03:34.\n",
            "  Batch 5,240  of  7,606.    Elapsed: 2:04:31.\n",
            "  Batch 5,280  of  7,606.    Elapsed: 2:05:28.\n",
            "  Batch 5,320  of  7,606.    Elapsed: 2:06:25.\n",
            "  Batch 5,360  of  7,606.    Elapsed: 2:07:22.\n",
            "  Batch 5,400  of  7,606.    Elapsed: 2:08:19.\n",
            "  Batch 5,440  of  7,606.    Elapsed: 2:09:16.\n",
            "  Batch 5,480  of  7,606.    Elapsed: 2:10:13.\n",
            "  Batch 5,520  of  7,606.    Elapsed: 2:11:10.\n",
            "  Batch 5,560  of  7,606.    Elapsed: 2:12:07.\n",
            "  Batch 5,600  of  7,606.    Elapsed: 2:13:04.\n",
            "  Batch 5,640  of  7,606.    Elapsed: 2:14:01.\n",
            "  Batch 5,680  of  7,606.    Elapsed: 2:14:59.\n",
            "  Batch 5,720  of  7,606.    Elapsed: 2:15:56.\n",
            "  Batch 5,760  of  7,606.    Elapsed: 2:16:53.\n",
            "  Batch 5,800  of  7,606.    Elapsed: 2:17:50.\n",
            "  Batch 5,840  of  7,606.    Elapsed: 2:18:47.\n",
            "  Batch 5,880  of  7,606.    Elapsed: 2:19:44.\n",
            "  Batch 5,920  of  7,606.    Elapsed: 2:20:41.\n",
            "  Batch 5,960  of  7,606.    Elapsed: 2:21:38.\n",
            "  Batch 6,000  of  7,606.    Elapsed: 2:22:35.\n",
            "  Batch 6,040  of  7,606.    Elapsed: 2:23:32.\n",
            "  Batch 6,080  of  7,606.    Elapsed: 2:24:29.\n",
            "  Batch 6,120  of  7,606.    Elapsed: 2:25:26.\n",
            "  Batch 6,160  of  7,606.    Elapsed: 2:26:23.\n",
            "  Batch 6,200  of  7,606.    Elapsed: 2:27:20.\n",
            "  Batch 6,240  of  7,606.    Elapsed: 2:28:17.\n",
            "  Batch 6,280  of  7,606.    Elapsed: 2:29:14.\n",
            "  Batch 6,320  of  7,606.    Elapsed: 2:30:11.\n",
            "  Batch 6,360  of  7,606.    Elapsed: 2:31:08.\n",
            "  Batch 6,400  of  7,606.    Elapsed: 2:32:05.\n",
            "  Batch 6,440  of  7,606.    Elapsed: 2:33:02.\n",
            "  Batch 6,480  of  7,606.    Elapsed: 2:33:59.\n",
            "  Batch 6,520  of  7,606.    Elapsed: 2:34:56.\n",
            "  Batch 6,560  of  7,606.    Elapsed: 2:35:53.\n",
            "  Batch 6,600  of  7,606.    Elapsed: 2:36:50.\n",
            "  Batch 6,640  of  7,606.    Elapsed: 2:37:47.\n",
            "  Batch 6,680  of  7,606.    Elapsed: 2:38:44.\n",
            "  Batch 6,720  of  7,606.    Elapsed: 2:39:41.\n",
            "  Batch 6,760  of  7,606.    Elapsed: 2:40:38.\n",
            "  Batch 6,800  of  7,606.    Elapsed: 2:41:35.\n",
            "  Batch 6,840  of  7,606.    Elapsed: 2:42:32.\n",
            "  Batch 6,880  of  7,606.    Elapsed: 2:43:29.\n",
            "  Batch 6,920  of  7,606.    Elapsed: 2:44:26.\n",
            "  Batch 6,960  of  7,606.    Elapsed: 2:45:23.\n",
            "  Batch 7,000  of  7,606.    Elapsed: 2:46:20.\n",
            "  Batch 7,040  of  7,606.    Elapsed: 2:47:17.\n",
            "  Batch 7,080  of  7,606.    Elapsed: 2:48:14.\n",
            "  Batch 7,120  of  7,606.    Elapsed: 2:49:11.\n",
            "  Batch 7,160  of  7,606.    Elapsed: 2:50:08.\n",
            "  Batch 7,200  of  7,606.    Elapsed: 2:51:05.\n",
            "  Batch 7,240  of  7,606.    Elapsed: 2:52:02.\n",
            "  Batch 7,280  of  7,606.    Elapsed: 2:52:59.\n",
            "  Batch 7,320  of  7,606.    Elapsed: 2:53:56.\n",
            "  Batch 7,360  of  7,606.    Elapsed: 2:54:53.\n",
            "  Batch 7,400  of  7,606.    Elapsed: 2:55:50.\n",
            "  Batch 7,440  of  7,606.    Elapsed: 2:56:48.\n",
            "  Batch 7,480  of  7,606.    Elapsed: 2:57:45.\n",
            "  Batch 7,520  of  7,606.    Elapsed: 2:58:42.\n",
            "  Batch 7,560  of  7,606.    Elapsed: 2:59:39.\n",
            "  Batch 7,600  of  7,606.    Elapsed: 3:00:36.\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 3:00:44\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.59\n",
            "  F1 score: 0.52\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.22      0.80      0.34     16158\n",
            "           1       0.95      0.55      0.70    105534\n",
            "\n",
            "    accuracy                           0.59    121692\n",
            "   macro avg       0.58      0.68      0.52    121692\n",
            "weighted avg       0.85      0.59      0.65    121692\n",
            "\n",
            "[[12993  3165]\n",
            " [47174 58360]]\n",
            "Mathews Correlation coefficient score for this epoch is : 0.24\n",
            "Model validation score: f1=0.699 auc=0.949\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Validation Loss: 0.66\n",
            "  Validation took: 0:15:46\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of  7,606.    Elapsed: 0:00:57.\n",
            "  Batch    80  of  7,606.    Elapsed: 0:01:54.\n",
            "  Batch   120  of  7,606.    Elapsed: 0:02:51.\n",
            "  Batch   160  of  7,606.    Elapsed: 0:03:48.\n",
            "  Batch   200  of  7,606.    Elapsed: 0:04:45.\n",
            "  Batch   240  of  7,606.    Elapsed: 0:05:42.\n",
            "  Batch   280  of  7,606.    Elapsed: 0:06:40.\n",
            "  Batch   320  of  7,606.    Elapsed: 0:07:37.\n",
            "  Batch   360  of  7,606.    Elapsed: 0:08:34.\n",
            "  Batch   400  of  7,606.    Elapsed: 0:09:31.\n",
            "  Batch   440  of  7,606.    Elapsed: 0:10:28.\n",
            "  Batch   480  of  7,606.    Elapsed: 0:11:25.\n",
            "  Batch   520  of  7,606.    Elapsed: 0:12:22.\n",
            "  Batch   560  of  7,606.    Elapsed: 0:13:19.\n",
            "  Batch   600  of  7,606.    Elapsed: 0:14:16.\n",
            "  Batch   640  of  7,606.    Elapsed: 0:15:13.\n",
            "  Batch   680  of  7,606.    Elapsed: 0:16:10.\n",
            "  Batch   720  of  7,606.    Elapsed: 0:17:07.\n",
            "  Batch   760  of  7,606.    Elapsed: 0:18:04.\n",
            "  Batch   800  of  7,606.    Elapsed: 0:19:01.\n",
            "  Batch   840  of  7,606.    Elapsed: 0:19:58.\n",
            "  Batch   880  of  7,606.    Elapsed: 0:20:55.\n",
            "  Batch   920  of  7,606.    Elapsed: 0:21:52.\n",
            "  Batch   960  of  7,606.    Elapsed: 0:22:49.\n",
            "  Batch 1,000  of  7,606.    Elapsed: 0:23:46.\n",
            "  Batch 1,040  of  7,606.    Elapsed: 0:24:43.\n",
            "  Batch 1,080  of  7,606.    Elapsed: 0:25:40.\n",
            "  Batch 1,120  of  7,606.    Elapsed: 0:26:37.\n",
            "  Batch 1,160  of  7,606.    Elapsed: 0:27:34.\n",
            "  Batch 1,200  of  7,606.    Elapsed: 0:28:31.\n",
            "  Batch 1,240  of  7,606.    Elapsed: 0:29:28.\n",
            "  Batch 1,280  of  7,606.    Elapsed: 0:30:25.\n",
            "  Batch 1,320  of  7,606.    Elapsed: 0:31:22.\n",
            "  Batch 1,360  of  7,606.    Elapsed: 0:32:19.\n",
            "  Batch 1,400  of  7,606.    Elapsed: 0:33:16.\n",
            "  Batch 1,440  of  7,606.    Elapsed: 0:34:13.\n",
            "  Batch 1,480  of  7,606.    Elapsed: 0:35:10.\n",
            "  Batch 1,520  of  7,606.    Elapsed: 0:36:07.\n",
            "  Batch 1,560  of  7,606.    Elapsed: 0:37:04.\n",
            "  Batch 1,600  of  7,606.    Elapsed: 0:38:01.\n",
            "  Batch 1,640  of  7,606.    Elapsed: 0:38:58.\n",
            "  Batch 1,680  of  7,606.    Elapsed: 0:39:55.\n",
            "  Batch 1,720  of  7,606.    Elapsed: 0:40:53.\n",
            "  Batch 1,760  of  7,606.    Elapsed: 0:41:50.\n",
            "  Batch 1,800  of  7,606.    Elapsed: 0:42:47.\n",
            "  Batch 1,840  of  7,606.    Elapsed: 0:43:44.\n",
            "  Batch 1,880  of  7,606.    Elapsed: 0:44:41.\n",
            "  Batch 1,920  of  7,606.    Elapsed: 0:45:38.\n",
            "  Batch 1,960  of  7,606.    Elapsed: 0:46:35.\n",
            "  Batch 2,000  of  7,606.    Elapsed: 0:47:32.\n",
            "  Batch 2,040  of  7,606.    Elapsed: 0:48:29.\n",
            "  Batch 2,080  of  7,606.    Elapsed: 0:49:26.\n",
            "  Batch 2,120  of  7,606.    Elapsed: 0:50:23.\n",
            "  Batch 2,160  of  7,606.    Elapsed: 0:51:20.\n",
            "  Batch 2,200  of  7,606.    Elapsed: 0:52:17.\n",
            "  Batch 2,240  of  7,606.    Elapsed: 0:53:14.\n",
            "  Batch 2,280  of  7,606.    Elapsed: 0:54:11.\n",
            "  Batch 2,320  of  7,606.    Elapsed: 0:55:08.\n",
            "  Batch 2,360  of  7,606.    Elapsed: 0:56:05.\n",
            "  Batch 2,400  of  7,606.    Elapsed: 0:57:02.\n",
            "  Batch 2,440  of  7,606.    Elapsed: 0:57:59.\n",
            "  Batch 2,480  of  7,606.    Elapsed: 0:58:56.\n",
            "  Batch 2,520  of  7,606.    Elapsed: 0:59:53.\n",
            "  Batch 2,560  of  7,606.    Elapsed: 1:00:50.\n",
            "  Batch 2,600  of  7,606.    Elapsed: 1:01:47.\n",
            "  Batch 2,640  of  7,606.    Elapsed: 1:02:44.\n",
            "  Batch 2,680  of  7,606.    Elapsed: 1:03:41.\n",
            "  Batch 2,720  of  7,606.    Elapsed: 1:04:38.\n",
            "  Batch 2,760  of  7,606.    Elapsed: 1:05:35.\n",
            "  Batch 2,800  of  7,606.    Elapsed: 1:06:32.\n",
            "  Batch 2,840  of  7,606.    Elapsed: 1:07:29.\n",
            "  Batch 2,880  of  7,606.    Elapsed: 1:08:26.\n",
            "  Batch 2,920  of  7,606.    Elapsed: 1:09:23.\n",
            "  Batch 2,960  of  7,606.    Elapsed: 1:10:20.\n",
            "  Batch 3,000  of  7,606.    Elapsed: 1:11:17.\n",
            "  Batch 3,040  of  7,606.    Elapsed: 1:12:14.\n",
            "  Batch 3,080  of  7,606.    Elapsed: 1:13:11.\n",
            "  Batch 3,120  of  7,606.    Elapsed: 1:14:08.\n",
            "  Batch 3,160  of  7,606.    Elapsed: 1:15:05.\n",
            "  Batch 3,200  of  7,606.    Elapsed: 1:16:02.\n",
            "  Batch 3,240  of  7,606.    Elapsed: 1:16:59.\n",
            "  Batch 3,280  of  7,606.    Elapsed: 1:17:56.\n",
            "  Batch 3,320  of  7,606.    Elapsed: 1:18:53.\n",
            "  Batch 3,360  of  7,606.    Elapsed: 1:19:51.\n",
            "  Batch 3,400  of  7,606.    Elapsed: 1:20:48.\n",
            "  Batch 3,440  of  7,606.    Elapsed: 1:21:45.\n",
            "  Batch 3,480  of  7,606.    Elapsed: 1:22:42.\n",
            "  Batch 3,520  of  7,606.    Elapsed: 1:23:39.\n",
            "  Batch 3,560  of  7,606.    Elapsed: 1:24:36.\n",
            "  Batch 3,600  of  7,606.    Elapsed: 1:25:33.\n",
            "  Batch 3,640  of  7,606.    Elapsed: 1:26:30.\n",
            "  Batch 3,680  of  7,606.    Elapsed: 1:27:27.\n",
            "  Batch 3,720  of  7,606.    Elapsed: 1:28:24.\n",
            "  Batch 3,760  of  7,606.    Elapsed: 1:29:21.\n",
            "  Batch 3,800  of  7,606.    Elapsed: 1:30:18.\n",
            "  Batch 3,840  of  7,606.    Elapsed: 1:31:15.\n",
            "  Batch 3,880  of  7,606.    Elapsed: 1:32:12.\n",
            "  Batch 3,920  of  7,606.    Elapsed: 1:33:09.\n",
            "  Batch 3,960  of  7,606.    Elapsed: 1:34:06.\n",
            "  Batch 4,000  of  7,606.    Elapsed: 1:35:03.\n",
            "  Batch 4,040  of  7,606.    Elapsed: 1:36:00.\n",
            "  Batch 4,080  of  7,606.    Elapsed: 1:36:57.\n",
            "  Batch 4,120  of  7,606.    Elapsed: 1:37:54.\n",
            "  Batch 4,160  of  7,606.    Elapsed: 1:38:51.\n",
            "  Batch 4,200  of  7,606.    Elapsed: 1:39:48.\n",
            "  Batch 4,240  of  7,606.    Elapsed: 1:40:45.\n",
            "  Batch 4,280  of  7,606.    Elapsed: 1:41:42.\n",
            "  Batch 4,320  of  7,606.    Elapsed: 1:42:39.\n",
            "  Batch 4,360  of  7,606.    Elapsed: 1:43:36.\n",
            "  Batch 4,400  of  7,606.    Elapsed: 1:44:33.\n",
            "  Batch 4,440  of  7,606.    Elapsed: 1:45:30.\n",
            "  Batch 4,480  of  7,606.    Elapsed: 1:46:27.\n",
            "  Batch 4,520  of  7,606.    Elapsed: 1:47:24.\n",
            "  Batch 4,560  of  7,606.    Elapsed: 1:48:21.\n",
            "  Batch 4,600  of  7,606.    Elapsed: 1:49:18.\n",
            "  Batch 4,640  of  7,606.    Elapsed: 1:50:15.\n",
            "  Batch 4,680  of  7,606.    Elapsed: 1:51:12.\n",
            "  Batch 4,720  of  7,606.    Elapsed: 1:52:09.\n",
            "  Batch 4,760  of  7,606.    Elapsed: 1:53:07.\n",
            "  Batch 4,800  of  7,606.    Elapsed: 1:54:04.\n",
            "  Batch 4,840  of  7,606.    Elapsed: 1:55:01.\n",
            "  Batch 4,880  of  7,606.    Elapsed: 1:55:58.\n",
            "  Batch 4,920  of  7,606.    Elapsed: 1:56:55.\n",
            "  Batch 4,960  of  7,606.    Elapsed: 1:57:52.\n",
            "  Batch 5,000  of  7,606.    Elapsed: 1:58:49.\n",
            "  Batch 5,040  of  7,606.    Elapsed: 1:59:46.\n",
            "  Batch 5,080  of  7,606.    Elapsed: 2:00:43.\n",
            "  Batch 5,120  of  7,606.    Elapsed: 2:01:40.\n",
            "  Batch 5,160  of  7,606.    Elapsed: 2:02:37.\n",
            "  Batch 5,200  of  7,606.    Elapsed: 2:03:34.\n",
            "  Batch 5,240  of  7,606.    Elapsed: 2:04:31.\n",
            "  Batch 5,280  of  7,606.    Elapsed: 2:05:28.\n",
            "  Batch 5,320  of  7,606.    Elapsed: 2:06:25.\n",
            "  Batch 5,360  of  7,606.    Elapsed: 2:07:22.\n",
            "  Batch 5,400  of  7,606.    Elapsed: 2:08:19.\n",
            "  Batch 5,440  of  7,606.    Elapsed: 2:09:16.\n",
            "  Batch 5,480  of  7,606.    Elapsed: 2:10:13.\n",
            "  Batch 5,520  of  7,606.    Elapsed: 2:11:10.\n",
            "  Batch 5,560  of  7,606.    Elapsed: 2:12:07.\n",
            "  Batch 5,600  of  7,606.    Elapsed: 2:13:04.\n",
            "  Batch 5,640  of  7,606.    Elapsed: 2:14:01.\n",
            "  Batch 5,680  of  7,606.    Elapsed: 2:14:58.\n",
            "  Batch 5,720  of  7,606.    Elapsed: 2:15:55.\n",
            "  Batch 5,760  of  7,606.    Elapsed: 2:16:52.\n",
            "  Batch 5,800  of  7,606.    Elapsed: 2:17:49.\n",
            "  Batch 5,840  of  7,606.    Elapsed: 2:18:46.\n",
            "  Batch 5,880  of  7,606.    Elapsed: 2:19:43.\n",
            "  Batch 5,920  of  7,606.    Elapsed: 2:20:40.\n",
            "  Batch 5,960  of  7,606.    Elapsed: 2:21:37.\n",
            "  Batch 6,000  of  7,606.    Elapsed: 2:22:34.\n",
            "  Batch 6,040  of  7,606.    Elapsed: 2:23:32.\n",
            "  Batch 6,080  of  7,606.    Elapsed: 2:24:29.\n",
            "  Batch 6,120  of  7,606.    Elapsed: 2:25:26.\n",
            "  Batch 6,160  of  7,606.    Elapsed: 2:26:23.\n",
            "  Batch 6,200  of  7,606.    Elapsed: 2:27:20.\n",
            "  Batch 6,240  of  7,606.    Elapsed: 2:28:17.\n",
            "  Batch 6,280  of  7,606.    Elapsed: 2:29:14.\n",
            "  Batch 6,320  of  7,606.    Elapsed: 2:30:11.\n",
            "  Batch 6,360  of  7,606.    Elapsed: 2:31:08.\n",
            "  Batch 6,400  of  7,606.    Elapsed: 2:32:05.\n",
            "  Batch 6,440  of  7,606.    Elapsed: 2:33:02.\n",
            "  Batch 6,480  of  7,606.    Elapsed: 2:33:59.\n",
            "  Batch 6,520  of  7,606.    Elapsed: 2:34:56.\n",
            "  Batch 6,560  of  7,606.    Elapsed: 2:35:53.\n",
            "  Batch 6,600  of  7,606.    Elapsed: 2:36:50.\n",
            "  Batch 6,640  of  7,606.    Elapsed: 2:37:47.\n",
            "  Batch 6,680  of  7,606.    Elapsed: 2:38:44.\n",
            "  Batch 6,720  of  7,606.    Elapsed: 2:39:41.\n",
            "  Batch 6,760  of  7,606.    Elapsed: 2:40:38.\n",
            "  Batch 6,800  of  7,606.    Elapsed: 2:41:35.\n",
            "  Batch 6,840  of  7,606.    Elapsed: 2:42:32.\n",
            "  Batch 6,880  of  7,606.    Elapsed: 2:43:29.\n",
            "  Batch 6,920  of  7,606.    Elapsed: 2:44:26.\n",
            "  Batch 6,960  of  7,606.    Elapsed: 2:45:23.\n",
            "  Batch 7,000  of  7,606.    Elapsed: 2:46:20.\n",
            "  Batch 7,040  of  7,606.    Elapsed: 2:47:17.\n",
            "  Batch 7,080  of  7,606.    Elapsed: 2:48:14.\n",
            "  Batch 7,120  of  7,606.    Elapsed: 2:49:11.\n",
            "  Batch 7,160  of  7,606.    Elapsed: 2:50:08.\n",
            "  Batch 7,200  of  7,606.    Elapsed: 2:51:05.\n",
            "  Batch 7,240  of  7,606.    Elapsed: 2:52:02.\n",
            "  Batch 7,280  of  7,606.    Elapsed: 2:53:00.\n",
            "  Batch 7,320  of  7,606.    Elapsed: 2:53:57.\n",
            "  Batch 7,360  of  7,606.    Elapsed: 2:54:54.\n",
            "  Batch 7,400  of  7,606.    Elapsed: 2:55:51.\n",
            "  Batch 7,440  of  7,606.    Elapsed: 2:56:48.\n",
            "  Batch 7,480  of  7,606.    Elapsed: 2:57:45.\n",
            "  Batch 7,520  of  7,606.    Elapsed: 2:58:42.\n",
            "  Batch 7,560  of  7,606.    Elapsed: 2:59:39.\n",
            "  Batch 7,600  of  7,606.    Elapsed: 3:00:36.\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 3:00:44\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.56\n",
            "  F1 score: 0.51\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.83      0.33     16158\n",
            "           1       0.95      0.52      0.67    105534\n",
            "\n",
            "    accuracy                           0.56    121692\n",
            "   macro avg       0.58      0.68      0.50    121692\n",
            "weighted avg       0.85      0.56      0.63    121692\n",
            "\n",
            "[[13386  2772]\n",
            " [50478 55056]]\n",
            "Mathews Correlation coefficient score for this epoch is : 0.24\n",
            "Model validation score: f1=0.674 auc=0.949\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Validation Loss: 0.68\n",
            "  Validation took: 0:15:48\n",
            "\n",
            "Training complete!\n",
            "Total training took 9:49:32 (h:mm:ss)\n"
          ]
        }
      ],
      "source": [
        "#using weighted random sampler\n",
        "from sklearn.metrics import classification_report,auc,confusion_matrix,f1_score,precision_recall_curve,plot_precision_recall_curve,matthews_corrcoef\n",
        "import random\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "scaler = GradScaler()\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    roberta_model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        roberta_model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        with autocast():\n",
        "            loss, logits = roberta_model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "            b_labels1=torch.nn.functional.one_hot(b_labels, num_classes=2)\n",
        "            #Calculating weights\n",
        "            #positive=torch.sum(b_labels1, dim=0)\n",
        "           # negative=len(b_labels1)-positive\n",
        "            #negative\n",
        "            #pos_weight  = positive / negative\n",
        "            #criterion.pos_weight = pos_weight\n",
        "            loss1 = loss_fn(logits,b_labels1).to(device)\n",
        "           # print(\"loss:\",loss1)\n",
        "            loss1 = loss1 / gradient_accumulations\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss1.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        scaler.scale(loss1).backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "       # torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm*scaler.get_scale())\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        if ((step + 1) % gradient_accumulations == 0):\n",
        "             scaler.step(optimizer)\n",
        "       # Updates the scale for next iteration.\n",
        "             scaler.update()\n",
        "        # Update the learning rate.\n",
        "             scheduler.step()       \n",
        "             optimizer.zero_grad()\n",
        "            # roberta_model.zero_grad()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    roberta_model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    total_f1_score =0\n",
        "    predlist =[]\n",
        "    lbllist =[]\n",
        "    total_logits=[]\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            (loss, logits) = roberta_model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            #Converting the labels to one hot to sync with same shape as logits\n",
        "            b_labels1=torch.nn.functional.one_hot(b_labels, num_classes=2)\n",
        "            loss1 = loss_fn(logits, b_labels1)\n",
        "       # print(\"loss1:\",loss1)\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss1.item()\n",
        "\n",
        "         #Converting for predictions by applying sigmoid to logits\n",
        "        pred_logits_sigmoid=torch.sigmoid(logits)\n",
        "        y_pred=torch.round(pred_logits_sigmoid)\n",
        "        \n",
        "        # Move logits and labels to CPU\n",
        "        logits_pred = y_pred.detach().cpu().numpy()\n",
        "        label_ids1 = b_labels.to('cpu').numpy()\n",
        "        logits=logits.detach().cpu().numpy()\n",
        "        #For confusion matrix and classification report to work we need same dimensions.\n",
        "        label_ids = b_labels1.to('cpu').numpy()\n",
        "        pred_logits_sigmoid=pred_logits_sigmoid.detach().cpu().numpy()\n",
        "     \n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids1)\n",
        "\n",
        "         #print(predictions)\n",
        "       # predictions=np.argmax(logits_pred, axis=1)\n",
        "        predictions =np.argmax(logits_pred,axis=1)\n",
        "        y_test=np.argmax(label_ids,axis=1)\n",
        "        predlist.extend(predictions)\n",
        "        lbllist.extend(y_test)\n",
        "        #Accumulating the sigmoid positive logits for precision recall curve\n",
        "        total_logits.extend(pred_logits_sigmoid[:,1])\n",
        "        #total_logits.extend(pred_logits_sigmoid)\n",
        "        total_f1_score += f1_score(predlist,lbllist, average = 'macro')\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    #f1 score\n",
        "\n",
        "    avg_f1_score =total_f1_score/len(validation_dataloader)\n",
        "    print(\"  F1 score: {0:.2f}\".format(avg_f1_score))\n",
        "\n",
        "     #classification report\n",
        "    print(classification_report(lbllist, predlist))  \n",
        "\n",
        "    #confusion matrix\n",
        "    cm = confusion_matrix(lbllist,predlist)\n",
        "    # constant for classes\n",
        "    print(cm)\n",
        "    #mcc score\n",
        "    print(\"Mathews Correlation coefficient score for this epoch is :\",round(matthews_corrcoef(lbllist, predlist),2))\n",
        "    #Precision recall curve plot\n",
        "    lr_precision, lr_recall, thresholds = precision_recall_curve(lbllist,total_logits)\n",
        "    lr_f1, lr_auc = f1_score( lbllist,predlist), auc(lr_recall, lr_precision)\n",
        "    print('Model validation score: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
        "    baseline = lbllist.count(1) / len(lbllist)\n",
        "    plt.plot([0, 1], [baseline, baseline], linestyle='--', label='baseline')\n",
        "    plt.plot(lr_recall, lr_precision, marker='.', label='Roberta model evaluation')\n",
        "    # axis labels\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    # show the legend\n",
        "    plt.legend()\n",
        "    # show the plot\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZJG_wq6vXbE",
        "outputId": "0070f375-d389-4513-ead2-08df01065d58"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "121692"
            ]
          },
          "execution_count": 160,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(total_logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wE0g5b4Gv1r-"
      },
      "outputs": [],
      "source": [
        "print(\"Mathews Correlation coefficient score for this epoch is :\",round(matthews_corrcoef(lbllist, predlist),2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JDZoR_yHd10G",
        "outputId": "4521458d-68eb-4ba2-d4e3-d6c3f342e8bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of  7,606.    Elapsed: 0:00:57.\n",
            "  Batch    80  of  7,606.    Elapsed: 0:01:54.\n",
            "  Batch   120  of  7,606.    Elapsed: 0:02:51.\n",
            "  Batch   160  of  7,606.    Elapsed: 0:03:48.\n",
            "  Batch   200  of  7,606.    Elapsed: 0:04:46.\n",
            "  Batch   240  of  7,606.    Elapsed: 0:05:43.\n",
            "  Batch   280  of  7,606.    Elapsed: 0:06:40.\n",
            "  Batch   320  of  7,606.    Elapsed: 0:07:37.\n",
            "  Batch   360  of  7,606.    Elapsed: 0:08:34.\n",
            "  Batch   400  of  7,606.    Elapsed: 0:09:31.\n",
            "  Batch   440  of  7,606.    Elapsed: 0:10:28.\n",
            "  Batch   480  of  7,606.    Elapsed: 0:11:25.\n",
            "  Batch   520  of  7,606.    Elapsed: 0:12:22.\n",
            "  Batch   560  of  7,606.    Elapsed: 0:13:19.\n",
            "  Batch   600  of  7,606.    Elapsed: 0:14:16.\n",
            "  Batch   640  of  7,606.    Elapsed: 0:15:13.\n",
            "  Batch   680  of  7,606.    Elapsed: 0:16:10.\n",
            "  Batch   720  of  7,606.    Elapsed: 0:17:07.\n",
            "  Batch   760  of  7,606.    Elapsed: 0:18:04.\n",
            "  Batch   800  of  7,606.    Elapsed: 0:19:01.\n",
            "  Batch   840  of  7,606.    Elapsed: 0:19:58.\n",
            "  Batch   880  of  7,606.    Elapsed: 0:20:55.\n",
            "  Batch   920  of  7,606.    Elapsed: 0:21:53.\n",
            "  Batch   960  of  7,606.    Elapsed: 0:22:50.\n",
            "  Batch 1,000  of  7,606.    Elapsed: 0:23:47.\n",
            "  Batch 1,040  of  7,606.    Elapsed: 0:24:44.\n",
            "  Batch 1,080  of  7,606.    Elapsed: 0:25:41.\n",
            "  Batch 1,120  of  7,606.    Elapsed: 0:26:38.\n",
            "  Batch 1,160  of  7,606.    Elapsed: 0:27:35.\n",
            "  Batch 1,200  of  7,606.    Elapsed: 0:28:32.\n",
            "  Batch 1,240  of  7,606.    Elapsed: 0:29:29.\n",
            "  Batch 1,280  of  7,606.    Elapsed: 0:30:26.\n",
            "  Batch 1,320  of  7,606.    Elapsed: 0:31:23.\n",
            "  Batch 1,360  of  7,606.    Elapsed: 0:32:20.\n",
            "  Batch 1,400  of  7,606.    Elapsed: 0:33:17.\n",
            "  Batch 1,440  of  7,606.    Elapsed: 0:34:14.\n",
            "  Batch 1,480  of  7,606.    Elapsed: 0:35:11.\n",
            "  Batch 1,520  of  7,606.    Elapsed: 0:36:08.\n",
            "  Batch 1,560  of  7,606.    Elapsed: 0:37:05.\n",
            "  Batch 1,600  of  7,606.    Elapsed: 0:38:02.\n",
            "  Batch 1,640  of  7,606.    Elapsed: 0:38:59.\n",
            "  Batch 1,680  of  7,606.    Elapsed: 0:39:57.\n",
            "  Batch 1,720  of  7,606.    Elapsed: 0:40:54.\n",
            "  Batch 1,760  of  7,606.    Elapsed: 0:41:51.\n",
            "  Batch 1,800  of  7,606.    Elapsed: 0:42:48.\n",
            "  Batch 1,840  of  7,606.    Elapsed: 0:43:45.\n",
            "  Batch 1,880  of  7,606.    Elapsed: 0:44:42.\n",
            "  Batch 1,920  of  7,606.    Elapsed: 0:45:39.\n",
            "  Batch 1,960  of  7,606.    Elapsed: 0:46:36.\n",
            "  Batch 2,000  of  7,606.    Elapsed: 0:47:33.\n",
            "  Batch 2,040  of  7,606.    Elapsed: 0:48:30.\n",
            "  Batch 2,080  of  7,606.    Elapsed: 0:49:27.\n",
            "  Batch 2,120  of  7,606.    Elapsed: 0:50:24.\n",
            "  Batch 2,160  of  7,606.    Elapsed: 0:51:21.\n",
            "  Batch 2,200  of  7,606.    Elapsed: 0:52:18.\n",
            "  Batch 2,240  of  7,606.    Elapsed: 0:53:15.\n",
            "  Batch 2,280  of  7,606.    Elapsed: 0:54:12.\n",
            "  Batch 2,320  of  7,606.    Elapsed: 0:55:09.\n",
            "  Batch 2,360  of  7,606.    Elapsed: 0:56:06.\n",
            "  Batch 2,400  of  7,606.    Elapsed: 0:57:03.\n",
            "  Batch 2,440  of  7,606.    Elapsed: 0:58:01.\n",
            "  Batch 2,480  of  7,606.    Elapsed: 0:58:58.\n",
            "  Batch 2,520  of  7,606.    Elapsed: 0:59:55.\n",
            "  Batch 2,560  of  7,606.    Elapsed: 1:00:52.\n",
            "  Batch 2,600  of  7,606.    Elapsed: 1:01:49.\n",
            "  Batch 2,640  of  7,606.    Elapsed: 1:02:46.\n",
            "  Batch 2,680  of  7,606.    Elapsed: 1:03:43.\n",
            "  Batch 2,720  of  7,606.    Elapsed: 1:04:40.\n",
            "  Batch 2,760  of  7,606.    Elapsed: 1:05:37.\n",
            "  Batch 2,800  of  7,606.    Elapsed: 1:06:34.\n",
            "  Batch 2,840  of  7,606.    Elapsed: 1:07:31.\n",
            "  Batch 2,880  of  7,606.    Elapsed: 1:08:28.\n",
            "  Batch 2,920  of  7,606.    Elapsed: 1:09:25.\n",
            "  Batch 2,960  of  7,606.    Elapsed: 1:10:22.\n",
            "  Batch 3,000  of  7,606.    Elapsed: 1:11:19.\n",
            "  Batch 3,040  of  7,606.    Elapsed: 1:12:16.\n",
            "  Batch 3,080  of  7,606.    Elapsed: 1:13:13.\n",
            "  Batch 3,120  of  7,606.    Elapsed: 1:14:10.\n",
            "  Batch 3,160  of  7,606.    Elapsed: 1:15:07.\n",
            "  Batch 3,200  of  7,606.    Elapsed: 1:16:04.\n",
            "  Batch 3,240  of  7,606.    Elapsed: 1:17:02.\n",
            "  Batch 3,280  of  7,606.    Elapsed: 1:17:59.\n",
            "  Batch 3,320  of  7,606.    Elapsed: 1:18:56.\n",
            "  Batch 3,360  of  7,606.    Elapsed: 1:19:53.\n",
            "  Batch 3,400  of  7,606.    Elapsed: 1:20:50.\n",
            "  Batch 3,440  of  7,606.    Elapsed: 1:21:47.\n",
            "  Batch 3,480  of  7,606.    Elapsed: 1:22:44.\n",
            "  Batch 3,520  of  7,606.    Elapsed: 1:23:41.\n",
            "  Batch 3,560  of  7,606.    Elapsed: 1:24:38.\n",
            "  Batch 3,600  of  7,606.    Elapsed: 1:25:35.\n",
            "  Batch 3,640  of  7,606.    Elapsed: 1:26:32.\n",
            "  Batch 3,680  of  7,606.    Elapsed: 1:27:29.\n",
            "  Batch 3,720  of  7,606.    Elapsed: 1:28:26.\n",
            "  Batch 3,760  of  7,606.    Elapsed: 1:29:23.\n",
            "  Batch 3,800  of  7,606.    Elapsed: 1:30:20.\n",
            "  Batch 3,840  of  7,606.    Elapsed: 1:31:17.\n",
            "  Batch 3,880  of  7,606.    Elapsed: 1:32:14.\n",
            "  Batch 3,920  of  7,606.    Elapsed: 1:33:11.\n",
            "  Batch 3,960  of  7,606.    Elapsed: 1:34:08.\n",
            "  Batch 4,000  of  7,606.    Elapsed: 1:35:05.\n",
            "  Batch 4,040  of  7,606.    Elapsed: 1:36:02.\n",
            "  Batch 4,080  of  7,606.    Elapsed: 1:37:00.\n",
            "  Batch 4,120  of  7,606.    Elapsed: 1:37:57.\n",
            "  Batch 4,160  of  7,606.    Elapsed: 1:38:54.\n",
            "  Batch 4,200  of  7,606.    Elapsed: 1:39:51.\n",
            "  Batch 4,240  of  7,606.    Elapsed: 1:40:48.\n",
            "  Batch 4,280  of  7,606.    Elapsed: 1:41:45.\n",
            "  Batch 4,320  of  7,606.    Elapsed: 1:42:42.\n",
            "  Batch 4,360  of  7,606.    Elapsed: 1:43:39.\n",
            "  Batch 4,400  of  7,606.    Elapsed: 1:44:36.\n",
            "  Batch 4,440  of  7,606.    Elapsed: 1:45:33.\n",
            "  Batch 4,480  of  7,606.    Elapsed: 1:46:30.\n",
            "  Batch 4,520  of  7,606.    Elapsed: 1:47:27.\n",
            "  Batch 4,560  of  7,606.    Elapsed: 1:48:24.\n",
            "  Batch 4,600  of  7,606.    Elapsed: 1:49:21.\n",
            "  Batch 4,640  of  7,606.    Elapsed: 1:50:18.\n",
            "  Batch 4,680  of  7,606.    Elapsed: 1:51:15.\n",
            "  Batch 4,720  of  7,606.    Elapsed: 1:52:12.\n",
            "  Batch 4,760  of  7,606.    Elapsed: 1:53:09.\n",
            "  Batch 4,800  of  7,606.    Elapsed: 1:54:06.\n",
            "  Batch 4,840  of  7,606.    Elapsed: 1:55:03.\n",
            "  Batch 4,880  of  7,606.    Elapsed: 1:56:01.\n",
            "  Batch 4,920  of  7,606.    Elapsed: 1:56:58.\n",
            "  Batch 4,960  of  7,606.    Elapsed: 1:57:55.\n",
            "  Batch 5,000  of  7,606.    Elapsed: 1:58:52.\n",
            "  Batch 5,040  of  7,606.    Elapsed: 1:59:49.\n",
            "  Batch 5,080  of  7,606.    Elapsed: 2:00:46.\n",
            "  Batch 5,120  of  7,606.    Elapsed: 2:01:43.\n",
            "  Batch 5,160  of  7,606.    Elapsed: 2:02:40.\n",
            "  Batch 5,200  of  7,606.    Elapsed: 2:03:37.\n",
            "  Batch 5,240  of  7,606.    Elapsed: 2:04:34.\n",
            "  Batch 5,280  of  7,606.    Elapsed: 2:05:31.\n",
            "  Batch 5,320  of  7,606.    Elapsed: 2:06:28.\n",
            "  Batch 5,360  of  7,606.    Elapsed: 2:07:25.\n",
            "  Batch 5,400  of  7,606.    Elapsed: 2:08:22.\n",
            "  Batch 5,440  of  7,606.    Elapsed: 2:09:19.\n",
            "  Batch 5,480  of  7,606.    Elapsed: 2:10:16.\n",
            "  Batch 5,520  of  7,606.    Elapsed: 2:11:13.\n",
            "  Batch 5,560  of  7,606.    Elapsed: 2:12:10.\n",
            "  Batch 5,600  of  7,606.    Elapsed: 2:13:07.\n",
            "  Batch 5,640  of  7,606.    Elapsed: 2:14:05.\n",
            "  Batch 5,680  of  7,606.    Elapsed: 2:15:02.\n",
            "  Batch 5,720  of  7,606.    Elapsed: 2:15:59.\n",
            "  Batch 5,760  of  7,606.    Elapsed: 2:16:56.\n",
            "  Batch 5,800  of  7,606.    Elapsed: 2:17:53.\n",
            "  Batch 5,840  of  7,606.    Elapsed: 2:18:50.\n",
            "  Batch 5,880  of  7,606.    Elapsed: 2:19:47.\n",
            "  Batch 5,920  of  7,606.    Elapsed: 2:20:44.\n",
            "  Batch 5,960  of  7,606.    Elapsed: 2:21:41.\n",
            "  Batch 6,000  of  7,606.    Elapsed: 2:22:38.\n",
            "  Batch 6,040  of  7,606.    Elapsed: 2:23:35.\n",
            "  Batch 6,080  of  7,606.    Elapsed: 2:24:32.\n",
            "  Batch 6,120  of  7,606.    Elapsed: 2:25:29.\n",
            "  Batch 6,160  of  7,606.    Elapsed: 2:26:26.\n",
            "  Batch 6,200  of  7,606.    Elapsed: 2:27:23.\n",
            "  Batch 6,240  of  7,606.    Elapsed: 2:28:20.\n",
            "  Batch 6,280  of  7,606.    Elapsed: 2:29:17.\n",
            "  Batch 6,320  of  7,606.    Elapsed: 2:30:14.\n",
            "  Batch 6,360  of  7,606.    Elapsed: 2:31:11.\n",
            "  Batch 6,400  of  7,606.    Elapsed: 2:32:08.\n",
            "  Batch 6,440  of  7,606.    Elapsed: 2:33:06.\n",
            "  Batch 6,480  of  7,606.    Elapsed: 2:34:03.\n",
            "  Batch 6,520  of  7,606.    Elapsed: 2:35:00.\n",
            "  Batch 6,560  of  7,606.    Elapsed: 2:35:57.\n",
            "  Batch 6,600  of  7,606.    Elapsed: 2:36:54.\n",
            "  Batch 6,640  of  7,606.    Elapsed: 2:37:51.\n",
            "  Batch 6,680  of  7,606.    Elapsed: 2:38:48.\n",
            "  Batch 6,720  of  7,606.    Elapsed: 2:39:45.\n",
            "  Batch 6,760  of  7,606.    Elapsed: 2:40:42.\n",
            "  Batch 6,800  of  7,606.    Elapsed: 2:41:39.\n",
            "  Batch 6,840  of  7,606.    Elapsed: 2:42:36.\n",
            "  Batch 6,880  of  7,606.    Elapsed: 2:43:33.\n",
            "  Batch 6,920  of  7,606.    Elapsed: 2:44:30.\n",
            "  Batch 6,960  of  7,606.    Elapsed: 2:45:27.\n",
            "  Batch 7,000  of  7,606.    Elapsed: 2:46:24.\n",
            "  Batch 7,040  of  7,606.    Elapsed: 2:47:21.\n",
            "  Batch 7,080  of  7,606.    Elapsed: 2:48:18.\n",
            "  Batch 7,120  of  7,606.    Elapsed: 2:49:15.\n",
            "  Batch 7,160  of  7,606.    Elapsed: 2:50:12.\n",
            "  Batch 7,200  of  7,606.    Elapsed: 2:51:10.\n",
            "  Batch 7,240  of  7,606.    Elapsed: 2:52:07.\n",
            "  Batch 7,280  of  7,606.    Elapsed: 2:53:04.\n",
            "  Batch 7,320  of  7,606.    Elapsed: 2:54:01.\n",
            "  Batch 7,360  of  7,606.    Elapsed: 2:54:58.\n",
            "  Batch 7,400  of  7,606.    Elapsed: 2:55:55.\n",
            "  Batch 7,440  of  7,606.    Elapsed: 2:56:52.\n",
            "  Batch 7,480  of  7,606.    Elapsed: 2:57:49.\n",
            "  Batch 7,520  of  7,606.    Elapsed: 2:58:46.\n",
            "  Batch 7,560  of  7,606.    Elapsed: 2:59:43.\n",
            "  Batch 7,600  of  7,606.    Elapsed: 3:00:40.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 3:00:48\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.87\n",
            "  F1 score: 0.57\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.64      0.36     16168\n",
            "           1       0.93      0.70      0.80    105524\n",
            "\n",
            "    accuracy                           0.69    121692\n",
            "   macro avg       0.59      0.67      0.58    121692\n",
            "weighted avg       0.84      0.69      0.74    121692\n",
            "\n",
            "[[10397  5771]\n",
            " [31879 73645]]\n",
            "Mathews Correlation coefficient score for this epoch is : 0.24\n",
            "Model validation score: f1=0.796 auc=0.942\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3xU1bn/8c9DAJGLyE2qgIAtKgHCLaJIEQqiVDFWxVP4eQEvxUvRtp7+ftXT0xqptrTq8VRLj8WfyMXWG7UWtCoWQdTjhVABBQQRUIIcjVxFoJDkOX/MTpyESTJJZmdmMt/36zUvZu+1L89Ohnmy1tprbXN3REREKmuS7ABERCQ1KUGIiEhMShAiIhKTEoSIiMSkBCEiIjE1TXYAidKxY0fv0aNHssMQEUkrK1as+NzdO8UqazQJokePHhQUFCQ7DBGRtGJmH1VVpiYmERGJSQlCRERiUoIQEZGYGk0fhEgqOnz4MIWFhRw8eDDZoUiGa9GiBV27dqVZs2Zx76MEIRKiwsJC2rRpQ48ePTCzZIcjGcrd2bFjB4WFhfTs2TPu/UJrYjKzWWb2mZm9V0W5mdn9ZrbRzFab2aCosklm9kHwmhRWjCJhO3jwIB06dFBykKQyMzp06FDrmmyYNYjZwO+AuVWUfxvoFbxOB/4LON3M2gO3A7mAAyvMbIG77wot0vx2QCnQBPLDO41kJiUHSQV1+RyGVoNw92XAzmo2uRCY6xFvAsea2fHAucBL7r4zSAovAWPDivOr5EDk3/x2oZ1KRCSdJPMupi7A1qjlwmBdVeuPYGZTzKzAzAqKiorqGEZpDcsi6WvLli307ds3lGMvXbqUcePGAbBgwQKmT58eynkkedK6k9rdZwIzAXJzc+v45CMj0pJVRnf+itRWXl4eeXl5yQ5DEiyZ34bbgG5Ry12DdVWtD8fU5VEL6oOQxqe4uJjLLruM3r17M378ePbv38+0adM47bTT6Nu3L1OmTKHsyZL3338/2dnZ5OTkMGHCBAC+/PJLrr76aoYMGcLAgQP561//esQ5Zs+ezdSpUwGYPHkyN998M2eeeSYnnXQS8+fPL9/u7rvv5rTTTiMnJ4fbb7+9Aa5e6iOZNYgFwFQze5xIJ/Ued99uZi8CvzSzss6Ac4DbGiQiJQcJ2Xf/8MYR68blHM8VQ3tw4FAJkx95+4jy8YO7cmluN3Z+eYgbHl1RoeyJ64bWeM7169fz8MMPM2zYMK6++mp+//vfM3XqVH7+858DcMUVV/Dss89ywQUXMH36dDZv3sxRRx3F7t27AbjrrrsYNWoUs2bNYvfu3QwZMoSzzz672nNu376d1157jffff5+8vDzGjx/PokWL+OCDD3j77bdxd/Ly8li2bBlnnXVWjdcgyRHmba6PAW8Ap5hZoZldY2bXm9n1wSZ/AzYBG4GHgBsB3H0n8AtgefCaFqwLK9LwDi2SArp168awYcMAuPzyy3nttddYsmQJp59+Ov369ePll19mzZo1AOTk5HDZZZfx6KOP0rRp5O/HRYsWMX36dAYMGMDIkSM5ePAgH3/8cbXn/M53vkOTJk3Izs7m008/LT/OokWLGDhwIIMGDeL999/ngw8+CPHKpb5Cq0G4+8Qayh34fhVls4BZYcQlkkzV/cV/dPOsasvbt2oeV42hssq3N5oZN954IwUFBXTr1o38/Pzy++Ofe+45li1bxsKFC7nrrrt49913cXf+/Oc/c8opp1Q4TtkXfyxHHXVU+fuy5it357bbbuO6666r9TVIcqhHVqSR+/jjj3njjUjT1p/+9Ce++c1vAtCxY0f27dtX3kdQWlrK1q1b+da3vsWvf/1r9uzZw759+zj33HN54IEHyr/o33nnnTrFce655zJr1iz27dsHwLZt2/jss8/qe3kSorS+i0lEanbKKacwY8YMrr76arKzs7nhhhvYtWsXffv25Wtf+xqnnXYaACUlJVx++eXs2bMHd+fmm2/m2GOP5Wc/+xk//OEPycnJobS0lJ49e/Lss8/WOo5zzjmHdevWMXRopBbUunVrHn30UY477riEXq8kjpX9VZDucnNzvU4PDNrxITwQzPKRvyexQUnGW7duHb179052GCJA7M+jma1w99xY26uJSUREYlKCEBGRmJQgREQkJiUIzbQpIhKTEoSIiMSkBCEiIjEpQYg0cllZWQwYMIC+fftywQUXlM+xVJWRI0dSp1vGA1u2bOFPf/pTnfdPhB49evD555/Xe5v6qM/PcenSpfz3f/93+fKDDz7I3LlVPXstPEoQmotJGrmjjz6alStX8t5779G+fXtmzJgR2rmKi4tTIkGku8oJ4vrrr+fKK69s8DiUIKLlt9UT5ST5tr4Nr94b+TfBhg4dyrZtkdnzV65cyRlnnEFOTg4XXXQRu3Z9NZvxvHnzymsdb78diaOqab9nz55NXl4eo0aNYvTo0dx66628+uqrDBgwgPvuu48tW7YwfPhwBg0axKBBgyp88ZXZsmULp556KpMnT+bkk0/msssu4+9//zvDhg2jV69e5THs3LmT73znO+Tk5HDGGWewevVqAHbs2ME555xDnz59uPbaa4keAPzoo48yZMgQBgwYwHXXXUdJSUm1P6NFixYxdOhQBg0axKWXXsq+fft44YUXuPTSS8u3iX5Y0g033EBubi59+vSpcgrz1q1bl7+fP38+kydPBmDhwoWcfvrpDBw4kLPPPptPP/2ULVu28OCDD3LfffcxYMAAXn31VfLz87nnnnuq/b2NHDmSn/zkJwwZMoSTTz6ZV199tdrrjIu7N4rX4MGDvU52bHK//ZhKr2PrdiyRStauXfvVwt9+4j7rvOpf//VN9/xjI5/D/GMjy9Vt/7ef1BhDq1at3N29uLjYx48f788//7y7u/fr18+XLl3q7u4/+9nP/Ac/+IG7u48YMcKvvfZad3d/5ZVXvE+fPu7uftttt/m8efPc3X3Xrl3eq1cv37dvnz/yyCPepUsX37Fjh7u7L1myxM8///zy83/55Zd+4MABd3ffsGGDx/q/unnzZs/KyvLVq1d7SUmJDxo0yK+66iovLS31Z555xi+88EJ3d586darn5+e7u/vixYu9f//+7u5+0003+R133OHu7s8++6wDXlRU5GvXrvVx48b5oUOH3N39hhtu8Dlz5ri7e/fu3b2oqKhCHEVFRT58+HDft2+fu7tPnz7d77jjDj98+LB369atfP31119f/rMou+7i4mIfMWKEr1q1qvznuHz58gq/A3f3p556yidNmuTu7jt37vTS0lJ3d3/ooYf8lltucXf322+/3e++++7yfaKXq/u9le3/3HPP+ejRo4/4OVf4PAaAAq/ie1VzMcW8zVWPHZUkObgHPPj8eWlk+ahj6nXIAwcOMGDAALZt20bv3r0ZM2YMe/bsYffu3YwYMQKASZMmVfgLeeLEyGTMZ511Fnv37mX37t0sWrSIBQsWlP8lGz3t95gxY2jfvn3M8x8+fJipU6eycuVKsrKy2LBhQ8ztevbsSb9+/QDo06cPo0ePxszo168fW7ZsAeC1117jz3/+MwCjRo1ix44d7N27l2XLlvH0008DcP7559OuXaQlYPHixaxYsaJ8vqkDBw5UO/fTm2++ydq1a8unRz906BBDhw6ladOmjB07loULFzJ+/Hiee+45fvOb3wDw5JNPMnPmTIqLi9m+fTtr164lJyenynNEKyws5Lvf/S7bt2/n0KFD9OzZs9rta/q9XXzxxQAMHjy4/GdWH0oQvx2U7AgkU3w7jmc2b30b5uRBySHIag6X/H/oNqRepy3rg9i/fz/nnnsuM2bMYNKkSdXuE2uKcK9i2u+33nqLVq1aVXms++67j86dO7Nq1SpKS0tp0aJFzO2ipwhv0qRJ+XKTJk0oLi6uNt6quDuTJk3iV7/6Vdzbjxkzhscee+yIsgkTJvC73/2O9u3bk5ubS5s2bdi8eTP33HMPy5cvp127dkyePLl86vRo0T/P6PKbbrqJW265hby8PJYuXUp+fn7tLzJK2c8sKyurzj+zaOqDoIr2yPy2ML1Hg0YiQrchMGkBjPpp5N96JodoLVu25P777+fee++lVatWtGvXrrydet68eeV/lQI88cQTQOQv9rZt29K2bdu4p/1u06YNX3zxRfnynj17OP7442nSpAnz5s2rsQ+gOsOHD+ePf/wjEOkH6NixI8cccwxnnXVWecf4888/X94uP3r0aObPn18+rfjOnTv56KOPqjz+GWecweuvv87GjRuBSL9LWY1nxIgR/OMf/+Chhx4qfxzr3r17adWqFW3btuXTTz/l+eefj3nczp07s27dOkpLS/nLX/5S4WfTpUsXAObMmVO+vvLPsEzbtm2r/b0lmmoQ1Tm4K+i41iyv0oC6DUloYog2cOBAcnJyeOyxx5gzZw7XX389+/fv56STTuKRRx4p365FixYMHDiQw4cPM2tW5Nld8U77nZOTQ1ZWFv3792fy5MnceOONXHLJJcydO5exY8dWW9uoSX5+PldffTU5OTm0bNmy/Ev19ttvZ+LEifTp04czzzyTE088EYDs7GzuvPNOzjnnHEpLS2nWrBkzZsyge/fuMY/fqVMnZs+ezcSJE/nnP/8JwJ133snJJ59MVlYW48aNY/bs2eXn7d+/PwMHDuTUU0+t8OS+yqZPn864cePo1KkTubm55c/EyM/P59JLL6Vdu3aMGjWKzZs3A3DBBRcwfvx4/vrXv/LAAw9UOFZ1v7dE03Tf+W1rt32LdnDrltqfRzKSpvuWVKLpvsN2cJeankQkIyhB1EV501Mtax8iImlECaK+8tvCtI7JjkJSWGNpxpX0VpfPYagJwszGmtl6M9toZrfGKO9uZovNbLWZLTWzrlFlvzGzNWa2zszut8r33aWS0sOqTUhMLVq0YMeOHUoSklTuzo4dO6q8xbgqod3FZGZZwAxgDFAILDezBe6+Nmqze4C57j7HzEYBvwKuMLMzgWFA2WiT14ARwNKw4k0I3fEklXTt2pXCwkKKioqSHYpkuBYtWtC1a9eaN4wS5m2uQ4CN7r4JwMweBy4EohNENnBL8H4J8Ezw3oEWQHMis+k1Az4NMdbqNWkKpXEOOlGSkCjNmjWrcXSsSKoKs4mpC7A1arkwWBdtFXBx8P4ioI2ZdXD3N4gkjO3B60V3X1f5BGY2xcwKzKwglL/QThoV+bL/+Q4Y9sP49yvrwFazk4iksWQPlPsx8DszmwwsA7YBJWb2DaA3UFYfesnMhrt7hekJ3X0mMBMi4yASHt2VX414ZMwdkRfA3Itg08vxHaNyktA4ChFJE2EmiG1At6jlrsG6cu7+CUENwsxaA5e4+24z+x7wprvvC8qeB4YCCZi/NgGiE0dtawllt8hGU5OUiKSgMJuYlgO9zKynmTUHJgALojcws45mVhbDbcCs4P3HwAgza2pmzYh0UB/RxJQSEvHlruYoEUlBoSUIdy8GpgIvEvlyf9Ld15jZNDPLCzYbCaw3sw1AZ+CuYP184EPgXSL9FKvcfWFYsdZbomoAmiBQRFKI5mKq6i/3+vQVJKo2oKYnEQlZdXMxJbuTOjXVtyO57Iu9volCfRUikkRKELEk6i6jyl/oShgikkaUIBpS5S/0l26H1/+zHseLkXCUNEQkQTRZX9bRFZebtmy4c4+5I/Ff6BqkJyIJohpEVtOKTx3NatbwMUQniUR+sZcdS7UKEakDJYij28OhqGe/tuqQvFgg8f0WlY+hZCEicVKCOPxlxeVD+5MTR1XC6uhWohCRGihBVE4I//wi9napItYXe12ShmoVIlIDJQhrUv1yOqhvH4aShYjEkIbfhgnWGBJEtPw99fuS1x1QIhJQDaJZSzi096vl5q2SF0siJbJWEeuYItLoKUEc0Un9Zezt0lkYU38oWYg0emnenpIATSs9xLtZ7R7qnVbKmp9OGpWAY6kpSqSxUw2iZQfYH/W40tbHJS+WhhL9wCOo3xe95ocSabRUgyg9XHG55HDs7Rqz+nZsVziWahYijYVqECXFlZYPJSeOVJCoMRaV91OtQiQtKUEc2Flx+csdyYkjVSViniglC5G0pATRtEXFuZgacyd1fSXibiglC5G0oQSRiZ3U9ZWo+aE0L5RISlOCUCd1/dW3ZqFahUhKUoI4ul3F5ZZJnu47nam/QqRRCfU2VzMba2brzWyjmd0ao7y7mS02s9VmttTMukaVnWhmi8xsnZmtNbMeoQTZ4RsVlzudEsppMk7ZrbOJmBdKt82KJEVoNQgzywJmAGOAQmC5mS1w97VRm90DzHX3OWY2CvgVcEVQNhe4y91fMrPWQGkogXbuU3H5a/1DOU1GS2TNQrUKkQYTZg1iCLDR3Te5+yHgceDCSttkAy8H75eUlZtZNtDU3V8CcPd97h7Ok3w2vVJxeeOiUE4jAc02K5I2wuyD6AJsjVouBE6vtM0q4GLgt8BFQBsz6wCcDOw2s6eBnsDfgVvdPfrp0ZjZFGAKwIknnli3KHdtrri848O6HUdqR7PNiqS8ZE+18WNghJm9A4wAtgElRBLX8KD8NOAkYHLlnd19prvnuntup06d6hZBu54Vlyv3SUj4EtFfAeqzEEmwMGsQ24BuUctdg3Xl3P0TIjUIgn6GS9x9t5kVAivdfVNQ9gxwBvBwwqMc8j348O+R95YFw36Q8FNILSSiv6KqfVXDEKmVMGsQy4FeZtbTzJoDE4AF0RuYWUez8ke43QbMitr3WDMrqxaMAqI7txPHLJTDSgIkchJBUO1CpJZCSxDuXgxMBV4E1gFPuvsaM5tmZnnBZiOB9Wa2AegM3BXsW0KkeWmxmb0LGPBQKIG+HXVYL4HX/zOU00g9hJUolCxEqmXunuwYEiI3N9cLCgpqv+P9g2Hnxq+WO50K338rcYFJuBL1Ja/mJ8lQZrbC3XNjlWkkdfueFROEOqnTSxh9FkoWIoAShO5iakwS9TwLJQsRIPm3uSbf1jcrLm95NTlxSDg0ME+kzlSDaN254nKb45MTh4QrUQPzVKOQDKIahJqYMk99ahWqUUgGUQ2iqNLwiv9ZnZw4pOHVp1ahGoVkANUgeo2tuNy78nyCkhHKahXjflvL/VSjkMZL4yD2FcE93wBrAmfeDGPuSHxwkp7qPDW5ahWSPqobB6EaxCf/iPzrpfDWH2Dr28mNR1JHXfsqVKuQRkJ9EFujRk2XHIrc5tptSPLikdRT12duazyFpDnVILpFPaIiqzn0GJ68WCS16e4nyTCqQZwwKPJvk2YwdrpqD1KzutYoKu+jWoWkOCWIsj6I0sPwwq3QOVtJQuKjwXfSyKmJKVYfhEhtqflJGqG4ahBmNgzIB7oH+xjg7n5SeKE1EPVBSCJp8J00IvE2MT0M/AhYQeSZ0Y2H+iAkLPW9+0mJQpIs3gSxx92fDzWSZFEfhIStrrUKdWhLksWbIJaY2d3A08A/y1a6+z9CiaohRQ+M0zgICZvGVEgaiTdBlDXURw/HdmBUYsNJAvVBSDIk4lZZJQoJWVwJwt2/FXYgSdMl6INo1hKu/KtqD9KwlCgkhcV1m6uZtTWz/zCzguB1r5nV+Ik2s7Fmtt7MNprZrTHKu5vZYjNbbWZLzaxrpfJjzKzQzH4X/yWJpKFE3CarW2UlweIdBzEL+AL4l+C1F3ikuh3MLAuYAXwbyAYmmll2pc3uAea6ew4wDfhVpfJfAMvijLFutgXdKIf3w5w8TdYnyVWWKOqTLEQSJN4+iK+7+yVRy3eY2coa9hkCbHT3TQBm9jhwIRD9hJ5s4Jbg/RLgmbICMxsMdAZeoGLfR2Jpsj5JVfW9+0lNT1JP8dYgDpjZN8sWgoFzB2rYpwuwNWq5MFgXbRVwcfD+IqCNmXUwsybAvcCP44yv7tRJLemgLrUKNTtJPcVbg7gBmBP0OxiwE5icgPP/GPidmU0m0pS0jchAvBuBv7l7oZlVubOZTQGmAJx44ol1i0Cd1JJO6tKprRqF1FG8dzGtBPqb2THB8t44dtsGdIta7hqsiz7uJwQ1CDNrDVzi7rvNbCgw3MxuBFoDzc1sn7vfWmn/mcBMiDxRLp5rEWkU6tL8pLEUUkvVNjGZ2eXBv7eY2S3AtcC1UcvVWQ70MrOeZtYcmAAsqHT8jkFzEsBtRDrDcffL3P1Ed+9BpJYxt3JySBh1Uku6q+tT70RqUFMfRKvg3zZVvKrk7sXAVOBFYB3wpLuvMbNpZpYXbDYSWG9mG4h0SN9Vl4uol+1Rfe2azVXSVX36KApmhxKSpD9zbxwtM7m5uV5QUFD7Hbe+Hak5lByKdFJPWqB+CEl/dRp4p2anTGRmK9w95p2i8Q6U+00waK1ZMLCtqKz5Ke11GxJJCqN+quQgjYfuepIEiKsGYWYr3X2AmV0EjCMydmGZu/cPO8B41bkGIZIpavPlf81L+mMpQ9S7BsFXdzudDzzl7qqLiqSb2tQoHh6jGoXEnSCeNbP3gcHAYjPrBBwMLywRCUVZ09O439ZiHyWKTBV3J7WZtSfy4KASM2sJHOPu/xNqdLWgJiaROlBndsarromp2oFyZjbK3V82s4uj1kVv8nRiQhSRpKjryGwliYxQ00jqEcDLwAUxyhwlCJHGIX8PzBwFn6yIc3tN35EJNA5CRI5U60eiKlGkq0SMg/ilmR0btdzOzO5MVIAikmJqO45CHdmNUrx3MX3b3XeXLbj7LuC8cEISkZShRJHR4k0QWWZ2VNmCmR0NHFXN9iLSmNRlVLakvXgTxB+JjH+4xsyuAV4C5oQXloiknPLaRNXPaKm4vZJEuqvNOIixwNnB4kvu/mJoUdWBOqlFGlitbo1VJ3aqqvM4iErWAcXu/ncza2lmbdz9i8SEKCJppzZjKDR2Ii3FexfT94D5wB+CVV2AZ8IKSkTSSLwd2erATjvx9kF8HxgG7AVw9w+A48IKSkTSULw1BCWJtBFvgvinux8qWzCzpkRGUouIfKU2SUKJIuXFmyBeMbN/A442szHAU8DC8MISkbRVm7ET+W1heo9Qw5G6izdB/AQoAt4FrgP+Bvx7WEGJSCMQb5I4uEs1ihRV411MZpYFrHH3U4GHwg9JRBqN2s4Wq7udUkqNNQh3LwHWm9mJDRCPiDRGtZ2uQ1JCvE1M7YA1ZrbYzBaUvWrayczGmtl6M9toZrfGKO8eHHO1mS01s67B+gFm9oaZrQnKvlu7yxKRlFPbvgkliqSLayS1mY2Itd7dX6lmnyxgAzAGKASWAxPdfW3UNk8Bz7r7HDMbBVzl7leY2cmRw/sHZnYCsALoHT1hYGUaSS2SZuJudlKTU5jq80S5FsD1wDeIdFA/7O7FcZ53CLDR3TcFx3ocuBBYG7VNNnBL8H4JweA7d99QtoG7f2JmnwGdgCoThIikmXj7J/RwoqSpqYlpDpBLJDl8G7i3FsfuAmyNWi4M1kVbBZQ9zvQioI2ZdYjewMyGAM2BDyufwMymmFmBmRUUFRXVIjQRSRkaYJeyakoQ2e5+ubv/ARgPDE/w+X8MjDCzd4g83nQbUFJWaGbHA/OIND2VVt7Z3We6e66753bq1CnBoYlIg1HfREqqKUEcLntTi6alMtuAblHLXYN15dz9E3e/2N0HAj8N1u0GMLNjgOeAn7r7m7U8t4ikm9p2YkvoakoQ/c1sb/D6Asgpe29me2vYdznQy8x6mllzYAJQ4c4nM+toZmUx3AbMCtY3B/4CzHX3+bW9KBFJY0oSKaPaBOHuWe5+TPBq4+5No94fU8O+xcBU4EUiU4U/6e5rzGyameUFm40kMsZiA9AZuCtY/y/AWcBkM1sZvAbU/TJFJK1ohtiUEPcDg1KdbnMVaaR0O2yoqrvNNd6BciIiyVGb2oQklBKEiKQHNTk1OCUIEUkf6sBuUEoQIpJe1IHdYJQgRCQ9qTYROiUIEUlf+XugbRxPIlCSqBMlCBFJbz96V3c5hUQJQkQaByWJhFOCEJHGI54ObHVex00JQkQaH9UmEkIJQkQaJyWJelOCEJHGS+Ml6kUJQkQaN42XqDMlCBFp/DThX50oQYhI5lCSqBUlCBHJLEoScVOCEJHME+94iQynBCEimUuD6qqlBCEimU1NTlVSghARUZKISQlCRASUJGIINUGY2VgzW29mG83s1hjl3c1ssZmtNrOlZtY1qmySmX0QvCaFGaeICKAkUYm5ezgHNssCNgBjgEJgOTDR3ddGbfMU8Ky7zzGzUcBV7n6FmbUHCoBcwIEVwGB331XV+XJzc72goCCUaxGRDBRPIoh3lHYKM7MV7p4bqyzMGsQQYKO7b3L3Q8DjwIWVtskGXg7eL4kqPxd4yd13BknhJWBsiLGKiFSk2kSoCaILsDVquTBYF20VcHHw/iKgjZl1iHNfzGyKmRWYWUFRUVHCAhcRATI+SSS7k/rHwAgzewcYAWwDSuLd2d1nunuuu+d26tQprBhFJJNlcJIIM0FsA7pFLXcN1pVz90/c/WJ3Hwj8NFi3O559RUQaTIYmiTATxHKgl5n1NLPmwARgQfQGZtbRzMpiuA2YFbx/ETjHzNqZWTvgnGCdiEhyZOD0HKElCHcvBqYS+WJfBzzp7mvMbJqZ5QWbjQTWm9kGoDNwV7DvTuAXRJLMcmBasE5EJLkyKEmEdptrQ9NtriLSoGpKBGlyC2yybnMVEWm84pror13DxBISJQgRkbqqsZZQmtZNTkoQIiL10YjvcFKCEBGprzTpb6gtJQgRkURohA8fUoIQEUmURjZWQglCRCTRThhcfXmaJAklCBGRRJvycs3bpEGSUIIQEQlDI+i4VoIQEQlLTX0SKV6LUIIQEQlbmiYJJQgRkWRL0SShBCEi0hDSsE9CCUJEpKGkWVOTEoSISENKoyShBCEikkp+2TXZEZRTghARaWjV1SIOfQH39Wu4WKqhBCEikgzVJYk9HzdcHNVQghARSZYU749QghARSaYUThJKECIiqSyJ/RGhJggzG2tm681so5ndGqP8RDNbYmbvmNlqMzsvWN/MzOaY2btmts7MbgszThGRpErR/ojQEoSZZQEzgG8D2cBEM8uutNm/A0+6+0BgAvD7YP2lwFHu3g8YDFxnZj3CilVEJOlSsKkpzJFNmKYAAAjrSURBVBrEEGCju29y90PA48CFlbZx4JjgfVvgk6j1rcysKXA0cAjYG2KsIiLJl2LTcYSZILoAW6OWC4N10fKBy82sEPgbcFOwfj7wJbAd+Bi4x913Vj6BmU0xswIzKygqKkpw+CIiKSQJtYhkd1JPBGa7e1fgPGCemTUhUvsoAU4AegL/amYnVd7Z3We6e66753bq1Kkh4xYRCUcKNTWFmSC2Ad2ilrsG66JdAzwJ4O5vAC2AjsD/AV5w98Pu/hnwOpAbYqwiIumhAZNEmAliOdDLzHqaWXMindALKm3zMTAawMx6E0kQRcH6UcH6VsAZwPshxioikjpSpC8itATh7sXAVOBFYB2Ru5XWmNk0M8sLNvtX4Htmtgp4DJjs7k7k7qfWZraGSKJ5xN1XhxWriEjKSYGmJot8H6e/3NxcLygoSHYYIiKJVV0ySEBNw8xWuHvMJvxkd1KLiEiKUoIQEUllSWxqUoIQEUl1Seq0VoIQEUlnIdYilCBERNJBEmoRShAiIukupFqEEoSISLpo4FqEEoSIiMSkBCEikk6qqkWE0MykBCEiIjEpQYiIpJtrXoq9PsG1CCUIEZF0021Ig5ymaYOcJQ189w9vHLFuXM7xXDG0BwcOlTD5kbePKB8/uCuX5nZj55eHuOHRFUeUX35Gdy7ofwKf7D7Aj55YeUT594afxNnZnfmwaB//9vS7R5TfNKoX3+zVkTWf7GHawrVHlP+/sacwuHt7Vny0k9+8sP6I8p9fkE2fE9ry2gef88DLHxxR/suL+/H1Tq35+9pPeejVTUeU3/fdAZxw7NEsXPUJj7750RHl/3X5YNq3as5TBVuZv6LwiPLZVw3h6OZZzHtjC8+u3n5E+RPXDQVg5rIPWbzuswplLZplMefqyH+C+xd/wOsbP69Q3q5lcx68YjAAv37hff7x0a4K5ce3bcF/ThgIwB0L17D2k4pPrD2pUyt+dXEOALc9vZpNRV9WKM8+4Rhuv6APAD98/B227zlYoXxQ93b8ZOypAFw/bwW79h+qUD7sGx25eXQvACbNepuDh0sqlI/ufRxTzvo6oM+ePnt1/+w5YEdcXeKoBiEiko7y9+BEkkRYNN23iEi6itXnUMuxEpruW0SkMaqcDBI8kE59ECIi6SzE0dWqQYiISExKECIiEpMShIiIxKQEISIiMSlBiIhITEoQIiISU6MZKGdmRcCRY/Lj1xH4vMatGpdMu+ZMu17QNWeK+lxzd3fvFKug0SSI+jKzgqpGEzZWmXbNmXa9oGvOFGFds5qYREQkJiUIERGJSQniKzOTHUASZNo1Z9r1gq45U4RyzeqDEBGRmFSDEBGRmJQgREQkpoxKEGY21szWm9lGM7s1RvlRZvZEUP6WmfVo+CgTK45rvsXM1prZajNbbGbdkxFnItV0zVHbXWJmbmZpf0tkPNdsZv8S/K7XmNmfGjrGRIvjs32imS0xs3eCz/d5yYgzUcxslpl9ZmbvVVFuZnZ/8PNYbWaD6n1Sd8+IF5AFfAicBDQHVgHZlba5EXgweD8BeCLZcTfANX8LaBm8vyETrjnYrg2wDHgTyE123A3we+4FvAO0C5aPS3bcDXDNM4EbgvfZwJZkx13Paz4LGAS8V0X5ecDzRB5TfQbwVn3PmUk1iCHARnff5O6HgMeBCyttcyEwJ3g/HxhtZmE+EzxsNV6zuy9x9/3B4ptA1waOMdHi+T0D/AL4NXAwRlm6ieeavwfMcPddAO7+WQPHmGjxXLMDxwTv2wKfNGB8Cefuy4Cd1WxyITDXI94EjjWz4+tzzkxKEF2ArVHLhcG6mNu4ezGwB+jQINGFI55rjnYNkb9A0lmN1xxUvbu5+3MNGViI4vk9nwycbGavm9mbZja2waILRzzXnA9cbmaFwN+AmxomtKSp7f/3GumRowKAmV0O5AIjkh1LmMysCfAfwOQkh9LQmhJpZhpJpJa4zMz6ufvupEYVronAbHe/18yGAvPMrK+7lyY7sHSRSTWIbUC3qOWuwbqY25hZUyLV0h0NEl044rlmzOxs4KdAnrv/s4FiC0tN19wG6AssNbMtRNpqF6R5R3U8v+dCYIG7H3b3zcAGIgkjXcVzzdcATwK4+xtACyKT2jVWcf1/r41MShDLgV5m1tPMmhPphF5QaZsFwKTg/XjgZQ96f9JUjddsZgOBPxBJDuneLg01XLO773H3ju7ew917EOl3yXP3guSEmxDxfLafIVJ7wMw6Emly2tSQQSZYPNf8MTAawMx6E0kQRQ0aZcNaAFwZ3M10BrDH3bfX54AZ08Tk7sVmNhV4kcgdELPcfY2ZTQMK3H0B8DCRauhGIp1BE5IXcf3Fec13A62Bp4L++I/dPS9pQddTnNfcqMR5zS8C55jZWqAE+L/unra14ziv+V+Bh8zsR0Q6rCen8x98ZvYYkSTfMehXuR1oBuDuDxLpZzkP2AjsB66q9znT+OclIiIhyqQmJhERqQUlCBERiUkJQkREYlKCEBGRmJQgREQkJiUIkVowsxIzW2lm75nZQjM7NsHH3xKMU8DM9iXy2CK1pQQhUjsH3H2Au/clMlbm+8kOSCQsShAidfcGwWRoZvZ1M3vBzFaY2atmdmqwvrOZ/cXMVgWvM4P1zwTbrjGzKUm8BpEqZcxIapFEMrMsItM4PBysmglc7+4fmNnpwO+BUcD9wCvuflGwT+tg+6vdfaeZHQ0sN7M/p/PIZmmclCBEaudoM1tJpOawDnjJzFoDZ/LVdCUARwX/jgKuBHD3EiJTyAPcbGYXBe+7EZk4TwlCUooShEjtHHD3AWbWksg8QN8HZgO73X1APAcws5HA2cBQd99vZkuJTCQnklLUByFSB8FT+G4mMiHcfmCzmV0K5c8G7h9supjIo1wxsywza0tkGvldQXI4lciU4yIpRwlCpI7c/R1gNZEH01wGXGNmq4A1fPX4yx8A3zKzd4EVRJ6N/ALQ1MzWAdOJTDkuknI0m6uIiMSkGoSIiMSkBCEiIjEpQYiISExKECIiEpMShIiIxKQEISIiMSlBiIhITP8LEMU6tvyQ2VsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Validation Loss: 0.53\n",
            "  Validation took: 0:14:18\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of  7,606.    Elapsed: 0:00:57.\n",
            "  Batch    80  of  7,606.    Elapsed: 0:01:54.\n",
            "  Batch   120  of  7,606.    Elapsed: 0:02:51.\n",
            "  Batch   160  of  7,606.    Elapsed: 0:03:48.\n",
            "  Batch   200  of  7,606.    Elapsed: 0:04:45.\n",
            "  Batch   240  of  7,606.    Elapsed: 0:05:43.\n",
            "  Batch   280  of  7,606.    Elapsed: 0:06:40.\n",
            "  Batch   320  of  7,606.    Elapsed: 0:07:37.\n",
            "  Batch   360  of  7,606.    Elapsed: 0:08:34.\n",
            "  Batch   400  of  7,606.    Elapsed: 0:09:31.\n",
            "  Batch   440  of  7,606.    Elapsed: 0:10:28.\n",
            "  Batch   480  of  7,606.    Elapsed: 0:11:25.\n",
            "  Batch   520  of  7,606.    Elapsed: 0:12:22.\n",
            "  Batch   560  of  7,606.    Elapsed: 0:13:19.\n",
            "  Batch   600  of  7,606.    Elapsed: 0:14:16.\n",
            "  Batch   640  of  7,606.    Elapsed: 0:15:13.\n",
            "  Batch   680  of  7,606.    Elapsed: 0:16:10.\n",
            "  Batch   720  of  7,606.    Elapsed: 0:17:07.\n",
            "  Batch   760  of  7,606.    Elapsed: 0:18:04.\n",
            "  Batch   800  of  7,606.    Elapsed: 0:19:01.\n",
            "  Batch   840  of  7,606.    Elapsed: 0:19:58.\n",
            "  Batch   880  of  7,606.    Elapsed: 0:20:55.\n",
            "  Batch   920  of  7,606.    Elapsed: 0:21:52.\n",
            "  Batch   960  of  7,606.    Elapsed: 0:22:49.\n",
            "  Batch 1,000  of  7,606.    Elapsed: 0:23:47.\n",
            "  Batch 1,040  of  7,606.    Elapsed: 0:24:44.\n",
            "  Batch 1,080  of  7,606.    Elapsed: 0:25:41.\n",
            "  Batch 1,120  of  7,606.    Elapsed: 0:26:38.\n",
            "  Batch 1,160  of  7,606.    Elapsed: 0:27:35.\n",
            "  Batch 1,200  of  7,606.    Elapsed: 0:28:32.\n",
            "  Batch 1,240  of  7,606.    Elapsed: 0:29:29.\n",
            "  Batch 1,280  of  7,606.    Elapsed: 0:30:26.\n",
            "  Batch 1,320  of  7,606.    Elapsed: 0:31:23.\n",
            "  Batch 1,360  of  7,606.    Elapsed: 0:32:20.\n",
            "  Batch 1,400  of  7,606.    Elapsed: 0:33:17.\n",
            "  Batch 1,440  of  7,606.    Elapsed: 0:34:14.\n",
            "  Batch 1,480  of  7,606.    Elapsed: 0:35:11.\n",
            "  Batch 1,520  of  7,606.    Elapsed: 0:36:08.\n",
            "  Batch 1,560  of  7,606.    Elapsed: 0:37:05.\n",
            "  Batch 1,600  of  7,606.    Elapsed: 0:38:02.\n",
            "  Batch 1,640  of  7,606.    Elapsed: 0:38:59.\n",
            "  Batch 1,680  of  7,606.    Elapsed: 0:39:56.\n",
            "  Batch 1,720  of  7,606.    Elapsed: 0:40:53.\n",
            "  Batch 1,760  of  7,606.    Elapsed: 0:41:50.\n",
            "  Batch 1,800  of  7,606.    Elapsed: 0:42:48.\n",
            "  Batch 1,840  of  7,606.    Elapsed: 0:43:45.\n",
            "  Batch 1,880  of  7,606.    Elapsed: 0:44:42.\n",
            "  Batch 1,920  of  7,606.    Elapsed: 0:45:39.\n",
            "  Batch 1,960  of  7,606.    Elapsed: 0:46:36.\n",
            "  Batch 2,000  of  7,606.    Elapsed: 0:47:33.\n",
            "  Batch 2,040  of  7,606.    Elapsed: 0:48:30.\n",
            "  Batch 2,080  of  7,606.    Elapsed: 0:49:27.\n",
            "  Batch 2,120  of  7,606.    Elapsed: 0:50:24.\n",
            "  Batch 2,160  of  7,606.    Elapsed: 0:51:21.\n",
            "  Batch 2,200  of  7,606.    Elapsed: 0:52:18.\n",
            "  Batch 2,240  of  7,606.    Elapsed: 0:53:15.\n",
            "  Batch 2,280  of  7,606.    Elapsed: 0:54:12.\n",
            "  Batch 2,320  of  7,606.    Elapsed: 0:55:09.\n",
            "  Batch 2,360  of  7,606.    Elapsed: 0:56:06.\n",
            "  Batch 2,400  of  7,606.    Elapsed: 0:57:03.\n",
            "  Batch 2,440  of  7,606.    Elapsed: 0:58:00.\n",
            "  Batch 2,480  of  7,606.    Elapsed: 0:58:57.\n",
            "  Batch 2,520  of  7,606.    Elapsed: 0:59:54.\n",
            "  Batch 2,560  of  7,606.    Elapsed: 1:00:51.\n",
            "  Batch 2,600  of  7,606.    Elapsed: 1:01:49.\n",
            "  Batch 2,640  of  7,606.    Elapsed: 1:02:46.\n",
            "  Batch 2,680  of  7,606.    Elapsed: 1:03:43.\n",
            "  Batch 2,720  of  7,606.    Elapsed: 1:04:40.\n",
            "  Batch 2,760  of  7,606.    Elapsed: 1:05:37.\n",
            "  Batch 2,800  of  7,606.    Elapsed: 1:06:34.\n",
            "  Batch 2,840  of  7,606.    Elapsed: 1:07:31.\n",
            "  Batch 2,880  of  7,606.    Elapsed: 1:08:28.\n",
            "  Batch 2,920  of  7,606.    Elapsed: 1:09:25.\n",
            "  Batch 2,960  of  7,606.    Elapsed: 1:10:22.\n",
            "  Batch 3,000  of  7,606.    Elapsed: 1:11:19.\n",
            "  Batch 3,040  of  7,606.    Elapsed: 1:12:16.\n",
            "  Batch 3,080  of  7,606.    Elapsed: 1:13:13.\n",
            "  Batch 3,120  of  7,606.    Elapsed: 1:14:10.\n",
            "  Batch 3,160  of  7,606.    Elapsed: 1:15:07.\n",
            "  Batch 3,200  of  7,606.    Elapsed: 1:16:04.\n",
            "  Batch 3,240  of  7,606.    Elapsed: 1:17:01.\n",
            "  Batch 3,280  of  7,606.    Elapsed: 1:17:58.\n",
            "  Batch 3,320  of  7,606.    Elapsed: 1:18:55.\n",
            "  Batch 3,360  of  7,606.    Elapsed: 1:19:53.\n",
            "  Batch 3,400  of  7,606.    Elapsed: 1:20:50.\n",
            "  Batch 3,440  of  7,606.    Elapsed: 1:21:47.\n",
            "  Batch 3,480  of  7,606.    Elapsed: 1:22:44.\n",
            "  Batch 3,520  of  7,606.    Elapsed: 1:23:41.\n",
            "  Batch 3,560  of  7,606.    Elapsed: 1:24:38.\n",
            "  Batch 3,600  of  7,606.    Elapsed: 1:25:35.\n",
            "  Batch 3,640  of  7,606.    Elapsed: 1:26:32.\n",
            "  Batch 3,680  of  7,606.    Elapsed: 1:27:29.\n",
            "  Batch 3,720  of  7,606.    Elapsed: 1:28:26.\n",
            "  Batch 3,760  of  7,606.    Elapsed: 1:29:23.\n",
            "  Batch 3,800  of  7,606.    Elapsed: 1:30:20.\n",
            "  Batch 3,840  of  7,606.    Elapsed: 1:31:17.\n",
            "  Batch 3,880  of  7,606.    Elapsed: 1:32:14.\n",
            "  Batch 3,920  of  7,606.    Elapsed: 1:33:11.\n",
            "  Batch 3,960  of  7,606.    Elapsed: 1:34:08.\n",
            "  Batch 4,000  of  7,606.    Elapsed: 1:35:05.\n",
            "  Batch 4,040  of  7,606.    Elapsed: 1:36:02.\n",
            "  Batch 4,080  of  7,606.    Elapsed: 1:36:59.\n",
            "  Batch 4,120  of  7,606.    Elapsed: 1:37:56.\n",
            "  Batch 4,160  of  7,606.    Elapsed: 1:38:53.\n",
            "  Batch 4,200  of  7,606.    Elapsed: 1:39:50.\n",
            "  Batch 4,240  of  7,606.    Elapsed: 1:40:48.\n",
            "  Batch 4,280  of  7,606.    Elapsed: 1:41:45.\n",
            "  Batch 4,320  of  7,606.    Elapsed: 1:42:42.\n",
            "  Batch 4,360  of  7,606.    Elapsed: 1:43:39.\n",
            "  Batch 4,400  of  7,606.    Elapsed: 1:44:36.\n",
            "  Batch 4,440  of  7,606.    Elapsed: 1:45:33.\n",
            "  Batch 4,480  of  7,606.    Elapsed: 1:46:30.\n",
            "  Batch 4,520  of  7,606.    Elapsed: 1:47:27.\n",
            "  Batch 4,560  of  7,606.    Elapsed: 1:48:24.\n",
            "  Batch 4,600  of  7,606.    Elapsed: 1:49:21.\n",
            "  Batch 4,640  of  7,606.    Elapsed: 1:50:18.\n",
            "  Batch 4,680  of  7,606.    Elapsed: 1:51:15.\n",
            "  Batch 4,720  of  7,606.    Elapsed: 1:52:12.\n",
            "  Batch 4,760  of  7,606.    Elapsed: 1:53:09.\n",
            "  Batch 4,800  of  7,606.    Elapsed: 1:54:06.\n",
            "  Batch 4,840  of  7,606.    Elapsed: 1:55:03.\n",
            "  Batch 4,880  of  7,606.    Elapsed: 1:56:00.\n",
            "  Batch 4,920  of  7,606.    Elapsed: 1:56:57.\n",
            "  Batch 4,960  of  7,606.    Elapsed: 1:57:54.\n",
            "  Batch 5,000  of  7,606.    Elapsed: 1:58:52.\n",
            "  Batch 5,040  of  7,606.    Elapsed: 1:59:49.\n",
            "  Batch 5,080  of  7,606.    Elapsed: 2:00:46.\n",
            "  Batch 5,120  of  7,606.    Elapsed: 2:01:43.\n",
            "  Batch 5,160  of  7,606.    Elapsed: 2:02:40.\n",
            "  Batch 5,200  of  7,606.    Elapsed: 2:03:37.\n",
            "  Batch 5,240  of  7,606.    Elapsed: 2:04:34.\n",
            "  Batch 5,280  of  7,606.    Elapsed: 2:05:31.\n",
            "  Batch 5,320  of  7,606.    Elapsed: 2:06:28.\n",
            "  Batch 5,360  of  7,606.    Elapsed: 2:07:25.\n",
            "  Batch 5,400  of  7,606.    Elapsed: 2:08:22.\n",
            "  Batch 5,440  of  7,606.    Elapsed: 2:09:19.\n",
            "  Batch 5,480  of  7,606.    Elapsed: 2:10:16.\n",
            "  Batch 5,520  of  7,606.    Elapsed: 2:11:13.\n",
            "  Batch 5,560  of  7,606.    Elapsed: 2:12:10.\n",
            "  Batch 5,600  of  7,606.    Elapsed: 2:13:07.\n",
            "  Batch 5,640  of  7,606.    Elapsed: 2:14:04.\n",
            "  Batch 5,680  of  7,606.    Elapsed: 2:15:01.\n",
            "  Batch 5,720  of  7,606.    Elapsed: 2:15:58.\n",
            "  Batch 5,760  of  7,606.    Elapsed: 2:16:55.\n",
            "  Batch 5,800  of  7,606.    Elapsed: 2:17:52.\n",
            "  Batch 5,840  of  7,606.    Elapsed: 2:18:50.\n",
            "  Batch 5,880  of  7,606.    Elapsed: 2:19:47.\n",
            "  Batch 5,920  of  7,606.    Elapsed: 2:20:44.\n",
            "  Batch 5,960  of  7,606.    Elapsed: 2:21:41.\n",
            "  Batch 6,000  of  7,606.    Elapsed: 2:22:38.\n",
            "  Batch 6,040  of  7,606.    Elapsed: 2:23:35.\n",
            "  Batch 6,080  of  7,606.    Elapsed: 2:24:32.\n",
            "  Batch 6,120  of  7,606.    Elapsed: 2:25:29.\n",
            "  Batch 6,160  of  7,606.    Elapsed: 2:26:26.\n",
            "  Batch 6,200  of  7,606.    Elapsed: 2:27:23.\n",
            "  Batch 6,240  of  7,606.    Elapsed: 2:28:20.\n",
            "  Batch 6,280  of  7,606.    Elapsed: 2:29:17.\n",
            "  Batch 6,320  of  7,606.    Elapsed: 2:30:14.\n",
            "  Batch 6,360  of  7,606.    Elapsed: 2:31:11.\n",
            "  Batch 6,400  of  7,606.    Elapsed: 2:32:08.\n",
            "  Batch 6,440  of  7,606.    Elapsed: 2:33:05.\n",
            "  Batch 6,480  of  7,606.    Elapsed: 2:34:02.\n",
            "  Batch 6,520  of  7,606.    Elapsed: 2:34:59.\n",
            "  Batch 6,560  of  7,606.    Elapsed: 2:35:56.\n",
            "  Batch 6,600  of  7,606.    Elapsed: 2:36:53.\n",
            "  Batch 6,640  of  7,606.    Elapsed: 2:37:51.\n",
            "  Batch 6,680  of  7,606.    Elapsed: 2:38:48.\n",
            "  Batch 6,720  of  7,606.    Elapsed: 2:39:45.\n",
            "  Batch 6,760  of  7,606.    Elapsed: 2:40:42.\n",
            "  Batch 6,800  of  7,606.    Elapsed: 2:41:39.\n",
            "  Batch 6,840  of  7,606.    Elapsed: 2:42:36.\n",
            "  Batch 6,880  of  7,606.    Elapsed: 2:43:33.\n",
            "  Batch 6,920  of  7,606.    Elapsed: 2:44:30.\n",
            "  Batch 6,960  of  7,606.    Elapsed: 2:45:27.\n",
            "  Batch 7,000  of  7,606.    Elapsed: 2:46:24.\n",
            "  Batch 7,040  of  7,606.    Elapsed: 2:47:21.\n",
            "  Batch 7,080  of  7,606.    Elapsed: 2:48:18.\n",
            "  Batch 7,120  of  7,606.    Elapsed: 2:49:15.\n",
            "  Batch 7,160  of  7,606.    Elapsed: 2:50:12.\n",
            "  Batch 7,200  of  7,606.    Elapsed: 2:51:09.\n",
            "  Batch 7,240  of  7,606.    Elapsed: 2:52:06.\n",
            "  Batch 7,280  of  7,606.    Elapsed: 2:53:03.\n",
            "  Batch 7,320  of  7,606.    Elapsed: 2:54:00.\n",
            "  Batch 7,360  of  7,606.    Elapsed: 2:54:57.\n",
            "  Batch 7,400  of  7,606.    Elapsed: 2:55:55.\n",
            "  Batch 7,440  of  7,606.    Elapsed: 2:56:52.\n",
            "  Batch 7,480  of  7,606.    Elapsed: 2:57:49.\n",
            "  Batch 7,520  of  7,606.    Elapsed: 2:58:46.\n",
            "  Batch 7,560  of  7,606.    Elapsed: 2:59:43.\n",
            "  Batch 7,600  of  7,606.    Elapsed: 3:00:40.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 3:00:48\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.87\n",
            "  F1 score: 0.55\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.23      0.73      0.35     16168\n",
            "           1       0.94      0.63      0.75    105524\n",
            "\n",
            "    accuracy                           0.64    121692\n",
            "   macro avg       0.58      0.68      0.55    121692\n",
            "weighted avg       0.84      0.64      0.70    121692\n",
            "\n",
            "[[11834  4334]\n",
            " [39554 65970]]\n",
            "Mathews Correlation coefficient score for this epoch is : 0.25\n",
            "Model validation score: f1=0.750 auc=0.945\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Validation Loss: 0.56\n",
            "  Validation took: 0:14:18\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of  7,606.    Elapsed: 0:00:57.\n",
            "  Batch    80  of  7,606.    Elapsed: 0:01:54.\n",
            "  Batch   120  of  7,606.    Elapsed: 0:02:51.\n",
            "  Batch   160  of  7,606.    Elapsed: 0:03:48.\n",
            "  Batch   200  of  7,606.    Elapsed: 0:04:45.\n",
            "  Batch   240  of  7,606.    Elapsed: 0:05:43.\n",
            "  Batch   280  of  7,606.    Elapsed: 0:06:40.\n",
            "  Batch   320  of  7,606.    Elapsed: 0:07:37.\n",
            "  Batch   360  of  7,606.    Elapsed: 0:08:34.\n",
            "  Batch   400  of  7,606.    Elapsed: 0:09:31.\n",
            "  Batch   440  of  7,606.    Elapsed: 0:10:28.\n",
            "  Batch   480  of  7,606.    Elapsed: 0:11:25.\n",
            "  Batch   520  of  7,606.    Elapsed: 0:12:22.\n",
            "  Batch   560  of  7,606.    Elapsed: 0:13:19.\n",
            "  Batch   600  of  7,606.    Elapsed: 0:14:16.\n",
            "  Batch   640  of  7,606.    Elapsed: 0:15:13.\n",
            "  Batch   680  of  7,606.    Elapsed: 0:16:10.\n",
            "  Batch   720  of  7,606.    Elapsed: 0:17:07.\n",
            "  Batch   760  of  7,606.    Elapsed: 0:18:04.\n",
            "  Batch   800  of  7,606.    Elapsed: 0:19:01.\n",
            "  Batch   840  of  7,606.    Elapsed: 0:19:58.\n",
            "  Batch   880  of  7,606.    Elapsed: 0:20:55.\n",
            "  Batch   920  of  7,606.    Elapsed: 0:21:53.\n",
            "  Batch   960  of  7,606.    Elapsed: 0:22:50.\n",
            "  Batch 1,000  of  7,606.    Elapsed: 0:23:47.\n",
            "  Batch 1,040  of  7,606.    Elapsed: 0:24:44.\n",
            "  Batch 1,080  of  7,606.    Elapsed: 0:25:41.\n",
            "  Batch 1,120  of  7,606.    Elapsed: 0:26:38.\n",
            "  Batch 1,160  of  7,606.    Elapsed: 0:27:35.\n",
            "  Batch 1,200  of  7,606.    Elapsed: 0:28:32.\n",
            "  Batch 1,240  of  7,606.    Elapsed: 0:29:29.\n",
            "  Batch 1,280  of  7,606.    Elapsed: 0:30:26.\n",
            "  Batch 1,320  of  7,606.    Elapsed: 0:31:23.\n",
            "  Batch 1,360  of  7,606.    Elapsed: 0:32:20.\n",
            "  Batch 1,400  of  7,606.    Elapsed: 0:33:17.\n",
            "  Batch 1,440  of  7,606.    Elapsed: 0:34:14.\n",
            "  Batch 1,480  of  7,606.    Elapsed: 0:35:11.\n",
            "  Batch 1,520  of  7,606.    Elapsed: 0:36:08.\n",
            "  Batch 1,560  of  7,606.    Elapsed: 0:37:05.\n",
            "  Batch 1,600  of  7,606.    Elapsed: 0:38:02.\n",
            "  Batch 1,640  of  7,606.    Elapsed: 0:38:59.\n",
            "  Batch 1,680  of  7,606.    Elapsed: 0:39:57.\n",
            "  Batch 1,720  of  7,606.    Elapsed: 0:40:54.\n",
            "  Batch 1,760  of  7,606.    Elapsed: 0:41:51.\n",
            "  Batch 1,800  of  7,606.    Elapsed: 0:42:48.\n",
            "  Batch 1,840  of  7,606.    Elapsed: 0:43:45.\n",
            "  Batch 1,880  of  7,606.    Elapsed: 0:44:42.\n",
            "  Batch 1,920  of  7,606.    Elapsed: 0:45:39.\n",
            "  Batch 1,960  of  7,606.    Elapsed: 0:46:36.\n",
            "  Batch 2,000  of  7,606.    Elapsed: 0:47:33.\n",
            "  Batch 2,040  of  7,606.    Elapsed: 0:48:30.\n",
            "  Batch 2,080  of  7,606.    Elapsed: 0:49:27.\n",
            "  Batch 2,120  of  7,606.    Elapsed: 0:50:24.\n",
            "  Batch 2,160  of  7,606.    Elapsed: 0:51:21.\n",
            "  Batch 2,200  of  7,606.    Elapsed: 0:52:18.\n",
            "  Batch 2,240  of  7,606.    Elapsed: 0:53:15.\n",
            "  Batch 2,280  of  7,606.    Elapsed: 0:54:12.\n",
            "  Batch 2,320  of  7,606.    Elapsed: 0:55:09.\n",
            "  Batch 2,360  of  7,606.    Elapsed: 0:56:06.\n",
            "  Batch 2,400  of  7,606.    Elapsed: 0:57:03.\n",
            "  Batch 2,440  of  7,606.    Elapsed: 0:58:00.\n",
            "  Batch 2,480  of  7,606.    Elapsed: 0:58:58.\n",
            "  Batch 2,520  of  7,606.    Elapsed: 0:59:55.\n",
            "  Batch 2,560  of  7,606.    Elapsed: 1:00:52.\n",
            "  Batch 2,600  of  7,606.    Elapsed: 1:01:49.\n",
            "  Batch 2,640  of  7,606.    Elapsed: 1:02:46.\n",
            "  Batch 2,680  of  7,606.    Elapsed: 1:03:43.\n",
            "  Batch 2,720  of  7,606.    Elapsed: 1:04:40.\n",
            "  Batch 2,760  of  7,606.    Elapsed: 1:05:37.\n",
            "  Batch 2,800  of  7,606.    Elapsed: 1:06:34.\n",
            "  Batch 2,840  of  7,606.    Elapsed: 1:07:31.\n",
            "  Batch 2,880  of  7,606.    Elapsed: 1:08:28.\n",
            "  Batch 2,920  of  7,606.    Elapsed: 1:09:25.\n",
            "  Batch 2,960  of  7,606.    Elapsed: 1:10:22.\n",
            "  Batch 3,000  of  7,606.    Elapsed: 1:11:19.\n",
            "  Batch 3,040  of  7,606.    Elapsed: 1:12:16.\n",
            "  Batch 3,080  of  7,606.    Elapsed: 1:13:13.\n",
            "  Batch 3,120  of  7,606.    Elapsed: 1:14:10.\n",
            "  Batch 3,160  of  7,606.    Elapsed: 1:15:07.\n",
            "  Batch 3,200  of  7,606.    Elapsed: 1:16:04.\n",
            "  Batch 3,240  of  7,606.    Elapsed: 1:17:01.\n",
            "  Batch 3,280  of  7,606.    Elapsed: 1:17:59.\n",
            "  Batch 3,320  of  7,606.    Elapsed: 1:18:56.\n",
            "  Batch 3,360  of  7,606.    Elapsed: 1:19:53.\n",
            "  Batch 3,400  of  7,606.    Elapsed: 1:20:50.\n",
            "  Batch 3,440  of  7,606.    Elapsed: 1:21:47.\n",
            "  Batch 3,480  of  7,606.    Elapsed: 1:22:44.\n",
            "  Batch 3,520  of  7,606.    Elapsed: 1:23:41.\n",
            "  Batch 3,560  of  7,606.    Elapsed: 1:24:38.\n",
            "  Batch 3,600  of  7,606.    Elapsed: 1:25:35.\n",
            "  Batch 3,640  of  7,606.    Elapsed: 1:26:32.\n",
            "  Batch 3,680  of  7,606.    Elapsed: 1:27:29.\n",
            "  Batch 3,720  of  7,606.    Elapsed: 1:28:26.\n",
            "  Batch 3,760  of  7,606.    Elapsed: 1:29:23.\n",
            "  Batch 3,800  of  7,606.    Elapsed: 1:30:20.\n",
            "  Batch 3,840  of  7,606.    Elapsed: 1:31:17.\n",
            "  Batch 3,880  of  7,606.    Elapsed: 1:32:14.\n",
            "  Batch 3,920  of  7,606.    Elapsed: 1:33:11.\n",
            "  Batch 3,960  of  7,606.    Elapsed: 1:34:08.\n",
            "  Batch 4,000  of  7,606.    Elapsed: 1:35:06.\n",
            "  Batch 4,040  of  7,606.    Elapsed: 1:36:03.\n",
            "  Batch 4,080  of  7,606.    Elapsed: 1:37:00.\n",
            "  Batch 4,120  of  7,606.    Elapsed: 1:37:57.\n",
            "  Batch 4,160  of  7,606.    Elapsed: 1:38:54.\n",
            "  Batch 4,200  of  7,606.    Elapsed: 1:39:51.\n",
            "  Batch 4,240  of  7,606.    Elapsed: 1:40:48.\n",
            "  Batch 4,280  of  7,606.    Elapsed: 1:41:45.\n",
            "  Batch 4,320  of  7,606.    Elapsed: 1:42:42.\n",
            "  Batch 4,360  of  7,606.    Elapsed: 1:43:39.\n",
            "  Batch 4,400  of  7,606.    Elapsed: 1:44:36.\n",
            "  Batch 4,440  of  7,606.    Elapsed: 1:45:33.\n",
            "  Batch 4,480  of  7,606.    Elapsed: 1:46:30.\n",
            "  Batch 4,520  of  7,606.    Elapsed: 1:47:27.\n",
            "  Batch 4,560  of  7,606.    Elapsed: 1:48:24.\n",
            "  Batch 4,600  of  7,606.    Elapsed: 1:49:21.\n",
            "  Batch 4,640  of  7,606.    Elapsed: 1:50:18.\n",
            "  Batch 4,680  of  7,606.    Elapsed: 1:51:16.\n",
            "  Batch 4,720  of  7,606.    Elapsed: 1:52:13.\n",
            "  Batch 4,760  of  7,606.    Elapsed: 1:53:10.\n",
            "  Batch 4,800  of  7,606.    Elapsed: 1:54:07.\n",
            "  Batch 4,840  of  7,606.    Elapsed: 1:55:04.\n",
            "  Batch 4,880  of  7,606.    Elapsed: 1:56:01.\n",
            "  Batch 4,920  of  7,606.    Elapsed: 1:56:58.\n",
            "  Batch 4,960  of  7,606.    Elapsed: 1:57:55.\n",
            "  Batch 5,000  of  7,606.    Elapsed: 1:58:52.\n",
            "  Batch 5,040  of  7,606.    Elapsed: 1:59:49.\n",
            "  Batch 5,080  of  7,606.    Elapsed: 2:00:46.\n",
            "  Batch 5,120  of  7,606.    Elapsed: 2:01:43.\n",
            "  Batch 5,160  of  7,606.    Elapsed: 2:02:40.\n",
            "  Batch 5,200  of  7,606.    Elapsed: 2:03:37.\n",
            "  Batch 5,240  of  7,606.    Elapsed: 2:04:34.\n",
            "  Batch 5,280  of  7,606.    Elapsed: 2:05:31.\n",
            "  Batch 5,320  of  7,606.    Elapsed: 2:06:28.\n",
            "  Batch 5,360  of  7,606.    Elapsed: 2:07:25.\n",
            "  Batch 5,400  of  7,606.    Elapsed: 2:08:22.\n",
            "  Batch 5,440  of  7,606.    Elapsed: 2:09:20.\n",
            "  Batch 5,480  of  7,606.    Elapsed: 2:10:17.\n",
            "  Batch 5,520  of  7,606.    Elapsed: 2:11:14.\n",
            "  Batch 5,560  of  7,606.    Elapsed: 2:12:11.\n",
            "  Batch 5,600  of  7,606.    Elapsed: 2:13:08.\n",
            "  Batch 5,640  of  7,606.    Elapsed: 2:14:05.\n",
            "  Batch 5,680  of  7,606.    Elapsed: 2:15:02.\n",
            "  Batch 5,720  of  7,606.    Elapsed: 2:15:59.\n",
            "  Batch 5,760  of  7,606.    Elapsed: 2:16:56.\n",
            "  Batch 5,800  of  7,606.    Elapsed: 2:17:53.\n",
            "  Batch 5,840  of  7,606.    Elapsed: 2:18:50.\n",
            "  Batch 5,880  of  7,606.    Elapsed: 2:19:47.\n",
            "  Batch 5,920  of  7,606.    Elapsed: 2:20:44.\n",
            "  Batch 5,960  of  7,606.    Elapsed: 2:21:41.\n",
            "  Batch 6,000  of  7,606.    Elapsed: 2:22:38.\n",
            "  Batch 6,040  of  7,606.    Elapsed: 2:23:35.\n",
            "  Batch 6,080  of  7,606.    Elapsed: 2:24:32.\n",
            "  Batch 6,120  of  7,606.    Elapsed: 2:25:29.\n",
            "  Batch 6,160  of  7,606.    Elapsed: 2:26:27.\n",
            "  Batch 6,200  of  7,606.    Elapsed: 2:27:24.\n",
            "  Batch 6,240  of  7,606.    Elapsed: 2:28:21.\n",
            "  Batch 6,280  of  7,606.    Elapsed: 2:29:18.\n",
            "  Batch 6,320  of  7,606.    Elapsed: 2:30:15.\n",
            "  Batch 6,360  of  7,606.    Elapsed: 2:31:12.\n",
            "  Batch 6,400  of  7,606.    Elapsed: 2:32:09.\n",
            "  Batch 6,440  of  7,606.    Elapsed: 2:33:06.\n",
            "  Batch 6,480  of  7,606.    Elapsed: 2:34:03.\n",
            "  Batch 6,520  of  7,606.    Elapsed: 2:35:00.\n",
            "  Batch 6,560  of  7,606.    Elapsed: 2:35:57.\n",
            "  Batch 6,600  of  7,606.    Elapsed: 2:36:54.\n",
            "  Batch 6,640  of  7,606.    Elapsed: 2:37:51.\n",
            "  Batch 6,680  of  7,606.    Elapsed: 2:38:48.\n",
            "  Batch 6,720  of  7,606.    Elapsed: 2:39:45.\n",
            "  Batch 6,760  of  7,606.    Elapsed: 2:40:42.\n",
            "  Batch 6,800  of  7,606.    Elapsed: 2:41:39.\n",
            "  Batch 6,840  of  7,606.    Elapsed: 2:42:36.\n",
            "  Batch 6,880  of  7,606.    Elapsed: 2:43:34.\n",
            "  Batch 6,920  of  7,606.    Elapsed: 2:44:31.\n",
            "  Batch 6,960  of  7,606.    Elapsed: 2:45:28.\n",
            "  Batch 7,000  of  7,606.    Elapsed: 2:46:25.\n",
            "  Batch 7,040  of  7,606.    Elapsed: 2:47:22.\n",
            "  Batch 7,080  of  7,606.    Elapsed: 2:48:19.\n",
            "  Batch 7,120  of  7,606.    Elapsed: 2:49:16.\n",
            "  Batch 7,160  of  7,606.    Elapsed: 2:50:13.\n",
            "  Batch 7,200  of  7,606.    Elapsed: 2:51:10.\n",
            "  Batch 7,240  of  7,606.    Elapsed: 2:52:07.\n",
            "  Batch 7,280  of  7,606.    Elapsed: 2:53:04.\n",
            "  Batch 7,320  of  7,606.    Elapsed: 2:54:01.\n",
            "  Batch 7,360  of  7,606.    Elapsed: 2:54:58.\n",
            "  Batch 7,400  of  7,606.    Elapsed: 2:55:55.\n",
            "  Batch 7,440  of  7,606.    Elapsed: 2:56:52.\n",
            "  Batch 7,480  of  7,606.    Elapsed: 2:57:49.\n",
            "  Batch 7,520  of  7,606.    Elapsed: 2:58:46.\n",
            "  Batch 7,560  of  7,606.    Elapsed: 2:59:43.\n",
            "  Batch 7,600  of  7,606.    Elapsed: 3:00:40.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 3:00:49\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.87\n",
            "  F1 score: 0.60\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.28      0.57      0.38     16168\n",
            "           1       0.92      0.78      0.84    105524\n",
            "\n",
            "    accuracy                           0.75    121692\n",
            "   macro avg       0.60      0.67      0.61    121692\n",
            "weighted avg       0.84      0.75      0.78    121692\n",
            "\n",
            "[[ 9187  6981]\n",
            " [23461 82063]]\n",
            "Mathews Correlation coefficient score for this epoch is : 0.26\n",
            "Model validation score: f1=0.844 auc=0.947\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Validation Loss: 0.54\n",
            "  Validation took: 0:14:18\n",
            "\n",
            "Training complete!\n",
            "Total training took 9:45:18 (h:mm:ss)\n"
          ]
        }
      ],
      "source": [
        " #Using pos weight of loss_fn\n",
        "from sklearn.metrics import classification_report,confusion_matrix,f1_score,auc,precision_recall_curve,plot_precision_recall_curve,matthews_corrcoef\n",
        "import random\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "scaler = GradScaler()\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    roberta_model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        roberta_model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        with autocast():\n",
        "            loss, logits = roberta_model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "            b_labels1=torch.nn.functional.one_hot(b_labels, num_classes=2)\n",
        "            #Calculating weights\n",
        "            #positive=torch.sum(b_labels1, dim=0)\n",
        "           # negative=len(b_labels1)-positive\n",
        "            #negative\n",
        "            #pos_weight  = positive / negative\n",
        "            #criterion.pos_weight = pos_weight\n",
        "            loss1 = loss_fn1(logits,b_labels1).to(device)\n",
        "           # print(\"loss:\",loss1)\n",
        "            loss1 = loss1 / gradient_accumulations\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss1.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        scaler.scale(loss1).backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "       # torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm*scaler.get_scale())\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        if ((step + 1) % gradient_accumulations == 0):\n",
        "             scaler.step(optimizer)\n",
        "       # Updates the scale for next iteration.\n",
        "             scaler.update()\n",
        "        # Update the learning rate.\n",
        "             scheduler.step()       \n",
        "             optimizer.zero_grad()\n",
        "            # roberta_model.zero_grad()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    roberta_model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    total_f1_score =0\n",
        "    predlist =[]\n",
        "    lbllist =[]\n",
        "    total_logits =[]\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            (loss, logits) = roberta_model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            #Converting the labels to one hot to sync with same shape as logits\n",
        "            b_labels1=torch.nn.functional.one_hot(b_labels, num_classes=2)\n",
        "            loss1 = loss_fn(logits, b_labels1)\n",
        "       # print(\"loss1:\",loss1)\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss1.item()\n",
        "\n",
        "         #Converting for predictions by applying sigmoid to logits\n",
        "        pred_logits_sigmoid=torch.sigmoid(logits)\n",
        "        y_pred=torch.round(pred_logits_sigmoid)\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits_pred = y_pred.detach().cpu().numpy()\n",
        "        label_ids1 = b_labels.to('cpu').numpy()\n",
        "        logits=logits.detach().cpu().numpy()\n",
        "        #For confusion matrix and classification report to work we need same dimensions.\n",
        "        label_ids = b_labels1.to('cpu').numpy()\n",
        "        pred_logits_sigmoid = pred_logits_sigmoid.detach().cpu().numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids1)\n",
        "\n",
        "         #print(predictions)\n",
        "        predictions=np.argmax(logits_pred, axis=1)\n",
        "        y_test=np.argmax(label_ids,axis=1)\n",
        "        predlist.extend(predictions)\n",
        "        lbllist.extend(y_test)\n",
        "        #Accumulating the sigmoid positive logits for precision recall curve\n",
        "        total_logits.extend(pred_logits_sigmoid[:,1])\n",
        "        total_f1_score += f1_score(predictions,y_test, average = 'macro')\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    #f1 score\n",
        "\n",
        "    avg_f1_score =total_f1_score/len(validation_dataloader)\n",
        "    print(\"  F1 score: {0:.2f}\".format(avg_f1_score))\n",
        "\n",
        "     #classification report\n",
        "    print(classification_report(lbllist,predlist))  \n",
        "\n",
        "    #confusion matrix\n",
        "    cm = confusion_matrix(lbllist,predlist)\n",
        "    # constant for classes\n",
        "    print(cm)\n",
        "    #mcc score\n",
        "    print(\"Mathews Correlation coefficient score for this epoch is :\",round(matthews_corrcoef(lbllist, predlist),2))\n",
        "    #Precision recall curve plot\n",
        "    lr_precision, lr_recall, thresholds = precision_recall_curve(lbllist,total_logits)\n",
        "    lr_f1, lr_auc = f1_score( lbllist,predlist), auc(lr_recall, lr_precision)\n",
        "    print('Model validation score: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
        "    baseline = lbllist.count(1) / len(lbllist)\n",
        "    plt.plot([0, 1], [baseline, baseline], linestyle='--', label='baseline')\n",
        "    plt.plot(lr_recall, lr_precision, marker='.', label='Roberta model evaluation')\n",
        "    # axis labels\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    # show the legend\n",
        "    plt.legend()\n",
        "    # show the plot\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Gg2WU5bG01h",
        "outputId": "77cf1e94-c87c-4a65-af64-79ee6d2be01d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classes=np.unique(y_train)\n",
        "y_train[y_train==0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bkFUZgfsLL9",
        "outputId": "3c0d2f75-9f72-4477-e982-033150bdc1c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3.7842, 0.5761], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "#class weight computation method3\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "#compute the class weights\n",
        "y_train_indices = train_dataset.indices\n",
        "\n",
        "y_train = [target[i] for i in y_train_indices]\n",
        "\n",
        "class_wts = compute_class_weight(\"balanced\",classes= np.unique(y_train),y=y_train)\n",
        "#class_wts = dict(zip(np.unique(labels),class_wts))\n",
        "weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyL9rqq1GlER",
        "outputId": "c832edfb-c547-4b76-af1b-af2f0cfde09f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([     0,      1,      2, ..., 608455, 608456, 608457])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "np.unique(train_dataset.indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GctZMMYh3Brs"
      },
      "outputs": [],
      "source": [
        "#Tried to use cross entropy with computed weights for imbalance check\n",
        "weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "# loss function\n",
        "cross_entropy = torch.nn.CrossEntropyLoss(weight=weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iged2rKuSBf"
      },
      "outputs": [],
      "source": [
        "#For validation data loader\n",
        "cross_entropy1 =torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "p297s4FquSin",
        "outputId": "10871af3-aab4-4aad-e165-052bdb22ff32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 8 ========\n",
            "Training...\n",
            "  Batch    40  of  7,606.    Elapsed: 0:00:57.\n",
            "  Batch    80  of  7,606.    Elapsed: 0:01:54.\n",
            "  Batch   120  of  7,606.    Elapsed: 0:02:51.\n",
            "  Batch   160  of  7,606.    Elapsed: 0:03:48.\n",
            "  Batch   200  of  7,606.    Elapsed: 0:04:45.\n",
            "  Batch   240  of  7,606.    Elapsed: 0:05:42.\n",
            "  Batch   280  of  7,606.    Elapsed: 0:06:39.\n",
            "  Batch   320  of  7,606.    Elapsed: 0:07:36.\n",
            "  Batch   360  of  7,606.    Elapsed: 0:08:33.\n",
            "  Batch   400  of  7,606.    Elapsed: 0:09:30.\n",
            "  Batch   440  of  7,606.    Elapsed: 0:10:27.\n",
            "  Batch   480  of  7,606.    Elapsed: 0:11:24.\n",
            "  Batch   520  of  7,606.    Elapsed: 0:12:21.\n",
            "  Batch   560  of  7,606.    Elapsed: 0:13:18.\n",
            "  Batch   600  of  7,606.    Elapsed: 0:14:15.\n",
            "  Batch   640  of  7,606.    Elapsed: 0:15:12.\n",
            "  Batch   680  of  7,606.    Elapsed: 0:16:09.\n",
            "  Batch   720  of  7,606.    Elapsed: 0:17:06.\n",
            "  Batch   760  of  7,606.    Elapsed: 0:18:03.\n",
            "  Batch   800  of  7,606.    Elapsed: 0:19:00.\n",
            "  Batch   840  of  7,606.    Elapsed: 0:19:57.\n",
            "  Batch   880  of  7,606.    Elapsed: 0:20:54.\n",
            "  Batch   920  of  7,606.    Elapsed: 0:21:51.\n",
            "  Batch   960  of  7,606.    Elapsed: 0:22:48.\n",
            "  Batch 1,000  of  7,606.    Elapsed: 0:23:45.\n",
            "  Batch 1,040  of  7,606.    Elapsed: 0:24:42.\n",
            "  Batch 1,080  of  7,606.    Elapsed: 0:25:39.\n",
            "  Batch 1,120  of  7,606.    Elapsed: 0:26:36.\n",
            "  Batch 1,160  of  7,606.    Elapsed: 0:27:33.\n",
            "  Batch 1,200  of  7,606.    Elapsed: 0:28:30.\n",
            "  Batch 1,240  of  7,606.    Elapsed: 0:29:27.\n",
            "  Batch 1,280  of  7,606.    Elapsed: 0:30:24.\n",
            "  Batch 1,320  of  7,606.    Elapsed: 0:31:21.\n",
            "  Batch 1,360  of  7,606.    Elapsed: 0:32:18.\n",
            "  Batch 1,400  of  7,606.    Elapsed: 0:33:15.\n",
            "  Batch 1,440  of  7,606.    Elapsed: 0:34:12.\n",
            "  Batch 1,480  of  7,606.    Elapsed: 0:35:09.\n",
            "  Batch 1,520  of  7,606.    Elapsed: 0:36:06.\n",
            "  Batch 1,560  of  7,606.    Elapsed: 0:37:03.\n",
            "  Batch 1,600  of  7,606.    Elapsed: 0:38:00.\n",
            "  Batch 1,640  of  7,606.    Elapsed: 0:38:57.\n",
            "  Batch 1,680  of  7,606.    Elapsed: 0:39:54.\n",
            "  Batch 1,720  of  7,606.    Elapsed: 0:40:51.\n",
            "  Batch 1,760  of  7,606.    Elapsed: 0:41:48.\n",
            "  Batch 1,800  of  7,606.    Elapsed: 0:42:45.\n",
            "  Batch 1,840  of  7,606.    Elapsed: 0:43:42.\n",
            "  Batch 1,880  of  7,606.    Elapsed: 0:44:39.\n",
            "  Batch 1,920  of  7,606.    Elapsed: 0:45:36.\n",
            "  Batch 1,960  of  7,606.    Elapsed: 0:46:33.\n",
            "  Batch 2,000  of  7,606.    Elapsed: 0:47:30.\n",
            "  Batch 2,040  of  7,606.    Elapsed: 0:48:28.\n",
            "  Batch 2,080  of  7,606.    Elapsed: 0:49:25.\n",
            "  Batch 2,120  of  7,606.    Elapsed: 0:50:22.\n",
            "  Batch 2,160  of  7,606.    Elapsed: 0:51:19.\n",
            "  Batch 2,200  of  7,606.    Elapsed: 0:52:16.\n",
            "  Batch 2,240  of  7,606.    Elapsed: 0:53:13.\n",
            "  Batch 2,280  of  7,606.    Elapsed: 0:54:10.\n",
            "  Batch 2,320  of  7,606.    Elapsed: 0:55:07.\n",
            "  Batch 2,360  of  7,606.    Elapsed: 0:56:04.\n",
            "  Batch 2,400  of  7,606.    Elapsed: 0:57:01.\n",
            "  Batch 2,440  of  7,606.    Elapsed: 0:57:58.\n",
            "  Batch 2,480  of  7,606.    Elapsed: 0:58:55.\n",
            "  Batch 2,520  of  7,606.    Elapsed: 0:59:52.\n",
            "  Batch 2,560  of  7,606.    Elapsed: 1:00:49.\n",
            "  Batch 2,600  of  7,606.    Elapsed: 1:01:46.\n",
            "  Batch 2,640  of  7,606.    Elapsed: 1:02:43.\n",
            "  Batch 2,680  of  7,606.    Elapsed: 1:03:40.\n",
            "  Batch 2,720  of  7,606.    Elapsed: 1:04:37.\n",
            "  Batch 2,760  of  7,606.    Elapsed: 1:05:34.\n",
            "  Batch 2,800  of  7,606.    Elapsed: 1:06:31.\n",
            "  Batch 2,840  of  7,606.    Elapsed: 1:07:28.\n",
            "  Batch 2,880  of  7,606.    Elapsed: 1:08:25.\n",
            "  Batch 2,920  of  7,606.    Elapsed: 1:09:22.\n",
            "  Batch 2,960  of  7,606.    Elapsed: 1:10:19.\n",
            "  Batch 3,000  of  7,606.    Elapsed: 1:11:16.\n",
            "  Batch 3,040  of  7,606.    Elapsed: 1:12:13.\n",
            "  Batch 3,080  of  7,606.    Elapsed: 1:13:10.\n",
            "  Batch 3,120  of  7,606.    Elapsed: 1:14:07.\n",
            "  Batch 3,160  of  7,606.    Elapsed: 1:15:04.\n",
            "  Batch 3,200  of  7,606.    Elapsed: 1:16:01.\n",
            "  Batch 3,240  of  7,606.    Elapsed: 1:16:58.\n",
            "  Batch 3,280  of  7,606.    Elapsed: 1:17:55.\n",
            "  Batch 3,320  of  7,606.    Elapsed: 1:18:52.\n",
            "  Batch 3,360  of  7,606.    Elapsed: 1:19:49.\n",
            "  Batch 3,400  of  7,606.    Elapsed: 1:20:46.\n",
            "  Batch 3,440  of  7,606.    Elapsed: 1:21:43.\n",
            "  Batch 3,480  of  7,606.    Elapsed: 1:22:40.\n",
            "  Batch 3,520  of  7,606.    Elapsed: 1:23:37.\n",
            "  Batch 3,560  of  7,606.    Elapsed: 1:24:34.\n",
            "  Batch 3,600  of  7,606.    Elapsed: 1:25:31.\n",
            "  Batch 3,640  of  7,606.    Elapsed: 1:26:28.\n",
            "  Batch 3,680  of  7,606.    Elapsed: 1:27:25.\n",
            "  Batch 3,720  of  7,606.    Elapsed: 1:28:22.\n",
            "  Batch 3,760  of  7,606.    Elapsed: 1:29:19.\n",
            "  Batch 3,800  of  7,606.    Elapsed: 1:30:16.\n",
            "  Batch 3,840  of  7,606.    Elapsed: 1:31:13.\n",
            "  Batch 3,880  of  7,606.    Elapsed: 1:32:10.\n",
            "  Batch 3,920  of  7,606.    Elapsed: 1:33:07.\n",
            "  Batch 3,960  of  7,606.    Elapsed: 1:34:04.\n",
            "  Batch 4,000  of  7,606.    Elapsed: 1:35:01.\n",
            "  Batch 4,040  of  7,606.    Elapsed: 1:35:58.\n",
            "  Batch 4,080  of  7,606.    Elapsed: 1:36:55.\n",
            "  Batch 4,120  of  7,606.    Elapsed: 1:37:52.\n",
            "  Batch 4,160  of  7,606.    Elapsed: 1:38:49.\n",
            "  Batch 4,200  of  7,606.    Elapsed: 1:39:46.\n",
            "  Batch 4,240  of  7,606.    Elapsed: 1:40:43.\n",
            "  Batch 4,280  of  7,606.    Elapsed: 1:41:40.\n",
            "  Batch 4,320  of  7,606.    Elapsed: 1:42:37.\n",
            "  Batch 4,360  of  7,606.    Elapsed: 1:43:34.\n",
            "  Batch 4,400  of  7,606.    Elapsed: 1:44:31.\n",
            "  Batch 4,440  of  7,606.    Elapsed: 1:45:28.\n",
            "  Batch 4,480  of  7,606.    Elapsed: 1:46:25.\n",
            "  Batch 4,520  of  7,606.    Elapsed: 1:47:22.\n",
            "  Batch 4,560  of  7,606.    Elapsed: 1:48:19.\n",
            "  Batch 4,600  of  7,606.    Elapsed: 1:49:16.\n",
            "  Batch 4,640  of  7,606.    Elapsed: 1:50:13.\n",
            "  Batch 4,680  of  7,606.    Elapsed: 1:51:10.\n",
            "  Batch 4,720  of  7,606.    Elapsed: 1:52:07.\n",
            "  Batch 4,760  of  7,606.    Elapsed: 1:53:04.\n",
            "  Batch 4,800  of  7,606.    Elapsed: 1:54:01.\n",
            "  Batch 4,840  of  7,606.    Elapsed: 1:54:58.\n",
            "  Batch 4,880  of  7,606.    Elapsed: 1:55:55.\n",
            "  Batch 4,920  of  7,606.    Elapsed: 1:56:52.\n",
            "  Batch 4,960  of  7,606.    Elapsed: 1:57:49.\n",
            "  Batch 5,000  of  7,606.    Elapsed: 1:58:46.\n",
            "  Batch 5,040  of  7,606.    Elapsed: 1:59:43.\n",
            "  Batch 5,080  of  7,606.    Elapsed: 2:00:40.\n",
            "  Batch 5,120  of  7,606.    Elapsed: 2:01:37.\n",
            "  Batch 5,160  of  7,606.    Elapsed: 2:02:34.\n",
            "  Batch 5,200  of  7,606.    Elapsed: 2:03:31.\n",
            "  Batch 5,240  of  7,606.    Elapsed: 2:04:28.\n",
            "  Batch 5,280  of  7,606.    Elapsed: 2:05:25.\n",
            "  Batch 5,320  of  7,606.    Elapsed: 2:06:22.\n",
            "  Batch 5,360  of  7,606.    Elapsed: 2:07:19.\n",
            "  Batch 5,400  of  7,606.    Elapsed: 2:08:16.\n",
            "  Batch 5,440  of  7,606.    Elapsed: 2:09:13.\n",
            "  Batch 5,480  of  7,606.    Elapsed: 2:10:10.\n",
            "  Batch 5,520  of  7,606.    Elapsed: 2:11:07.\n",
            "  Batch 5,560  of  7,606.    Elapsed: 2:12:04.\n",
            "  Batch 5,600  of  7,606.    Elapsed: 2:13:01.\n",
            "  Batch 5,640  of  7,606.    Elapsed: 2:13:58.\n",
            "  Batch 5,680  of  7,606.    Elapsed: 2:14:55.\n",
            "  Batch 5,720  of  7,606.    Elapsed: 2:15:52.\n",
            "  Batch 5,760  of  7,606.    Elapsed: 2:16:49.\n",
            "  Batch 5,800  of  7,606.    Elapsed: 2:17:46.\n",
            "  Batch 5,840  of  7,606.    Elapsed: 2:18:43.\n",
            "  Batch 5,880  of  7,606.    Elapsed: 2:19:40.\n",
            "  Batch 5,920  of  7,606.    Elapsed: 2:20:37.\n",
            "  Batch 5,960  of  7,606.    Elapsed: 2:21:34.\n",
            "  Batch 6,000  of  7,606.    Elapsed: 2:22:31.\n",
            "  Batch 6,040  of  7,606.    Elapsed: 2:23:28.\n",
            "  Batch 6,080  of  7,606.    Elapsed: 2:24:25.\n",
            "  Batch 6,120  of  7,606.    Elapsed: 2:25:22.\n",
            "  Batch 6,160  of  7,606.    Elapsed: 2:26:19.\n",
            "  Batch 6,200  of  7,606.    Elapsed: 2:27:16.\n",
            "  Batch 6,240  of  7,606.    Elapsed: 2:28:13.\n",
            "  Batch 6,280  of  7,606.    Elapsed: 2:29:10.\n",
            "  Batch 6,320  of  7,606.    Elapsed: 2:30:07.\n",
            "  Batch 6,360  of  7,606.    Elapsed: 2:31:04.\n",
            "  Batch 6,400  of  7,606.    Elapsed: 2:32:01.\n",
            "  Batch 6,440  of  7,606.    Elapsed: 2:32:58.\n",
            "  Batch 6,480  of  7,606.    Elapsed: 2:33:55.\n",
            "  Batch 6,520  of  7,606.    Elapsed: 2:34:52.\n",
            "  Batch 6,560  of  7,606.    Elapsed: 2:35:49.\n",
            "  Batch 6,600  of  7,606.    Elapsed: 2:36:47.\n",
            "  Batch 6,640  of  7,606.    Elapsed: 2:37:44.\n",
            "  Batch 6,680  of  7,606.    Elapsed: 2:38:41.\n",
            "  Batch 6,720  of  7,606.    Elapsed: 2:39:38.\n",
            "  Batch 6,760  of  7,606.    Elapsed: 2:40:35.\n",
            "  Batch 6,800  of  7,606.    Elapsed: 2:41:32.\n",
            "  Batch 6,840  of  7,606.    Elapsed: 2:42:29.\n",
            "  Batch 6,880  of  7,606.    Elapsed: 2:43:26.\n",
            "  Batch 6,920  of  7,606.    Elapsed: 2:44:23.\n",
            "  Batch 6,960  of  7,606.    Elapsed: 2:45:20.\n",
            "  Batch 7,000  of  7,606.    Elapsed: 2:46:17.\n",
            "  Batch 7,040  of  7,606.    Elapsed: 2:47:14.\n",
            "  Batch 7,080  of  7,606.    Elapsed: 2:48:11.\n",
            "  Batch 7,120  of  7,606.    Elapsed: 2:49:08.\n",
            "  Batch 7,160  of  7,606.    Elapsed: 2:50:05.\n",
            "  Batch 7,200  of  7,606.    Elapsed: 2:51:02.\n",
            "  Batch 7,240  of  7,606.    Elapsed: 2:51:59.\n",
            "  Batch 7,280  of  7,606.    Elapsed: 2:52:56.\n",
            "  Batch 7,320  of  7,606.    Elapsed: 2:53:53.\n",
            "  Batch 7,360  of  7,606.    Elapsed: 2:54:50.\n",
            "  Batch 7,400  of  7,606.    Elapsed: 2:55:47.\n",
            "  Batch 7,440  of  7,606.    Elapsed: 2:56:44.\n",
            "  Batch 7,480  of  7,606.    Elapsed: 2:57:41.\n",
            "  Batch 7,520  of  7,606.    Elapsed: 2:58:38.\n",
            "  Batch 7,560  of  7,606.    Elapsed: 2:59:35.\n",
            "  Batch 7,600  of  7,606.    Elapsed: 3:00:32.\n",
            "\n",
            "  Average training loss: 0.07\n",
            "  Training epcoh took: 3:00:40\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.75\n",
            "  F1 score: 0.59\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.27      0.53      0.35     16123\n",
            "           1       0.92      0.77      0.84    105569\n",
            "\n",
            "    accuracy                           0.74    121692\n",
            "   macro avg       0.59      0.65      0.60    121692\n",
            "weighted avg       0.83      0.74      0.78    121692\n",
            "\n",
            "[[ 8596  7527]\n",
            " [23783 81786]]\n",
            "Mathews Correlation coefficient score for this epoch is : 0.24\n",
            "Model validation score: f1=0.839 auc=0.928\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Validation Loss: 0.46\n",
            "  Validation took: 0:14:17\n",
            "\n",
            "======== Epoch 2 / 8 ========\n",
            "Training...\n",
            "  Batch    40  of  7,606.    Elapsed: 0:00:57.\n",
            "  Batch    80  of  7,606.    Elapsed: 0:01:54.\n",
            "  Batch   120  of  7,606.    Elapsed: 0:02:51.\n",
            "  Batch   160  of  7,606.    Elapsed: 0:03:48.\n",
            "  Batch   200  of  7,606.    Elapsed: 0:04:45.\n",
            "  Batch   240  of  7,606.    Elapsed: 0:05:42.\n",
            "  Batch   280  of  7,606.    Elapsed: 0:06:39.\n",
            "  Batch   320  of  7,606.    Elapsed: 0:07:36.\n",
            "  Batch   360  of  7,606.    Elapsed: 0:08:33.\n",
            "  Batch   400  of  7,606.    Elapsed: 0:09:30.\n",
            "  Batch   440  of  7,606.    Elapsed: 0:10:27.\n",
            "  Batch   480  of  7,606.    Elapsed: 0:11:24.\n",
            "  Batch   520  of  7,606.    Elapsed: 0:12:21.\n",
            "  Batch   560  of  7,606.    Elapsed: 0:13:18.\n",
            "  Batch   600  of  7,606.    Elapsed: 0:14:15.\n",
            "  Batch   640  of  7,606.    Elapsed: 0:15:13.\n",
            "  Batch   680  of  7,606.    Elapsed: 0:16:10.\n",
            "  Batch   720  of  7,606.    Elapsed: 0:17:07.\n",
            "  Batch   760  of  7,606.    Elapsed: 0:18:04.\n",
            "  Batch   800  of  7,606.    Elapsed: 0:19:01.\n",
            "  Batch   840  of  7,606.    Elapsed: 0:19:58.\n",
            "  Batch   880  of  7,606.    Elapsed: 0:20:55.\n",
            "  Batch   920  of  7,606.    Elapsed: 0:21:52.\n",
            "  Batch   960  of  7,606.    Elapsed: 0:22:49.\n",
            "  Batch 1,000  of  7,606.    Elapsed: 0:23:46.\n",
            "  Batch 1,040  of  7,606.    Elapsed: 0:24:43.\n",
            "  Batch 1,080  of  7,606.    Elapsed: 0:25:40.\n",
            "  Batch 1,120  of  7,606.    Elapsed: 0:26:37.\n",
            "  Batch 1,160  of  7,606.    Elapsed: 0:27:34.\n",
            "  Batch 1,200  of  7,606.    Elapsed: 0:28:31.\n",
            "  Batch 1,240  of  7,606.    Elapsed: 0:29:28.\n",
            "  Batch 1,280  of  7,606.    Elapsed: 0:30:25.\n",
            "  Batch 1,320  of  7,606.    Elapsed: 0:31:22.\n",
            "  Batch 1,360  of  7,606.    Elapsed: 0:32:19.\n",
            "  Batch 1,400  of  7,606.    Elapsed: 0:33:16.\n",
            "  Batch 1,440  of  7,606.    Elapsed: 0:34:13.\n",
            "  Batch 1,480  of  7,606.    Elapsed: 0:35:10.\n",
            "  Batch 1,520  of  7,606.    Elapsed: 0:36:07.\n",
            "  Batch 1,560  of  7,606.    Elapsed: 0:37:04.\n",
            "  Batch 1,600  of  7,606.    Elapsed: 0:38:01.\n",
            "  Batch 1,640  of  7,606.    Elapsed: 0:38:58.\n",
            "  Batch 1,680  of  7,606.    Elapsed: 0:39:55.\n",
            "  Batch 1,720  of  7,606.    Elapsed: 0:40:52.\n",
            "  Batch 1,760  of  7,606.    Elapsed: 0:41:49.\n",
            "  Batch 1,800  of  7,606.    Elapsed: 0:42:46.\n",
            "  Batch 1,840  of  7,606.    Elapsed: 0:43:43.\n",
            "  Batch 1,880  of  7,606.    Elapsed: 0:44:40.\n",
            "  Batch 1,920  of  7,606.    Elapsed: 0:45:37.\n",
            "  Batch 1,960  of  7,606.    Elapsed: 0:46:34.\n",
            "  Batch 2,000  of  7,606.    Elapsed: 0:47:31.\n",
            "  Batch 2,040  of  7,606.    Elapsed: 0:48:28.\n",
            "  Batch 2,080  of  7,606.    Elapsed: 0:49:25.\n",
            "  Batch 2,120  of  7,606.    Elapsed: 0:50:22.\n",
            "  Batch 2,160  of  7,606.    Elapsed: 0:51:19.\n",
            "  Batch 2,200  of  7,606.    Elapsed: 0:52:16.\n",
            "  Batch 2,240  of  7,606.    Elapsed: 0:53:13.\n",
            "  Batch 2,280  of  7,606.    Elapsed: 0:54:10.\n",
            "  Batch 2,320  of  7,606.    Elapsed: 0:55:07.\n",
            "  Batch 2,360  of  7,606.    Elapsed: 0:56:04.\n",
            "  Batch 2,400  of  7,606.    Elapsed: 0:57:01.\n",
            "  Batch 2,440  of  7,606.    Elapsed: 0:57:58.\n",
            "  Batch 2,480  of  7,606.    Elapsed: 0:58:55.\n",
            "  Batch 2,520  of  7,606.    Elapsed: 0:59:52.\n",
            "  Batch 2,560  of  7,606.    Elapsed: 1:00:49.\n",
            "  Batch 2,600  of  7,606.    Elapsed: 1:01:46.\n",
            "  Batch 2,640  of  7,606.    Elapsed: 1:02:43.\n",
            "  Batch 2,680  of  7,606.    Elapsed: 1:03:40.\n",
            "  Batch 2,720  of  7,606.    Elapsed: 1:04:37.\n",
            "  Batch 2,760  of  7,606.    Elapsed: 1:05:34.\n",
            "  Batch 2,800  of  7,606.    Elapsed: 1:06:31.\n",
            "  Batch 2,840  of  7,606.    Elapsed: 1:07:28.\n",
            "  Batch 2,880  of  7,606.    Elapsed: 1:08:25.\n",
            "  Batch 2,920  of  7,606.    Elapsed: 1:09:22.\n",
            "  Batch 2,960  of  7,606.    Elapsed: 1:10:19.\n",
            "  Batch 3,000  of  7,606.    Elapsed: 1:11:16.\n",
            "  Batch 3,040  of  7,606.    Elapsed: 1:12:13.\n",
            "  Batch 3,080  of  7,606.    Elapsed: 1:13:10.\n",
            "  Batch 3,120  of  7,606.    Elapsed: 1:14:07.\n",
            "  Batch 3,160  of  7,606.    Elapsed: 1:15:04.\n",
            "  Batch 3,200  of  7,606.    Elapsed: 1:16:01.\n",
            "  Batch 3,240  of  7,606.    Elapsed: 1:16:58.\n",
            "  Batch 3,280  of  7,606.    Elapsed: 1:17:55.\n",
            "  Batch 3,320  of  7,606.    Elapsed: 1:18:52.\n",
            "  Batch 3,360  of  7,606.    Elapsed: 1:19:49.\n",
            "  Batch 3,400  of  7,606.    Elapsed: 1:20:46.\n",
            "  Batch 3,440  of  7,606.    Elapsed: 1:21:43.\n",
            "  Batch 3,480  of  7,606.    Elapsed: 1:22:40.\n",
            "  Batch 3,520  of  7,606.    Elapsed: 1:23:37.\n",
            "  Batch 3,560  of  7,606.    Elapsed: 1:24:34.\n",
            "  Batch 3,600  of  7,606.    Elapsed: 1:25:31.\n",
            "  Batch 3,640  of  7,606.    Elapsed: 1:26:28.\n",
            "  Batch 3,680  of  7,606.    Elapsed: 1:27:25.\n",
            "  Batch 3,720  of  7,606.    Elapsed: 1:28:22.\n",
            "  Batch 3,760  of  7,606.    Elapsed: 1:29:19.\n",
            "  Batch 3,800  of  7,606.    Elapsed: 1:30:16.\n",
            "  Batch 3,840  of  7,606.    Elapsed: 1:31:14.\n",
            "  Batch 3,880  of  7,606.    Elapsed: 1:32:11.\n",
            "  Batch 3,920  of  7,606.    Elapsed: 1:33:08.\n",
            "  Batch 3,960  of  7,606.    Elapsed: 1:34:05.\n",
            "  Batch 4,000  of  7,606.    Elapsed: 1:35:02.\n",
            "  Batch 4,040  of  7,606.    Elapsed: 1:35:59.\n",
            "  Batch 4,080  of  7,606.    Elapsed: 1:36:56.\n",
            "  Batch 4,120  of  7,606.    Elapsed: 1:37:53.\n",
            "  Batch 4,160  of  7,606.    Elapsed: 1:38:50.\n",
            "  Batch 4,200  of  7,606.    Elapsed: 1:39:47.\n",
            "  Batch 4,240  of  7,606.    Elapsed: 1:40:44.\n",
            "  Batch 4,280  of  7,606.    Elapsed: 1:41:41.\n",
            "  Batch 4,320  of  7,606.    Elapsed: 1:42:38.\n",
            "  Batch 4,360  of  7,606.    Elapsed: 1:43:35.\n",
            "  Batch 4,400  of  7,606.    Elapsed: 1:44:32.\n",
            "  Batch 4,440  of  7,606.    Elapsed: 1:45:29.\n",
            "  Batch 4,480  of  7,606.    Elapsed: 1:46:26.\n",
            "  Batch 4,520  of  7,606.    Elapsed: 1:47:23.\n",
            "  Batch 4,560  of  7,606.    Elapsed: 1:48:20.\n",
            "  Batch 4,600  of  7,606.    Elapsed: 1:49:17.\n",
            "  Batch 4,640  of  7,606.    Elapsed: 1:50:14.\n",
            "  Batch 4,680  of  7,606.    Elapsed: 1:51:11.\n",
            "  Batch 4,720  of  7,606.    Elapsed: 1:52:08.\n",
            "  Batch 4,760  of  7,606.    Elapsed: 1:53:05.\n",
            "  Batch 4,800  of  7,606.    Elapsed: 1:54:02.\n",
            "  Batch 4,840  of  7,606.    Elapsed: 1:54:59.\n",
            "  Batch 4,880  of  7,606.    Elapsed: 1:55:56.\n",
            "  Batch 4,920  of  7,606.    Elapsed: 1:56:53.\n",
            "  Batch 4,960  of  7,606.    Elapsed: 1:57:50.\n",
            "  Batch 5,000  of  7,606.    Elapsed: 1:58:47.\n",
            "  Batch 5,040  of  7,606.    Elapsed: 1:59:44.\n",
            "  Batch 5,080  of  7,606.    Elapsed: 2:00:41.\n",
            "  Batch 5,120  of  7,606.    Elapsed: 2:01:38.\n",
            "  Batch 5,160  of  7,606.    Elapsed: 2:02:35.\n",
            "  Batch 5,200  of  7,606.    Elapsed: 2:03:32.\n",
            "  Batch 5,240  of  7,606.    Elapsed: 2:04:29.\n",
            "  Batch 5,280  of  7,606.    Elapsed: 2:05:26.\n",
            "  Batch 5,320  of  7,606.    Elapsed: 2:06:23.\n",
            "  Batch 5,360  of  7,606.    Elapsed: 2:07:20.\n",
            "  Batch 5,400  of  7,606.    Elapsed: 2:08:17.\n",
            "  Batch 5,440  of  7,606.    Elapsed: 2:09:14.\n",
            "  Batch 5,480  of  7,606.    Elapsed: 2:10:11.\n",
            "  Batch 5,520  of  7,606.    Elapsed: 2:11:08.\n",
            "  Batch 5,560  of  7,606.    Elapsed: 2:12:05.\n",
            "  Batch 5,600  of  7,606.    Elapsed: 2:13:02.\n",
            "  Batch 5,640  of  7,606.    Elapsed: 2:13:59.\n",
            "  Batch 5,680  of  7,606.    Elapsed: 2:14:56.\n",
            "  Batch 5,720  of  7,606.    Elapsed: 2:15:54.\n",
            "  Batch 5,760  of  7,606.    Elapsed: 2:16:51.\n",
            "  Batch 5,800  of  7,606.    Elapsed: 2:17:48.\n",
            "  Batch 5,840  of  7,606.    Elapsed: 2:18:45.\n",
            "  Batch 5,880  of  7,606.    Elapsed: 2:19:42.\n",
            "  Batch 5,920  of  7,606.    Elapsed: 2:20:39.\n",
            "  Batch 5,960  of  7,606.    Elapsed: 2:21:36.\n",
            "  Batch 6,000  of  7,606.    Elapsed: 2:22:33.\n",
            "  Batch 6,040  of  7,606.    Elapsed: 2:23:30.\n",
            "  Batch 6,080  of  7,606.    Elapsed: 2:24:27.\n",
            "  Batch 6,120  of  7,606.    Elapsed: 2:25:24.\n",
            "  Batch 6,160  of  7,606.    Elapsed: 2:26:21.\n",
            "  Batch 6,200  of  7,606.    Elapsed: 2:27:18.\n",
            "  Batch 6,240  of  7,606.    Elapsed: 2:28:15.\n",
            "  Batch 6,280  of  7,606.    Elapsed: 2:29:12.\n",
            "  Batch 6,320  of  7,606.    Elapsed: 2:30:09.\n",
            "  Batch 6,360  of  7,606.    Elapsed: 2:31:06.\n",
            "  Batch 6,400  of  7,606.    Elapsed: 2:32:03.\n",
            "  Batch 6,440  of  7,606.    Elapsed: 2:33:00.\n",
            "  Batch 6,480  of  7,606.    Elapsed: 2:33:57.\n",
            "  Batch 6,520  of  7,606.    Elapsed: 2:34:54.\n",
            "  Batch 6,560  of  7,606.    Elapsed: 2:35:51.\n",
            "  Batch 6,600  of  7,606.    Elapsed: 2:36:48.\n",
            "  Batch 6,640  of  7,606.    Elapsed: 2:37:45.\n",
            "  Batch 6,680  of  7,606.    Elapsed: 2:38:42.\n",
            "  Batch 6,720  of  7,606.    Elapsed: 2:39:39.\n",
            "  Batch 6,760  of  7,606.    Elapsed: 2:40:36.\n",
            "  Batch 6,800  of  7,606.    Elapsed: 2:41:33.\n",
            "  Batch 6,840  of  7,606.    Elapsed: 2:42:30.\n",
            "  Batch 6,880  of  7,606.    Elapsed: 2:43:27.\n",
            "  Batch 6,920  of  7,606.    Elapsed: 2:44:24.\n",
            "  Batch 6,960  of  7,606.    Elapsed: 2:45:21.\n",
            "  Batch 7,000  of  7,606.    Elapsed: 2:46:18.\n",
            "  Batch 7,040  of  7,606.    Elapsed: 2:47:15.\n",
            "  Batch 7,080  of  7,606.    Elapsed: 2:48:12.\n",
            "  Batch 7,120  of  7,606.    Elapsed: 2:49:09.\n",
            "  Batch 7,160  of  7,606.    Elapsed: 2:50:06.\n",
            "  Batch 7,200  of  7,606.    Elapsed: 2:51:03.\n",
            "  Batch 7,240  of  7,606.    Elapsed: 2:52:00.\n",
            "  Batch 7,280  of  7,606.    Elapsed: 2:52:57.\n",
            "  Batch 7,320  of  7,606.    Elapsed: 2:53:54.\n",
            "  Batch 7,360  of  7,606.    Elapsed: 2:54:51.\n",
            "  Batch 7,400  of  7,606.    Elapsed: 2:55:48.\n",
            "  Batch 7,440  of  7,606.    Elapsed: 2:56:45.\n",
            "  Batch 7,480  of  7,606.    Elapsed: 2:57:42.\n",
            "  Batch 7,520  of  7,606.    Elapsed: 2:58:39.\n",
            "  Batch 7,560  of  7,606.    Elapsed: 2:59:36.\n",
            "  Batch 7,600  of  7,606.    Elapsed: 3:00:33.\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 3:00:42\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.52\n",
            "  F1 score: 0.43\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.18      0.89      0.30     16123\n",
            "           1       0.96      0.39      0.56    105569\n",
            "\n",
            "    accuracy                           0.46    121692\n",
            "   macro avg       0.57      0.64      0.43    121692\n",
            "weighted avg       0.86      0.46      0.52    121692\n",
            "\n",
            "[[14409  1714]\n",
            " [64066 41503]]\n",
            "Mathews Correlation coefficient score for this epoch is : 0.2\n",
            "Model validation score: f1=0.558 auc=0.945\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Validation Loss: 0.76\n",
            "  Validation took: 0:14:17\n",
            "\n",
            "======== Epoch 3 / 8 ========\n",
            "Training...\n",
            "  Batch    40  of  7,606.    Elapsed: 0:00:57.\n",
            "  Batch    80  of  7,606.    Elapsed: 0:01:54.\n",
            "  Batch   120  of  7,606.    Elapsed: 0:02:51.\n",
            "  Batch   160  of  7,606.    Elapsed: 0:03:48.\n",
            "  Batch   200  of  7,606.    Elapsed: 0:04:45.\n",
            "  Batch   240  of  7,606.    Elapsed: 0:05:42.\n",
            "  Batch   280  of  7,606.    Elapsed: 0:06:39.\n",
            "  Batch   320  of  7,606.    Elapsed: 0:07:36.\n",
            "  Batch   360  of  7,606.    Elapsed: 0:08:33.\n",
            "  Batch   400  of  7,606.    Elapsed: 0:09:30.\n",
            "  Batch   440  of  7,606.    Elapsed: 0:10:27.\n",
            "  Batch   480  of  7,606.    Elapsed: 0:11:24.\n",
            "  Batch   520  of  7,606.    Elapsed: 0:12:21.\n",
            "  Batch   560  of  7,606.    Elapsed: 0:13:18.\n",
            "  Batch   600  of  7,606.    Elapsed: 0:14:15.\n",
            "  Batch   640  of  7,606.    Elapsed: 0:15:12.\n",
            "  Batch   680  of  7,606.    Elapsed: 0:16:09.\n",
            "  Batch   720  of  7,606.    Elapsed: 0:17:06.\n",
            "  Batch   760  of  7,606.    Elapsed: 0:18:03.\n",
            "  Batch   800  of  7,606.    Elapsed: 0:19:00.\n",
            "  Batch   840  of  7,606.    Elapsed: 0:19:57.\n",
            "  Batch   880  of  7,606.    Elapsed: 0:20:54.\n",
            "  Batch   920  of  7,606.    Elapsed: 0:21:51.\n",
            "  Batch   960  of  7,606.    Elapsed: 0:22:48.\n",
            "  Batch 1,000  of  7,606.    Elapsed: 0:23:45.\n",
            "  Batch 1,040  of  7,606.    Elapsed: 0:24:42.\n",
            "  Batch 1,080  of  7,606.    Elapsed: 0:25:39.\n",
            "  Batch 1,120  of  7,606.    Elapsed: 0:26:36.\n",
            "  Batch 1,160  of  7,606.    Elapsed: 0:27:33.\n",
            "  Batch 1,200  of  7,606.    Elapsed: 0:28:30.\n",
            "  Batch 1,240  of  7,606.    Elapsed: 0:29:27.\n",
            "  Batch 1,280  of  7,606.    Elapsed: 0:30:24.\n",
            "  Batch 1,320  of  7,606.    Elapsed: 0:31:21.\n",
            "  Batch 1,360  of  7,606.    Elapsed: 0:32:18.\n",
            "  Batch 1,400  of  7,606.    Elapsed: 0:33:15.\n",
            "  Batch 1,440  of  7,606.    Elapsed: 0:34:12.\n",
            "  Batch 1,480  of  7,606.    Elapsed: 0:35:09.\n",
            "  Batch 1,520  of  7,606.    Elapsed: 0:36:06.\n",
            "  Batch 1,560  of  7,606.    Elapsed: 0:37:03.\n",
            "  Batch 1,600  of  7,606.    Elapsed: 0:38:00.\n",
            "  Batch 1,640  of  7,606.    Elapsed: 0:38:58.\n",
            "  Batch 1,680  of  7,606.    Elapsed: 0:39:55.\n",
            "  Batch 1,720  of  7,606.    Elapsed: 0:40:52.\n",
            "  Batch 1,760  of  7,606.    Elapsed: 0:41:49.\n",
            "  Batch 1,800  of  7,606.    Elapsed: 0:42:46.\n",
            "  Batch 1,840  of  7,606.    Elapsed: 0:43:43.\n",
            "  Batch 1,880  of  7,606.    Elapsed: 0:44:40.\n",
            "  Batch 1,920  of  7,606.    Elapsed: 0:45:37.\n",
            "  Batch 1,960  of  7,606.    Elapsed: 0:46:34.\n",
            "  Batch 2,000  of  7,606.    Elapsed: 0:47:31.\n",
            "  Batch 2,040  of  7,606.    Elapsed: 0:48:28.\n",
            "  Batch 2,080  of  7,606.    Elapsed: 0:49:25.\n",
            "  Batch 2,120  of  7,606.    Elapsed: 0:50:22.\n",
            "  Batch 2,160  of  7,606.    Elapsed: 0:51:19.\n",
            "  Batch 2,200  of  7,606.    Elapsed: 0:52:16.\n",
            "  Batch 2,240  of  7,606.    Elapsed: 0:53:13.\n",
            "  Batch 2,280  of  7,606.    Elapsed: 0:54:10.\n",
            "  Batch 2,320  of  7,606.    Elapsed: 0:55:07.\n",
            "  Batch 2,360  of  7,606.    Elapsed: 0:56:04.\n",
            "  Batch 2,400  of  7,606.    Elapsed: 0:57:01.\n",
            "  Batch 2,440  of  7,606.    Elapsed: 0:57:58.\n",
            "  Batch 2,480  of  7,606.    Elapsed: 0:58:55.\n",
            "  Batch 2,520  of  7,606.    Elapsed: 0:59:52.\n",
            "  Batch 2,560  of  7,606.    Elapsed: 1:00:49.\n",
            "  Batch 2,600  of  7,606.    Elapsed: 1:01:46.\n",
            "  Batch 2,640  of  7,606.    Elapsed: 1:02:43.\n",
            "  Batch 2,680  of  7,606.    Elapsed: 1:03:40.\n",
            "  Batch 2,720  of  7,606.    Elapsed: 1:04:37.\n",
            "  Batch 2,760  of  7,606.    Elapsed: 1:05:34.\n",
            "  Batch 2,800  of  7,606.    Elapsed: 1:06:31.\n",
            "  Batch 2,840  of  7,606.    Elapsed: 1:07:28.\n",
            "  Batch 2,880  of  7,606.    Elapsed: 1:08:25.\n",
            "  Batch 2,920  of  7,606.    Elapsed: 1:09:22.\n",
            "  Batch 2,960  of  7,606.    Elapsed: 1:10:19.\n",
            "  Batch 3,000  of  7,606.    Elapsed: 1:11:16.\n",
            "  Batch 3,040  of  7,606.    Elapsed: 1:12:13.\n",
            "  Batch 3,080  of  7,606.    Elapsed: 1:13:10.\n",
            "  Batch 3,120  of  7,606.    Elapsed: 1:14:07.\n",
            "  Batch 3,160  of  7,606.    Elapsed: 1:15:04.\n",
            "  Batch 3,200  of  7,606.    Elapsed: 1:16:01.\n",
            "  Batch 3,240  of  7,606.    Elapsed: 1:16:58.\n",
            "  Batch 3,280  of  7,606.    Elapsed: 1:17:55.\n",
            "  Batch 3,320  of  7,606.    Elapsed: 1:18:52.\n",
            "  Batch 3,360  of  7,606.    Elapsed: 1:19:49.\n",
            "  Batch 3,400  of  7,606.    Elapsed: 1:20:46.\n",
            "  Batch 3,440  of  7,606.    Elapsed: 1:21:43.\n",
            "  Batch 3,480  of  7,606.    Elapsed: 1:22:40.\n",
            "  Batch 3,520  of  7,606.    Elapsed: 1:23:37.\n",
            "  Batch 3,560  of  7,606.    Elapsed: 1:24:34.\n",
            "  Batch 3,600  of  7,606.    Elapsed: 1:25:31.\n",
            "  Batch 3,640  of  7,606.    Elapsed: 1:26:28.\n",
            "  Batch 3,680  of  7,606.    Elapsed: 1:27:25.\n",
            "  Batch 3,720  of  7,606.    Elapsed: 1:28:22.\n",
            "  Batch 3,760  of  7,606.    Elapsed: 1:29:19.\n",
            "  Batch 3,800  of  7,606.    Elapsed: 1:30:16.\n",
            "  Batch 3,840  of  7,606.    Elapsed: 1:31:13.\n",
            "  Batch 3,880  of  7,606.    Elapsed: 1:32:10.\n",
            "  Batch 3,920  of  7,606.    Elapsed: 1:33:07.\n",
            "  Batch 3,960  of  7,606.    Elapsed: 1:34:04.\n",
            "  Batch 4,000  of  7,606.    Elapsed: 1:35:01.\n",
            "  Batch 4,040  of  7,606.    Elapsed: 1:35:58.\n",
            "  Batch 4,080  of  7,606.    Elapsed: 1:36:55.\n",
            "  Batch 4,120  of  7,606.    Elapsed: 1:37:52.\n",
            "  Batch 4,160  of  7,606.    Elapsed: 1:38:49.\n",
            "  Batch 4,200  of  7,606.    Elapsed: 1:39:46.\n",
            "  Batch 4,240  of  7,606.    Elapsed: 1:40:43.\n",
            "  Batch 4,280  of  7,606.    Elapsed: 1:41:40.\n",
            "  Batch 4,320  of  7,606.    Elapsed: 1:42:37.\n",
            "  Batch 4,360  of  7,606.    Elapsed: 1:43:34.\n",
            "  Batch 4,400  of  7,606.    Elapsed: 1:44:31.\n",
            "  Batch 4,440  of  7,606.    Elapsed: 1:45:28.\n",
            "  Batch 4,480  of  7,606.    Elapsed: 1:46:25.\n",
            "  Batch 4,520  of  7,606.    Elapsed: 1:47:22.\n",
            "  Batch 4,560  of  7,606.    Elapsed: 1:48:19.\n",
            "  Batch 4,600  of  7,606.    Elapsed: 1:49:16.\n",
            "  Batch 4,640  of  7,606.    Elapsed: 1:50:13.\n",
            "  Batch 4,680  of  7,606.    Elapsed: 1:51:10.\n",
            "  Batch 4,720  of  7,606.    Elapsed: 1:52:07.\n",
            "  Batch 4,760  of  7,606.    Elapsed: 1:53:04.\n",
            "  Batch 4,800  of  7,606.    Elapsed: 1:54:01.\n",
            "  Batch 4,840  of  7,606.    Elapsed: 1:54:58.\n",
            "  Batch 4,880  of  7,606.    Elapsed: 1:55:55.\n",
            "  Batch 4,920  of  7,606.    Elapsed: 1:56:52.\n",
            "  Batch 4,960  of  7,606.    Elapsed: 1:57:49.\n",
            "  Batch 5,000  of  7,606.    Elapsed: 1:58:46.\n",
            "  Batch 5,040  of  7,606.    Elapsed: 1:59:43.\n",
            "  Batch 5,080  of  7,606.    Elapsed: 2:00:40.\n",
            "  Batch 5,120  of  7,606.    Elapsed: 2:01:37.\n",
            "  Batch 5,160  of  7,606.    Elapsed: 2:02:34.\n",
            "  Batch 5,200  of  7,606.    Elapsed: 2:03:31.\n",
            "  Batch 5,240  of  7,606.    Elapsed: 2:04:28.\n",
            "  Batch 5,280  of  7,606.    Elapsed: 2:05:25.\n",
            "  Batch 5,320  of  7,606.    Elapsed: 2:06:22.\n",
            "  Batch 5,360  of  7,606.    Elapsed: 2:07:19.\n",
            "  Batch 5,400  of  7,606.    Elapsed: 2:08:16.\n",
            "  Batch 5,440  of  7,606.    Elapsed: 2:09:13.\n",
            "  Batch 5,480  of  7,606.    Elapsed: 2:10:10.\n",
            "  Batch 5,520  of  7,606.    Elapsed: 2:11:07.\n",
            "  Batch 5,560  of  7,606.    Elapsed: 2:12:04.\n",
            "  Batch 5,600  of  7,606.    Elapsed: 2:13:01.\n",
            "  Batch 5,640  of  7,606.    Elapsed: 2:13:58.\n",
            "  Batch 5,680  of  7,606.    Elapsed: 2:14:55.\n",
            "  Batch 5,720  of  7,606.    Elapsed: 2:15:52.\n",
            "  Batch 5,760  of  7,606.    Elapsed: 2:16:49.\n",
            "  Batch 5,800  of  7,606.    Elapsed: 2:17:46.\n",
            "  Batch 5,840  of  7,606.    Elapsed: 2:18:43.\n",
            "  Batch 5,880  of  7,606.    Elapsed: 2:19:40.\n",
            "  Batch 5,920  of  7,606.    Elapsed: 2:20:37.\n",
            "  Batch 5,960  of  7,606.    Elapsed: 2:21:34.\n",
            "  Batch 6,000  of  7,606.    Elapsed: 2:22:31.\n",
            "  Batch 6,040  of  7,606.    Elapsed: 2:23:28.\n",
            "  Batch 6,080  of  7,606.    Elapsed: 2:24:25.\n",
            "  Batch 6,120  of  7,606.    Elapsed: 2:25:22.\n",
            "  Batch 6,160  of  7,606.    Elapsed: 2:26:19.\n",
            "  Batch 6,200  of  7,606.    Elapsed: 2:27:16.\n",
            "  Batch 6,240  of  7,606.    Elapsed: 2:28:13.\n",
            "  Batch 6,280  of  7,606.    Elapsed: 2:29:10.\n",
            "  Batch 6,320  of  7,606.    Elapsed: 2:30:07.\n",
            "  Batch 6,360  of  7,606.    Elapsed: 2:31:04.\n",
            "  Batch 6,400  of  7,606.    Elapsed: 2:32:01.\n",
            "  Batch 6,440  of  7,606.    Elapsed: 2:32:58.\n",
            "  Batch 6,480  of  7,606.    Elapsed: 2:33:55.\n",
            "  Batch 6,520  of  7,606.    Elapsed: 2:34:52.\n",
            "  Batch 6,560  of  7,606.    Elapsed: 2:35:49.\n",
            "  Batch 6,600  of  7,606.    Elapsed: 2:36:46.\n",
            "  Batch 6,640  of  7,606.    Elapsed: 2:37:43.\n",
            "  Batch 6,680  of  7,606.    Elapsed: 2:38:40.\n",
            "  Batch 6,720  of  7,606.    Elapsed: 2:39:37.\n",
            "  Batch 6,760  of  7,606.    Elapsed: 2:40:34.\n",
            "  Batch 6,800  of  7,606.    Elapsed: 2:41:31.\n",
            "  Batch 6,840  of  7,606.    Elapsed: 2:42:28.\n",
            "  Batch 6,880  of  7,606.    Elapsed: 2:43:25.\n",
            "  Batch 6,920  of  7,606.    Elapsed: 2:44:22.\n",
            "  Batch 6,960  of  7,606.    Elapsed: 2:45:19.\n",
            "  Batch 7,000  of  7,606.    Elapsed: 2:46:16.\n",
            "  Batch 7,040  of  7,606.    Elapsed: 2:47:13.\n",
            "  Batch 7,080  of  7,606.    Elapsed: 2:48:10.\n",
            "  Batch 7,120  of  7,606.    Elapsed: 2:49:07.\n",
            "  Batch 7,160  of  7,606.    Elapsed: 2:50:04.\n",
            "  Batch 7,200  of  7,606.    Elapsed: 2:51:01.\n",
            "  Batch 7,240  of  7,606.    Elapsed: 2:51:58.\n",
            "  Batch 7,280  of  7,606.    Elapsed: 2:52:55.\n",
            "  Batch 7,320  of  7,606.    Elapsed: 2:53:52.\n",
            "  Batch 7,360  of  7,606.    Elapsed: 2:54:49.\n",
            "  Batch 7,400  of  7,606.    Elapsed: 2:55:46.\n",
            "  Batch 7,440  of  7,606.    Elapsed: 2:56:43.\n",
            "  Batch 7,480  of  7,606.    Elapsed: 2:57:40.\n",
            "  Batch 7,520  of  7,606.    Elapsed: 2:58:37.\n",
            "  Batch 7,560  of  7,606.    Elapsed: 2:59:34.\n",
            "  Batch 7,600  of  7,606.    Elapsed: 3:00:31.\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 3:00:39\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.60\n",
            "  F1 score: 0.51\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.81      0.33     16123\n",
            "           1       0.95      0.54      0.69    105569\n",
            "\n",
            "    accuracy                           0.57    121692\n",
            "   macro avg       0.58      0.67      0.51    121692\n",
            "weighted avg       0.85      0.57      0.64    121692\n",
            "\n",
            "[[13063  3060]\n",
            " [48949 56620]]\n",
            "Mathews Correlation coefficient score for this epoch is : 0.24\n",
            "Model validation score: f1=0.685 auc=0.947\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Validation Loss: 0.68\n",
            "  Validation took: 0:14:17\n",
            "\n",
            "======== Epoch 4 / 8 ========\n",
            "Training...\n",
            "  Batch    40  of  7,606.    Elapsed: 0:00:57.\n",
            "  Batch    80  of  7,606.    Elapsed: 0:01:54.\n",
            "  Batch   120  of  7,606.    Elapsed: 0:02:51.\n",
            "  Batch   160  of  7,606.    Elapsed: 0:03:48.\n",
            "  Batch   200  of  7,606.    Elapsed: 0:04:45.\n",
            "  Batch   240  of  7,606.    Elapsed: 0:05:42.\n",
            "  Batch   280  of  7,606.    Elapsed: 0:06:39.\n",
            "  Batch   320  of  7,606.    Elapsed: 0:07:36.\n",
            "  Batch   360  of  7,606.    Elapsed: 0:08:33.\n",
            "  Batch   400  of  7,606.    Elapsed: 0:09:30.\n",
            "  Batch   440  of  7,606.    Elapsed: 0:10:27.\n",
            "  Batch   480  of  7,606.    Elapsed: 0:11:24.\n",
            "  Batch   520  of  7,606.    Elapsed: 0:12:21.\n",
            "  Batch   560  of  7,606.    Elapsed: 0:13:18.\n",
            "  Batch   600  of  7,606.    Elapsed: 0:14:15.\n",
            "  Batch   640  of  7,606.    Elapsed: 0:15:12.\n",
            "  Batch   680  of  7,606.    Elapsed: 0:16:09.\n",
            "  Batch   720  of  7,606.    Elapsed: 0:17:06.\n",
            "  Batch   760  of  7,606.    Elapsed: 0:18:03.\n",
            "  Batch   800  of  7,606.    Elapsed: 0:19:00.\n",
            "  Batch   840  of  7,606.    Elapsed: 0:19:57.\n",
            "  Batch   880  of  7,606.    Elapsed: 0:20:54.\n",
            "  Batch   920  of  7,606.    Elapsed: 0:21:51.\n",
            "  Batch   960  of  7,606.    Elapsed: 0:22:48.\n",
            "  Batch 1,000  of  7,606.    Elapsed: 0:23:45.\n",
            "  Batch 1,040  of  7,606.    Elapsed: 0:24:42.\n",
            "  Batch 1,080  of  7,606.    Elapsed: 0:25:39.\n",
            "  Batch 1,120  of  7,606.    Elapsed: 0:26:37.\n",
            "  Batch 1,160  of  7,606.    Elapsed: 0:27:34.\n",
            "  Batch 1,200  of  7,606.    Elapsed: 0:28:31.\n",
            "  Batch 1,240  of  7,606.    Elapsed: 0:29:28.\n",
            "  Batch 1,280  of  7,606.    Elapsed: 0:30:25.\n",
            "  Batch 1,320  of  7,606.    Elapsed: 0:31:22.\n",
            "  Batch 1,360  of  7,606.    Elapsed: 0:32:19.\n",
            "  Batch 1,400  of  7,606.    Elapsed: 0:33:16.\n",
            "  Batch 1,440  of  7,606.    Elapsed: 0:34:13.\n",
            "  Batch 1,480  of  7,606.    Elapsed: 0:35:10.\n",
            "  Batch 1,520  of  7,606.    Elapsed: 0:36:07.\n",
            "  Batch 1,560  of  7,606.    Elapsed: 0:37:04.\n",
            "  Batch 1,600  of  7,606.    Elapsed: 0:38:01.\n",
            "  Batch 1,640  of  7,606.    Elapsed: 0:38:58.\n",
            "  Batch 1,680  of  7,606.    Elapsed: 0:39:55.\n",
            "  Batch 1,720  of  7,606.    Elapsed: 0:40:52.\n",
            "  Batch 1,760  of  7,606.    Elapsed: 0:41:49.\n",
            "  Batch 1,800  of  7,606.    Elapsed: 0:42:46.\n",
            "  Batch 1,840  of  7,606.    Elapsed: 0:43:43.\n",
            "  Batch 1,880  of  7,606.    Elapsed: 0:44:40.\n",
            "  Batch 1,920  of  7,606.    Elapsed: 0:45:37.\n",
            "  Batch 1,960  of  7,606.    Elapsed: 0:46:34.\n",
            "  Batch 2,000  of  7,606.    Elapsed: 0:47:31.\n",
            "  Batch 2,040  of  7,606.    Elapsed: 0:48:28.\n",
            "  Batch 2,080  of  7,606.    Elapsed: 0:49:25.\n",
            "  Batch 2,120  of  7,606.    Elapsed: 0:50:22.\n",
            "  Batch 2,160  of  7,606.    Elapsed: 0:51:19.\n",
            "  Batch 2,200  of  7,606.    Elapsed: 0:52:16.\n",
            "  Batch 2,240  of  7,606.    Elapsed: 0:53:13.\n",
            "  Batch 2,280  of  7,606.    Elapsed: 0:54:10.\n",
            "  Batch 2,320  of  7,606.    Elapsed: 0:55:07.\n",
            "  Batch 2,360  of  7,606.    Elapsed: 0:56:04.\n",
            "  Batch 2,400  of  7,606.    Elapsed: 0:57:01.\n",
            "  Batch 2,440  of  7,606.    Elapsed: 0:57:58.\n",
            "  Batch 2,480  of  7,606.    Elapsed: 0:58:55.\n",
            "  Batch 2,520  of  7,606.    Elapsed: 0:59:52.\n",
            "  Batch 2,560  of  7,606.    Elapsed: 1:00:49.\n",
            "  Batch 2,600  of  7,606.    Elapsed: 1:01:46.\n",
            "  Batch 2,640  of  7,606.    Elapsed: 1:02:43.\n",
            "  Batch 2,680  of  7,606.    Elapsed: 1:03:40.\n",
            "  Batch 2,720  of  7,606.    Elapsed: 1:04:37.\n",
            "  Batch 2,760  of  7,606.    Elapsed: 1:05:34.\n",
            "  Batch 2,800  of  7,606.    Elapsed: 1:06:31.\n",
            "  Batch 2,840  of  7,606.    Elapsed: 1:07:28.\n",
            "  Batch 2,880  of  7,606.    Elapsed: 1:08:25.\n",
            "  Batch 2,920  of  7,606.    Elapsed: 1:09:22.\n",
            "  Batch 2,960  of  7,606.    Elapsed: 1:10:19.\n",
            "  Batch 3,000  of  7,606.    Elapsed: 1:11:16.\n",
            "  Batch 3,040  of  7,606.    Elapsed: 1:12:13.\n",
            "  Batch 3,080  of  7,606.    Elapsed: 1:13:10.\n",
            "  Batch 3,120  of  7,606.    Elapsed: 1:14:07.\n",
            "  Batch 3,160  of  7,606.    Elapsed: 1:15:04.\n",
            "  Batch 3,200  of  7,606.    Elapsed: 1:16:01.\n",
            "  Batch 3,240  of  7,606.    Elapsed: 1:16:58.\n",
            "  Batch 3,280  of  7,606.    Elapsed: 1:17:55.\n",
            "  Batch 3,320  of  7,606.    Elapsed: 1:18:52.\n",
            "  Batch 3,360  of  7,606.    Elapsed: 1:19:49.\n",
            "  Batch 3,400  of  7,606.    Elapsed: 1:20:46.\n",
            "  Batch 3,440  of  7,606.    Elapsed: 1:21:43.\n",
            "  Batch 3,480  of  7,606.    Elapsed: 1:22:40.\n",
            "  Batch 3,520  of  7,606.    Elapsed: 1:23:37.\n",
            "  Batch 3,560  of  7,606.    Elapsed: 1:24:34.\n",
            "  Batch 3,600  of  7,606.    Elapsed: 1:25:31.\n",
            "  Batch 3,640  of  7,606.    Elapsed: 1:26:28.\n",
            "  Batch 3,680  of  7,606.    Elapsed: 1:27:25.\n",
            "  Batch 3,720  of  7,606.    Elapsed: 1:28:22.\n",
            "  Batch 3,760  of  7,606.    Elapsed: 1:29:19.\n",
            "  Batch 3,800  of  7,606.    Elapsed: 1:30:16.\n",
            "  Batch 3,840  of  7,606.    Elapsed: 1:31:13.\n",
            "  Batch 3,880  of  7,606.    Elapsed: 1:32:10.\n",
            "  Batch 3,920  of  7,606.    Elapsed: 1:33:07.\n",
            "  Batch 3,960  of  7,606.    Elapsed: 1:34:04.\n",
            "  Batch 4,000  of  7,606.    Elapsed: 1:35:01.\n",
            "  Batch 4,040  of  7,606.    Elapsed: 1:35:58.\n",
            "  Batch 4,080  of  7,606.    Elapsed: 1:36:55.\n",
            "  Batch 4,120  of  7,606.    Elapsed: 1:37:52.\n",
            "  Batch 4,160  of  7,606.    Elapsed: 1:38:49.\n",
            "  Batch 4,200  of  7,606.    Elapsed: 1:39:46.\n",
            "  Batch 4,240  of  7,606.    Elapsed: 1:40:43.\n",
            "  Batch 4,280  of  7,606.    Elapsed: 1:41:40.\n",
            "  Batch 4,320  of  7,606.    Elapsed: 1:42:37.\n",
            "  Batch 4,360  of  7,606.    Elapsed: 1:43:34.\n",
            "  Batch 4,400  of  7,606.    Elapsed: 1:44:31.\n",
            "  Batch 4,440  of  7,606.    Elapsed: 1:45:28.\n",
            "  Batch 4,480  of  7,606.    Elapsed: 1:46:25.\n",
            "  Batch 4,520  of  7,606.    Elapsed: 1:47:22.\n",
            "  Batch 4,560  of  7,606.    Elapsed: 1:48:19.\n",
            "  Batch 4,600  of  7,606.    Elapsed: 1:49:16.\n",
            "  Batch 4,640  of  7,606.    Elapsed: 1:50:13.\n",
            "  Batch 4,680  of  7,606.    Elapsed: 1:51:10.\n",
            "  Batch 4,720  of  7,606.    Elapsed: 1:52:07.\n",
            "  Batch 4,760  of  7,606.    Elapsed: 1:53:04.\n",
            "  Batch 4,800  of  7,606.    Elapsed: 1:54:01.\n",
            "  Batch 4,840  of  7,606.    Elapsed: 1:54:58.\n",
            "  Batch 4,880  of  7,606.    Elapsed: 1:55:55.\n",
            "  Batch 4,920  of  7,606.    Elapsed: 1:56:52.\n",
            "  Batch 4,960  of  7,606.    Elapsed: 1:57:49.\n",
            "  Batch 5,000  of  7,606.    Elapsed: 1:58:46.\n",
            "  Batch 5,040  of  7,606.    Elapsed: 1:59:43.\n",
            "  Batch 5,080  of  7,606.    Elapsed: 2:00:40.\n",
            "  Batch 5,120  of  7,606.    Elapsed: 2:01:37.\n",
            "  Batch 5,160  of  7,606.    Elapsed: 2:02:34.\n",
            "  Batch 5,200  of  7,606.    Elapsed: 2:03:31.\n",
            "  Batch 5,240  of  7,606.    Elapsed: 2:04:28.\n",
            "  Batch 5,280  of  7,606.    Elapsed: 2:05:25.\n",
            "  Batch 5,320  of  7,606.    Elapsed: 2:06:22.\n",
            "  Batch 5,360  of  7,606.    Elapsed: 2:07:20.\n",
            "  Batch 5,400  of  7,606.    Elapsed: 2:08:17.\n",
            "  Batch 5,440  of  7,606.    Elapsed: 2:09:14.\n",
            "  Batch 5,480  of  7,606.    Elapsed: 2:10:11.\n",
            "  Batch 5,520  of  7,606.    Elapsed: 2:11:08.\n",
            "  Batch 5,560  of  7,606.    Elapsed: 2:12:05.\n",
            "  Batch 5,600  of  7,606.    Elapsed: 2:13:02.\n",
            "  Batch 5,640  of  7,606.    Elapsed: 2:13:59.\n",
            "  Batch 5,680  of  7,606.    Elapsed: 2:14:56.\n",
            "  Batch 5,720  of  7,606.    Elapsed: 2:15:53.\n",
            "  Batch 5,760  of  7,606.    Elapsed: 2:16:50.\n",
            "  Batch 5,800  of  7,606.    Elapsed: 2:17:47.\n",
            "  Batch 5,840  of  7,606.    Elapsed: 2:18:44.\n",
            "  Batch 5,880  of  7,606.    Elapsed: 2:19:41.\n",
            "  Batch 5,920  of  7,606.    Elapsed: 2:20:38.\n",
            "  Batch 5,960  of  7,606.    Elapsed: 2:21:35.\n",
            "  Batch 6,000  of  7,606.    Elapsed: 2:22:32.\n",
            "  Batch 6,040  of  7,606.    Elapsed: 2:23:29.\n",
            "  Batch 6,080  of  7,606.    Elapsed: 2:24:26.\n",
            "  Batch 6,120  of  7,606.    Elapsed: 2:25:23.\n",
            "  Batch 6,160  of  7,606.    Elapsed: 2:26:20.\n",
            "  Batch 6,200  of  7,606.    Elapsed: 2:27:17.\n",
            "  Batch 6,240  of  7,606.    Elapsed: 2:28:14.\n",
            "  Batch 6,280  of  7,606.    Elapsed: 2:29:11.\n",
            "  Batch 6,320  of  7,606.    Elapsed: 2:30:08.\n",
            "  Batch 6,360  of  7,606.    Elapsed: 2:31:05.\n",
            "  Batch 6,400  of  7,606.    Elapsed: 2:32:02.\n",
            "  Batch 6,440  of  7,606.    Elapsed: 2:32:59.\n",
            "  Batch 6,480  of  7,606.    Elapsed: 2:33:56.\n",
            "  Batch 6,520  of  7,606.    Elapsed: 2:34:53.\n",
            "  Batch 6,560  of  7,606.    Elapsed: 2:35:50.\n",
            "  Batch 6,600  of  7,606.    Elapsed: 2:36:47.\n",
            "  Batch 6,640  of  7,606.    Elapsed: 2:37:44.\n",
            "  Batch 6,680  of  7,606.    Elapsed: 2:38:41.\n",
            "  Batch 6,720  of  7,606.    Elapsed: 2:39:38.\n",
            "  Batch 6,760  of  7,606.    Elapsed: 2:40:35.\n",
            "  Batch 6,800  of  7,606.    Elapsed: 2:41:32.\n",
            "  Batch 6,840  of  7,606.    Elapsed: 2:42:29.\n",
            "  Batch 6,880  of  7,606.    Elapsed: 2:43:26.\n",
            "  Batch 6,920  of  7,606.    Elapsed: 2:44:23.\n",
            "  Batch 6,960  of  7,606.    Elapsed: 2:45:20.\n",
            "  Batch 7,000  of  7,606.    Elapsed: 2:46:17.\n",
            "  Batch 7,040  of  7,606.    Elapsed: 2:47:14.\n",
            "  Batch 7,080  of  7,606.    Elapsed: 2:48:11.\n",
            "  Batch 7,120  of  7,606.    Elapsed: 2:49:08.\n",
            "  Batch 7,160  of  7,606.    Elapsed: 2:50:05.\n",
            "  Batch 7,200  of  7,606.    Elapsed: 2:51:02.\n",
            "  Batch 7,240  of  7,606.    Elapsed: 2:51:59.\n",
            "  Batch 7,280  of  7,606.    Elapsed: 2:52:56.\n",
            "  Batch 7,320  of  7,606.    Elapsed: 2:53:53.\n",
            "  Batch 7,360  of  7,606.    Elapsed: 2:54:50.\n",
            "  Batch 7,400  of  7,606.    Elapsed: 2:55:47.\n",
            "  Batch 7,440  of  7,606.    Elapsed: 2:56:44.\n",
            "  Batch 7,480  of  7,606.    Elapsed: 2:57:41.\n",
            "  Batch 7,520  of  7,606.    Elapsed: 2:58:38.\n",
            "  Batch 7,560  of  7,606.    Elapsed: 2:59:35.\n",
            "  Batch 7,600  of  7,606.    Elapsed: 3:00:32.\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 3:00:40\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.72\n",
            "  F1 score: 0.57\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.68      0.37     16123\n",
            "           1       0.93      0.69      0.79    105569\n",
            "\n",
            "    accuracy                           0.69    121692\n",
            "   macro avg       0.59      0.68      0.58    121692\n",
            "weighted avg       0.84      0.69      0.74    121692\n",
            "\n",
            "[[11003  5120]\n",
            " [33029 72540]]\n",
            "Mathews Correlation coefficient score for this epoch is : 0.26\n",
            "Model validation score: f1=0.792 auc=0.949\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3hU5bn+8e9DAFFABKFWBQW7QQkQASOCqFAQRKtYFbZQUSju4ona1u3+VWsrkWI9126VbosVQfCAorXgEUQp4tZCUEABFaooQX6KHEIRERKe/cespEOYJJNkVuZ0f65rLmbWYeZZScidd73vepe5OyIiIhU1SHYBIiKSmhQQIiISkwJCRERiUkCIiEhMCggREYmpYbILSJTWrVt7+/btk12GiEhaWbZs2Vfu3ibWuowJiPbt21NYWJjsMkRE0oqZfVrZOp1iEhGRmBQQIiISkwJCRERiypg+CJFUtHfvXoqKiti9e3eyS5Es16RJE9q2bUujRo3i3kcBIRKioqIimjdvTvv27TGzZJcjWcrd2bJlC0VFRXTo0CHu/UI7xWRmU83sSzN7v5L1Zmb3mdk6M1tpZj2j1o02s7XBY3RYNYqEbffu3Rx++OEKB0kqM+Pwww+vcUs2zBbENOAB4NFK1p8NdAwepwD/A5xiZq2ACUA+4MAyM5vj7ttCq7SgRdTz4tA+RrKTwkFSQW1+DkNrQbj7ImBrFZucDzzqEW8Dh5nZkcBZwHx33xqEwnxgSFh17hcOsV6LiGSpZI5iOhrYEPW6KFhW2fIDmNk4Mys0s8LNmzeHVqhIulq/fj1du3YN5b0XLlzIueeeC8CcOXO4/fbbQ/kcSZ607qR29ynAFID8/Hzd+UgkSYYOHcrQoUOTXYYkWDJbEBuBdlGv2wbLKlsejop9DuqDkAxTUlLCJZdcQufOnRk2bBi7du1i4sSJnHzyyXTt2pVx48ZRdmfJ++67j9zcXPLy8hgxYgQAX3/9NWPHjqVXr1706NGDv/71rwd8xrRp0xg/fjwAY8aM4dprr+XUU0/luOOOY/bs2eXb3XXXXZx88snk5eUxYcKEejh6qYtktiDmAOPN7EkindTF7r7JzF4BfmdmLYPtBgM3hlpJwyZwyhUwaGKoHyNy8Z/eOmDZuXlHcmmf9nyzp5Qxjyw5YP2wk9oyPL8dW7/ew1Uzl+23btYVfar9zA8//JCHH36Yvn37MnbsWP74xz8yfvx4br75ZgAuvfRSnn/+ec477zxuv/12PvnkEw466CC2b98OwK233sqAAQOYOnUq27dvp1evXpx55plVfuamTZtYvHgxH3zwAUOHDmXYsGHMmzePtWvXsmTJEtydoUOHsmjRIs4444xqj0GSI8xhrk8AbwHHm1mRmV1uZlea2ZXBJi8CHwPrgIeAqwHcfSvwW2Bp8JgYLBORWmjXrh19+/YFYNSoUSxevJjXX3+dU045hW7duvHaa6+xatUqAPLy8rjkkkuYOXMmDRtG/n6cN28et99+O927d6d///7s3r2bzz77rMrP/OEPf0iDBg3Izc3liy++KH+fefPm0aNHD3r27MkHH3zA2rVrQzxyqavQWhDuPrKa9Q5cU8m6qcDUMOoSSaaq/uI/uHFOletbNW0cV4uhoorDG82Mq6++msLCQtq1a0dBQUH5+PgXXniBRYsWMXfuXG699Vbee+893J1nnnmG448/fr/3KfvFH8tBBx1U/rzs9JW7c+ONN3LFFVfU+BgkOTQXk0iG++yzz3jrrciprccff5zTTjsNgNatW7Nz587yPoJ9+/axYcMGvv/973PHHXdQXFzMzp07Oeuss7j//vvLf9G/++67tarjrLPOYurUqezcuROAjRs38uWXX9b18CREaT2KKRSTjoSSXdDwEPj1pmRXI1Jnxx9/PJMnT2bs2LHk5uZy1VVXsW3bNrp27cp3v/tdTj75ZABKS0sZNWoUxcXFuDvXXnsthx12GL/5zW/4+c9/Tl5eHvv27aNDhw48//zzNa5j8ODBrFmzhj59Iq2gZs2aMXPmTL7zne8k9Hglcazsr4J0l5+f77W+YdCkIyKd1H9/KBIOZRQSUkdr1qyhc+fOyS5DBIj982hmy9w9P9b2akFEiw6HsteahkNEspQCosyb/139NlVNw6HwEJEMo05qgJIEzNWvOZxEJMMoIBJJISEiGUQBkWgKCRHJEAqIMCgkRCQDKCAKWla/TW3c1Smc9xWpoZycHLp3707Xrl0577zzyudYqkz//v2p9ZBxIlOMP/7447XePxHat2/PV199Vedt6qIuX8eFCxfyv//7v+WvH3zwQR59tLJ7r4VHo5jYF3txvKOSKmstfP0FFBwGBVX/ZxQJ28EHH8zy5csBGD16NJMnT+amm24K5bNKSkrKA+JHP/pRKJ+RDRYuXEizZs049dRTAbjyyiur2SMcakGEyiMBUvYQiceGJfDGPZF/E6xPnz5s3BiZPX/58uX07t2bvLw8LrjgArZt+9ddfWfMmFHe6liyJFJHZdN+T5s2jaFDhzJgwAAGDhzIDTfcwBtvvEH37t259957Wb9+Paeffjo9e/akZ8+e+/1lXGb9+vWccMIJjBkzhk6dOnHJJZfw6quv0rdvXzp27Fhew9atW/nhD39IXl4evXv3ZuXKlQBs2bKFwYMH06VLF/7jP/6D6AuAZ86cSa9evejevTtXXHEFpaWlVX6N5s2bR58+fejZsyfDhw9n586dvPzyywwfPrx8m+ibJV111VXk5+fTpUuXSqcwb9asWfnz2bNnM2bMGADmzp3LKaecQo8ePTjzzDP54osvWL9+PQ8++CD33nsv3bt354033qCgoIC77767yu9b//79+eUvf0mvXr3o1KkTb7zxRpXHGQ+1IOqqoDj+X/7R210+H9r1CqcmSU0v3QD//72qt/l2B3zxPvg+sAZwRFc46NDKt/9uNzg7vju5lZaWsmDBAi6//HIALrvsMu6//3769evHzTffzC233MIf/vAHAHbt2sXy5ctZtGgRY8eO5f33369y2u933nmHlStX0qpVKxYuXMjdd99dPh3Hrl27mD9/Pk2aNGHt2rWMHDky5qmXdevW8fTTTzN16lROPvlkHn/8cRYvXsycOXP43e9+x3PPPceECRPo0aMHzz33HK+99hqXXXYZy5cv55ZbbuG0007j5ptv5oUXXuDhhx8GIlcOz5o1izfffJNGjRpx9dVX89hjj3HZZZfF/Bp99dVXTJo0iVdffZWmTZtyxx138Pvf/55f/epXjBs3jq+//pqmTZsya9as8vtl3HrrrbRq1YrS0lIGDhzIypUrycvLi+t7ctppp/H2229jZvz5z3/mzjvv5J577uHKK6+kWbNmXH/99QAsWLCgfJ+qvm8lJSUsWbKEF198kVtuuYVXX301rjoqo4BIhJqERJmHB1X+XpK9dhdHwgEi/+4urjog4vDNN9/QvXt3Nm7cSOfOnRk0aBDFxcVs376dfv36AZFTT9F/IY8cGZmM+YwzzmDHjh1s376defPmMWfOnPK/ZKOn/R40aBCtWrWK+fl79+5l/PjxLF++nJycHD766KOY23Xo0IFu3boB0KVLFwYOHIiZ0a1bN9avXw/A4sWLeeaZZwAYMGAAW7ZsYceOHSxatIhnn30WgB/84Ae0bBnpW1ywYAHLli0rn2/qm2++qXLup7fffpvVq1eXT4++Z88e+vTpQ8OGDRkyZAhz585l2LBhvPDCC9x5550APPXUU0yZMoWSkhI2bdrE6tWr4w6IoqIiLr74YjZt2sSePXvo0KFDldtX93278MILATjppJPKv2Z1oYBIlNqERMz3aaGQyFTx/KW/YQlMHwqleyCnMVz05zq3NMv6IHbt2sVZZ53F5MmTGT16dJX7xJoivLJpv//+97/TtGnTSt/r3nvv5YgjjmDFihXs27ePJk2axNwueorwBg0alL9u0KABJSUlVdZbGXdn9OjR3HbbbXFvP2jQIJ544okD1o0YMYIHHniAVq1akZ+fT/Pmzfnkk0+4++67Wbp0KS1btmTMmDHlU6dHi/56Rq//6U9/ynXXXcfQoUNZuHAhBQUFNT/IKGVfs5ycnFp/zaKpDyKW1sdXv00sifrFrv6K7NWuF4yeAwNuivybwNOQhxxyCPfddx/33HMPTZs2pWXLluXnqWfMmFH+VynArFmzgMhf7C1atKBFixZxT/vdvHlz/vnPf5a/Li4u5sgjj6RBgwbMmDGj2j6Aqpx++uk89thjQKQfoHXr1hx66KGcccYZ5SOnXnrppfLz8gMHDmT27Nnl04pv3bqVTz/9tNL37927N2+++Sbr1q0DIv0uZS2efv368c477/DQQw+Vn17asWMHTZs2pUWLFnzxxRe89NJLMd/3iCOOYM2aNezbt4+//OUv+31tjj76aACmT59evrzi17BMixYtqvy+JZpaELGMr0PnYCJbEhXfV7JDu16h9U/16NGDvLw8nnjiCaZPn86VV17Jrl27OO6443jkkUfKt2vSpAk9evRg7969TJ0auXdXvNN+5+XlkZOTw4knnsiYMWO4+uqrueiii3j00UcZMmRIla2N6hQUFDB27Fjy8vI45JBDyn+pTpgwgZEjR9KlSxdOPfVUjjnmGAByc3OZNGkSgwcPZt++fTRq1IjJkydz7LHHxnz/Nm3aMG3aNEaOHMm3334LwKRJk+jUqRM5OTmce+65TJs2rfxzTzzxRHr06MEJJ5yw3537Krr99ts599xzadOmDfn5+eX3xCgoKGD48OG0bNmSAQMG8MknnwBw3nnnMWzYMP76179y//337/deVX3fEk3Tfcf6ZR7GL+NEtwrO/W/IH5PY95SE03Tfkko03Xeqqix0JraGfXtr/n7P/yzyiOczRERqQX0QyXbzV4ntu1D/hYgkiAIiVSTyr//yi/MOS9x7Sq1lymlcSW+1+TkMNSDMbIiZfWhm68zshhjrjzWzBWa20swWmlnbqHV3mtkqM1tjZvdZxXF3maigeP9HnblaFUnWpEkTtmzZopCQpHJ3tmzZUukQ48qE1gdhZjnAZGAQUAQsNbM57r46arO7gUfdfbqZDQBuAy41s1OBvkDZ1SaLgX7AwrDqTUkVQ6Jw2oH9DnG/V4vY7ymhatu2LUVFRWzevDnZpUiWa9KkCW3btq1+wyhhdlL3Ata5+8cAZvYkcD4QHRC5wHXB89eB54LnDjQBGgMGNAK+CLHW9JA/5l8jl2rbKqhsPwVHKBo1alTt1bEiqSrMU0xHAxuiXhcFy6KtAC4Mnl8ANDezw939LSKBsSl4vOLuayp+gJmNM7NCMyvMur/QEnoqCp2KEpEDJHuY6/XAA2Y2BlgEbARKzezfgM5AWXtovpmd7u77TU/o7lOAKRC5DqLeqk410SHxzE/gvafq8F5BSDRpCTesr1NZIpLewmxBbATaRb1uGywr5+6fu/uF7t4DuClYtp1Ia+Jtd9/p7juBl4A+IdaaOS56KDEti93b1KIQyXJhBsRSoKOZdTCzxsAIYE70BmbW2szKargRmBo8/wzoZ2YNzawRkQ7qA04xSTUSERQ69SSStUI7xeTuJWY2HngFyAGmuvsqM5sIFLr7HKA/cJuZOZFTTNcEu88GBgDvEemwftnd54ZTaQP2v6tcBl4aUhYSdflFH72vOrRFsoLmYrqlFXjU7JKWAxO2Jq6wdFCn4FBYiKQzzcVUlZxGUFK6/+tsU/ZLfsOSym9kVOm+UeHSoFFk6hARyQgZeD6lho4+qerX2aRdr7q1CPbtVZ+FSAZRQJxZEDmtBJF/zyxIXi2pIpGd2woLkbSlPgiInFpZ/wa0Pz20G7WkvYTcBEn9FSKppqo+CAWE1FyiWgUKDJGkUye1JFbFX+x1nRdKQSGSkhQQUnd1vc6ibL+cg+A3XyamJhGpMwWEJE50S6A2YVH6rVoVIilEASHhSFSrouL7iUi9UUBIuNRfIZK2dB2E1K+6XmOhaytE6o0CQpJDQSGS8nSKSZKrrh3bmmVWJDRqQUjqqOttVNWiEEkotSAkNdW2ZaEWhUjCqAUhqa+2rQr1U4jUiVoQkj5qe22FrqkQqRUFhKSfRHVsa2oPkSrpFJOkt7q0Bsqm9tCpKJGYFBCS/hJxgyOIhMSkI+v+PiIZQqeYJHPU9dQTQMkuTeshEgg1IMxsCPDfQA7wZ3e/vcL6Y4GpQBtgKzDK3YuCdccAfwbaAQ6c4+7rw6xXMkgiwkJDZiXLhXZHOTPLAT4CBgFFwFJgpLuvjtrmaeB5d59uZgOAH7v7pcG6hcCt7j7fzJoB+9x9V2WfpzvKSVx061SR/STrjnK9gHXu/nFQxJPA+cDqqG1ygeuC568DzwXb5gIN3X0+gLvvDLFOySaJmF1WLQvJEmF2Uh8NbIh6XRQsi7YCuDB4fgHQ3MwOBzoB283sWTN718zuClok+zGzcWZWaGaFmzdvDuEQJONp0kCRSiV7FNP1QD8zexfoB2wESom0bE4P1p8MHAeMqbizu09x93x3z2/Tpk29FS0ZSEEhcoAwTzFtJNLBXKZtsKycu39O0III+hkucvftZlYELI86PfUc0Bt4OMR6RRJ3EZ5OPUkGCDMglgIdzawDkWAYAfwoegMzaw1sdfd9wI1ERjSV7XuYmbVx983AAEA90FK/6hIWCgrJAKEFhLuXmNl44BUiw1ynuvsqM5sIFLr7HKA/cJuZObAIuCbYt9TMrgcWmJkBy4CHwqpVpFp1nV1WQSFpKLRhrvVNw1yl3tW4VaGQkNSTrGGuIpmtprPLlm3X4hj4xXvh1CSSQAoIkbqqaVAUf/avbS0HJmwNpy6ROkr2MFeRzFGbobJequGxkrLUghBJtNrc2Khs29bHw/glia9JpBbUghAJS206pb/6UBfdScpQQIiEqey0U9+f12JfBYUkl4a5iiSDhshKiqhqmKtaECLJUNNf+GpNSBIoIESSpTajnhQUUo80ikkk2WozjYem8JB6oBaESCqpaatCLQoJkQJCJBXp1JOkAAWESKpSH4UkmQJCJNUpKCRJ4goIM+trZvPN7CMz+9jMPjGzj8MuTkSi1DYoRGop3lFMDwO/IHLjntLwyhGRatVmmvEmLeGG9aGVJJkp3lNMxe7+krt/6e5byh6hViYiVatJi2L3tkhQbNBEgBK/eFsQr5vZXcCzwLdlC939nVCqEpH41aRF8fCg/fcRqUK8AXFK8G/0fB0ODEhsOSJSazUJCl1oJ3GIKyDc/fthFyIiCVJQXLP+CYWEVCLeUUwtzOz3ZlYYPO4xMw2PEElVNemf0JBYqUS8ndRTgX8C/x48dgCPVLeTmQ0xsw/NbJ2Z3RBj/bFmtsDMVprZQjNrW2H9oWZWZGYPxFmniETTtB1SB/EGxPfcfYK7fxw8bgGOq2oHM8sBJgNnA7nASDPLrbDZ3cCj7p4HTARuq7D+t8CiOGsUkVg0v5PUUrwB8Y2ZnVb2wsz6At9Us08vYF0QKHuAJ4HzK2yTC7wWPH89er2ZnQQcAcyLs0YRqUptgqJwWmjlSOqLNyCuAiab2Xoz+xR4ALiymn2OBjZEvS4KlkVbAVwYPL8AaG5mh5tZA+Ae4PqqPsDMxpX1i2zevDnOQxHJcgXFkQvn4vH8z9SayGJxBYS7L3f3E4E8oJu793D3FQn4/OuBfmb2LtAP2EjkSu2rgRfdvaiauqa4e76757dp0yYB5YhkiRvW67STVKvKYa5mNsrdZ5rZdRWWA+Duv69i941Au6jXbYNl5dz9c4IWhJk1Ay5y9+1m1gc43cyuBpoBjc1sp7sf0NEtInVQm2k7oveTjFbddRBNg3+b1+K9lwIdzawDkWAYAfwoegMzaw1sdfd9wI1ERkvh7pdEbTMGyFc4iIRIQSExVBkQ7v6n4N9bavrG7l5iZuOBV4AcYKq7rzKziUChu88B+gO3mZkTGa10TU0/R0QSqKA40jH9/M/i3F4X2mUyc/fqNzK7E5hEZOTSy0T6In7h7jPDLS9++fn5XlhYmOwyRDJHTfscFBRpycyWuXt+rHXxjmIa7O47gHOB9cC/Af+VmPJEJCXp+omsF29AlJ2K+gHwtLvrTwWRbKGgyFrxBsTzZvYBcBKwwMzaALvDK0tEUk5tgkLSWrzXQdwAnEpkNNFe4GsOvCpaRLKBWhNZo7rrIAa4+2tmdmHUsuhNng2rMBFJYRoWmxWquw6iH5G5ks6Lsc5RQIhkt9oEhUIibcQ1zDUdaJirSAqIOygUEqmizsNczex3ZnZY1OuWZjYpUQWKSIbQTYoySryjmM529+1lL9x9G3BOOCWJSFrT3ewyRrwBkWNmB5W9MLODgYOq2F5Esl1Ng0JSTrwB8RiR6x8uN7PLgfnA9PDKEpGMoZBIW3F3UpvZEODM4OV8d38ltKpqQZ3UIimuJgGgTux6U1UndXXDXKOtAUrc/VUzO8TMmrv7PxNToohkvJoMidVw2JQQ7yimnwCzgT8Fi44GngurKBHJYPH2TagDO+ni7YO4BugL7ABw97XAd8IqSkSygPomUl68AfGtu+8pe2FmDYlcSS0iUnsKiZQWb0D8zcx+BRxsZoOAp4G54ZUlIllDp5xSVrwB8UtgM/AecAXwIvDrsIoSkSyk1kTKqXYUk5nlAKvc/QTgofBLEpGsFe9IJ80OWy+qbUG4eynwoZkdUw/1iIioNZEi4j3F1BJYZWYLzGxO2aO6ncxsiJl9aGbrzOyGGOuPDd5zpZktNLO2wfLuZvaWma0K1l1cs8MSkbSn+ZySLq4rqc2sX6zl7v63KvbJAT4CBgFFwFJgpLuvjtrmaeB5d59uZgOAH7v7pWbWKfL2vtbMjgKWAZ2jJwysSFdSi2QwTSMemlpP921mTczs58Bw4ATgTXf/W9mjms/tBaxz94+DIbJPcuBtSnOJ3JAI4PWy9e7+UXCtBe7+OfAl0KaazxORTKVTTklR3Smm6UA+kdFLZwP31OC9jwY2RL0uCpZFWwGU3c70AqC5mR0evYGZ9QIaA/+owWeLSKbRcNh6V11A5Lr7KHf/EzAMOD3Bn3890M/M3iVye9ONQGnZSjM7EphB5NTTvoo7m9k4Mys0s8LNmzcnuDQRSUlqTdSb6gJib9kTdy+p4XtvBNpFvW4bLCvn7p+7+4Xu3gO4KVi2HcDMDgVeAG5y97djfYC7T3H3fHfPb9NGZ6BEsoZaE/WiuoA40cx2BI9/Anllz81sRzX7LgU6mlkHM2sMjAD2G/lkZq3NrKyGG4GpwfLGwF+AR919dk0PSkSyhFoToaoyINw9x90PDR7N3b1h1PNDq9m3BBgPvEJkqvCn3H2VmU00s6HBZv2JXGPxEXAEcGuw/N+BM4AxZrY8eHSv/WGKSMZSSIQm7hsGpToNcxURDYetuVoPcxURSStqTSSUAkJEMktNOrClSgoIEclMCok6U0CISOZSSNSJAkJEMptCotYUECKS+RQStaKAEJHsEE/ntUJiPwoIEckuCom4KSBEJPvEExIKCgWEiGQp9UtUSwEhItlLIVElBYSIZDeFRKUUECIiComYFBAiIhD/MNiJreunnhSggBARiVZdSOzbmzWtCQWEiEhFOuUEKCBERGJTSCggREQqleUhoYAQEalKFs/hpIAQEYlHFoaEAkJEJF5ZFhKhBoSZDTGzD81snZndEGP9sWa2wMxWmtlCM2sbtW60ma0NHqPDrFNEJG5ZFBKhBYSZ5QCTgbOBXGCkmeVW2Oxu4FF3zwMmArcF+7YCJgCnAL2ACWbWMqxaRURqJEtCIswWRC9gnbt/7O57gCeB8ytskwu8Fjx/PWr9WcB8d9/q7tuA+cCQEGsVEamZLAiJMAPiaGBD1OuiYFm0FcCFwfMLgOZmdnic+2Jm48ys0MwKN2/enLDCRUTiEs8w2DSW7E7q64F+ZvYu0A/YCJTGu7O7T3H3fHfPb9OmTVg1iohUrqqQSPNWRJgBsRFoF/W6bbCsnLt/7u4XunsP4KZg2fZ49hURSRkZGhJhBsRSoKOZdTCzxsAIYE70BmbW2szKargRmBo8fwUYbGYtg87pwcEyEZHUdNRJla9L05AILSDcvQQYT+QX+xrgKXdfZWYTzWxosFl/4EMz+wg4Arg12Hcr8FsiIbMUmBgsExFJTeNeq3p9GoaEuXuya0iI/Px8LywsTHYZIpLtqguCFOvYNrNl7p4fa12yO6lFRDJLigVAXSggREQSLUM6rRUQIiJhyICQUECIiISl4SGVr0uDkFBAiIiE5debql6f4iGhgBARCVMad1orIEREwpam/REKCBGR+pCGIaGAEBGpL2nWaa2AEBGpL9V1WqcYBYSISH1Ko1NNCggRkfqWJiGhgBARSYY06I9QQIiIJEMaXESngBARSZYUv4hOASEikkwp3B+hgBARSbYUDQkFhIiIxKSAEBFJBSnYilBAiIikihQLCQWEiIjEFGpAmNkQM/vQzNaZ2Q0x1h9jZq+b2btmttLMzgmWNzKz6Wb2npmtMbMbw6xTRCRlpFArIrSAMLMcYDJwNpALjDSz3Aqb/Rp4yt17ACOAPwbLhwMHuXs34CTgCjNrH1atIiIpJUWujwizBdELWOfuH7v7HuBJ4PwK2zhwaPC8BfB51PKmZtYQOBjYA+wIsVYRkfRQj62IMAPiaGBD1OuiYFm0AmCUmRUBLwI/DZbPBr4GNgGfAXe7+9aKH2Bm48ys0MwKN2/enODyRUSSKAVaEcnupB4JTHP3tsA5wAwza0Ck9VEKHAV0AP7TzI6ruLO7T3H3fHfPb9OmTX3WLSKSPPXUiggzIDYC7aJetw2WRbsceArA3d8CmgCtgR8BL7v7Xnf/EngTyA+xVhGR1JPkVkSYAbEU6GhmHcysMZFO6DkVtvkMGAhgZp2JBMTmYPmAYHlToDfwQYi1ioikl3poRYQWEO5eAowHXgHWEBmttMrMJprZ0GCz/wR+YmYrgCeAMe7uREY/NTOzVUSC5hF3XxlWrSIiKSuJrQiL/D5Of/n5+V5YWJjsMkREEq+q1kIdA8TMlrl7zFP4ye6kFhGR6iSpFaGAEBGRmBQQIiLpoLJWRIid1QoIERGJSQEhIpIucg6KvTykVoQCQkQkXfzmy475hOwAAAdlSURBVHr9OAWEiIjEpIAQEUkn9dhZrYAQEZGYFBAiIummcfN6+RgFhIhIuvlVUezlCT7NpIAQEZGYGia7gFRx8Z/eOmDZuXlHcmmf9nyzp5Qxjyw5YP2wk9oyPL8dW7/ew1Uzlx2wflTvYznvxKP4fPs3/GLW8gPW/+T04zgz9wj+sXknv3r2vQPW/3RAR07r2JpVnxczce7qA9b/vyHHc9KxrVj26VbufPnDA9bffF4uXY5qweK1X3H/a2sPWP+7C7vxvTbNeHX1Fzz0xscHrL/34u4cddjBzF3xOTPf/vSA9f8z6iRaNW3M04UbmL3swL9opv24Fwc3zmHGW+t5fuWmA9bPuqIPAFMW/YMFa/YfvtekUQ7Tx/YC4L4Fa3lz3Vf7rW95SGMevPQkAO54+QPe+XTbfuuPbNGEP4zoAcAtc1ex+vP971h7XJum3HZhHgA3PruSjzd/vd/63KMOZcJ5XQD4+ZPvsql4937rex7bkl8OOQGAK2csY9uuPfut7/tvrbl2YEcARk9dwu69pfutH9j5O4w743uAfvb0s1f7nz0H7ICjSxy1IERE0lFBMU4kJMKi6b5FRNJVrD6HGs78qum+RUQyUcUwSPC04OqDEBFJZyHeK0ItCBERiUkBISIiMSkgREQkJgWEiIjEpIAQEZGYFBAiIhJTxlwoZ2abgQOvyY9fa+CrarfKLNl2zNl2vKBjzhZ1OeZj3b1NrBUZExB1ZWaFlV1NmKmy7Ziz7XhBx5wtwjpmnWISEZGYFBAiIhKTAuJfpiS7gCTItmPOtuMFHXO2COWY1QchIiIxqQUhIiIxKSBERCSmrAoIMxtiZh+a2TozuyHG+oPMbFaw/u9m1r7+q0ysOI75OjNbbWYrzWyBmR2bjDoTqbpjjtruIjNzM0v7IZHxHLOZ/XvwvV5lZo/Xd42JFsfP9jFm9rqZvRv8fJ+TjDoTxcymmtmXZvZ+JevNzO4Lvh4rzaxnnT/U3bPiAeQA/wCOAxoDK4DcCttcDTwYPB8BzEp23fVwzN8HDgmeX5UNxxxs1xxYBLwN5Ce77nr4PncE3gVaBq+/k+y66+GYpwBXBc9zgfXJrruOx3wG0BN4v5L15wAvEblNdW/g73X9zGxqQfQC1rn7x+6+B3gSOL/CNucD04Pns4GBZhbmPcHDVu0xu/vr7r4rePk20Laea0y0eL7PAL8F7gB2x1iXbuI55p8Ak919G4C7f1nPNSZaPMfswKHB8xbA5/VYX8K5+yJgaxWbnA886hFvA4eZ2ZF1+cxsCoijgQ1Rr4uCZTG3cfcSoBg4vF6qC0c8xxztciJ/gaSzao85aHq3c/cX6rOwEMXzfe4EdDKzN83sbTMbUm/VhSOeYy4ARplZEfAi8NP6KS1pavr/vVq65agAYGajgHygX7JrCZOZNQB+D4xJcin1rSGR00z9ibQSF5lZN3ffntSqwjUSmObu95hZH2CGmXV1933JLixdZFMLYiPQLup122BZzG3MrCGRZumWeqkuHPEcM2Z2JnATMNTdv62n2sJS3TE3B7oCC81sPZFztXPSvKM6nu9zETDH3fe6+yfAR0QCI13Fc8yXA08BuPtbQBMik9plqrj+v9dENgXEUqCjmXUws8ZEOqHnVNhmDjA6eD4MeM2D3p80Ve0xm1kP4E9EwiHdz0tDNcfs7sXu3trd27t7eyL9LkPdvTA55SZEPD/bzxFpPWBmrYmccvq4PotMsHiO+TNgIICZdSYSEJvrtcr6NQe4LBjN1BsodvdNdXnDrDnF5O4lZjYeeIXICIip7r7KzCYChe4+B3iYSDN0HZHOoBHJq7ju4jzmu4BmwNNBf/xn7j40aUXXUZzHnFHiPOZXgMFmthooBf7L3dO2dRznMf8n8JCZ/YJIh/WYdP6Dz8yeIBLyrYN+lQlAIwB3f5BIP8s5wDpgF/DjOn9mGn+9REQkRNl0iklERGpAASEiIjEpIEREJCYFhIiIxKSAEBGRmBQQIjVgZqVmttzM3jezuWZ2WILff31wnQJmtjOR7y1SUwoIkZr5xt27u3tXItfKXJPsgkTCooAQqb23CCZDM7PvmdnLZrbMzN4wsxOC5UeY2V/MbEXwODVY/lyw7SozG5fEYxCpVNZcSS2SSGaWQ2Qah4eDRVOAK919rZmdAvwRGADcB/zN3S8I9mkWbD/W3bea2cHAUjN7Jp2vbJbMpIAQqZmDzWw5kZbDGmC+mTUDTuVf05UAHBT8OwC4DMDdS4lMIQ9wrZldEDxvR2TiPAWEpBQFhEjNfOPu3c3sECLzAF0DTAO2u3v3eN7AzPoDZwJ93H2XmS0kMpGcSEpRH4RILQR34buWyIRwu4BPzGw4lN8b+MRg0wVEbuWKmeWYWQsi08hvC8LhBCJTjoukHAWESC25+7vASiI3prkEuNzMVgCr+NftL38GfN/M3gOWEbk38stAQzNbA9xOZMpxkZSj2VxFRCQmtSBERCQmBYSIiMSkgBARkZgUECIiEpMCQkREYlJAiIhITAoIERGJ6f8AWXR/zZ1fjb4AAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Validation Loss: 0.55\n",
            "  Validation took: 0:14:17\n",
            "\n",
            "======== Epoch 5 / 8 ========\n",
            "Training...\n",
            "  Batch    40  of  7,606.    Elapsed: 0:00:57.\n",
            "  Batch    80  of  7,606.    Elapsed: 0:01:54.\n",
            "  Batch   120  of  7,606.    Elapsed: 0:02:51.\n",
            "  Batch   160  of  7,606.    Elapsed: 0:03:48.\n",
            "  Batch   200  of  7,606.    Elapsed: 0:04:45.\n",
            "  Batch   240  of  7,606.    Elapsed: 0:05:42.\n",
            "  Batch   280  of  7,606.    Elapsed: 0:06:39.\n",
            "  Batch   320  of  7,606.    Elapsed: 0:07:36.\n",
            "  Batch   360  of  7,606.    Elapsed: 0:08:33.\n",
            "  Batch   400  of  7,606.    Elapsed: 0:09:30.\n",
            "  Batch   440  of  7,606.    Elapsed: 0:10:27.\n",
            "  Batch   480  of  7,606.    Elapsed: 0:11:24.\n",
            "  Batch   520  of  7,606.    Elapsed: 0:12:21.\n",
            "  Batch   560  of  7,606.    Elapsed: 0:13:18.\n",
            "  Batch   600  of  7,606.    Elapsed: 0:14:15.\n",
            "  Batch   640  of  7,606.    Elapsed: 0:15:12.\n",
            "  Batch   680  of  7,606.    Elapsed: 0:16:09.\n",
            "  Batch   720  of  7,606.    Elapsed: 0:17:06.\n",
            "  Batch   760  of  7,606.    Elapsed: 0:18:03.\n",
            "  Batch   800  of  7,606.    Elapsed: 0:19:00.\n",
            "  Batch   840  of  7,606.    Elapsed: 0:19:57.\n",
            "  Batch   880  of  7,606.    Elapsed: 0:20:54.\n",
            "  Batch   920  of  7,606.    Elapsed: 0:21:51.\n",
            "  Batch   960  of  7,606.    Elapsed: 0:22:48.\n",
            "  Batch 1,000  of  7,606.    Elapsed: 0:23:45.\n",
            "  Batch 1,040  of  7,606.    Elapsed: 0:24:42.\n",
            "  Batch 1,080  of  7,606.    Elapsed: 0:25:39.\n",
            "  Batch 1,120  of  7,606.    Elapsed: 0:26:36.\n",
            "  Batch 1,160  of  7,606.    Elapsed: 0:27:34.\n",
            "  Batch 1,200  of  7,606.    Elapsed: 0:28:31.\n",
            "  Batch 1,240  of  7,606.    Elapsed: 0:29:28.\n",
            "  Batch 1,280  of  7,606.    Elapsed: 0:30:25.\n",
            "  Batch 1,320  of  7,606.    Elapsed: 0:31:22.\n",
            "  Batch 1,360  of  7,606.    Elapsed: 0:32:19.\n",
            "  Batch 1,400  of  7,606.    Elapsed: 0:33:16.\n",
            "  Batch 1,440  of  7,606.    Elapsed: 0:34:13.\n",
            "  Batch 1,480  of  7,606.    Elapsed: 0:35:10.\n",
            "  Batch 1,520  of  7,606.    Elapsed: 0:36:07.\n",
            "  Batch 1,560  of  7,606.    Elapsed: 0:37:04.\n",
            "  Batch 1,600  of  7,606.    Elapsed: 0:38:01.\n",
            "  Batch 1,640  of  7,606.    Elapsed: 0:38:58.\n",
            "  Batch 1,680  of  7,606.    Elapsed: 0:39:55.\n",
            "  Batch 1,720  of  7,606.    Elapsed: 0:40:52.\n",
            "  Batch 1,760  of  7,606.    Elapsed: 0:41:49.\n",
            "  Batch 1,800  of  7,606.    Elapsed: 0:42:46.\n",
            "  Batch 1,840  of  7,606.    Elapsed: 0:43:43.\n",
            "  Batch 1,880  of  7,606.    Elapsed: 0:44:40.\n",
            "  Batch 1,920  of  7,606.    Elapsed: 0:45:37.\n",
            "  Batch 1,960  of  7,606.    Elapsed: 0:46:34.\n",
            "  Batch 2,000  of  7,606.    Elapsed: 0:47:31.\n",
            "  Batch 2,040  of  7,606.    Elapsed: 0:48:28.\n",
            "  Batch 2,080  of  7,606.    Elapsed: 0:49:25.\n",
            "  Batch 2,120  of  7,606.    Elapsed: 0:50:22.\n",
            "  Batch 2,160  of  7,606.    Elapsed: 0:51:19.\n",
            "  Batch 2,200  of  7,606.    Elapsed: 0:52:16.\n",
            "  Batch 2,240  of  7,606.    Elapsed: 0:53:13.\n",
            "  Batch 2,280  of  7,606.    Elapsed: 0:54:10.\n",
            "  Batch 2,320  of  7,606.    Elapsed: 0:55:07.\n",
            "  Batch 2,360  of  7,606.    Elapsed: 0:56:04.\n",
            "  Batch 2,400  of  7,606.    Elapsed: 0:57:01.\n",
            "  Batch 2,440  of  7,606.    Elapsed: 0:57:58.\n",
            "  Batch 2,480  of  7,606.    Elapsed: 0:58:55.\n",
            "  Batch 2,520  of  7,606.    Elapsed: 0:59:52.\n",
            "  Batch 2,560  of  7,606.    Elapsed: 1:00:49.\n",
            "  Batch 2,600  of  7,606.    Elapsed: 1:01:46.\n",
            "  Batch 2,640  of  7,606.    Elapsed: 1:02:43.\n",
            "  Batch 2,680  of  7,606.    Elapsed: 1:03:40.\n",
            "  Batch 2,720  of  7,606.    Elapsed: 1:04:37.\n",
            "  Batch 2,760  of  7,606.    Elapsed: 1:05:34.\n",
            "  Batch 2,800  of  7,606.    Elapsed: 1:06:31.\n",
            "  Batch 2,840  of  7,606.    Elapsed: 1:07:28.\n",
            "  Batch 2,880  of  7,606.    Elapsed: 1:08:25.\n",
            "  Batch 2,920  of  7,606.    Elapsed: 1:09:22.\n",
            "  Batch 2,960  of  7,606.    Elapsed: 1:10:19.\n",
            "  Batch 3,000  of  7,606.    Elapsed: 1:11:16.\n",
            "  Batch 3,040  of  7,606.    Elapsed: 1:12:13.\n",
            "  Batch 3,080  of  7,606.    Elapsed: 1:13:10.\n",
            "  Batch 3,120  of  7,606.    Elapsed: 1:14:07.\n",
            "  Batch 3,160  of  7,606.    Elapsed: 1:15:04.\n",
            "  Batch 3,200  of  7,606.    Elapsed: 1:16:01.\n",
            "  Batch 3,240  of  7,606.    Elapsed: 1:16:58.\n",
            "  Batch 3,280  of  7,606.    Elapsed: 1:17:55.\n",
            "  Batch 3,320  of  7,606.    Elapsed: 1:18:52.\n",
            "  Batch 3,360  of  7,606.    Elapsed: 1:19:49.\n",
            "  Batch 3,400  of  7,606.    Elapsed: 1:20:46.\n",
            "  Batch 3,440  of  7,606.    Elapsed: 1:21:43.\n",
            "  Batch 3,480  of  7,606.    Elapsed: 1:22:40.\n",
            "  Batch 3,520  of  7,606.    Elapsed: 1:23:37.\n",
            "  Batch 3,560  of  7,606.    Elapsed: 1:24:34.\n",
            "  Batch 3,600  of  7,606.    Elapsed: 1:25:31.\n",
            "  Batch 3,640  of  7,606.    Elapsed: 1:26:28.\n",
            "  Batch 3,680  of  7,606.    Elapsed: 1:27:25.\n",
            "  Batch 3,720  of  7,606.    Elapsed: 1:28:23.\n",
            "  Batch 3,760  of  7,606.    Elapsed: 1:29:20.\n",
            "  Batch 3,800  of  7,606.    Elapsed: 1:30:17.\n",
            "  Batch 3,840  of  7,606.    Elapsed: 1:31:14.\n",
            "  Batch 3,880  of  7,606.    Elapsed: 1:32:11.\n"
          ]
        }
      ],
      "source": [
        "#Using class weight of cross entropy loss\n",
        "from sklearn.metrics import classification_report,auc,confusion_matrix,f1_score,precision_recall_curve,plot_precision_recall_curve,matthews_corrcoef\n",
        "import random\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "scaler = GradScaler()\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    roberta_model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        roberta_model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        with autocast():\n",
        "            loss, logits = roberta_model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "            b_labels1=torch.nn.functional.one_hot(b_labels, num_classes=2)\n",
        "            #Calculating weights\n",
        "            #positive=torch.sum(b_labels1, dim=0)\n",
        "           # negative=len(b_labels1)-positive\n",
        "            #negative\n",
        "            #pos_weight  = positive / negative\n",
        "            #criterion.pos_weight = pos_weight\n",
        "            loss1 = cross_entropy(logits,b_labels).to(device)\n",
        "           # print(\"loss:\",loss1)\n",
        "            loss1 = loss1 / gradient_accumulations\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss1.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        scaler.scale(loss1).backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "       # torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm*scaler.get_scale())\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        if ((step + 1) % gradient_accumulations == 0):\n",
        "             scaler.step(optimizer)\n",
        "       # Updates the scale for next iteration.\n",
        "             scaler.update()\n",
        "        # Update the learning rate.\n",
        "             scheduler.step()       \n",
        "             optimizer.zero_grad()\n",
        "            # roberta_model.zero_grad()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    roberta_model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    total_f1_score =0\n",
        "    predlist=[]\n",
        "    lbllist=[]\n",
        "    total_logits=[]\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            (loss, logits) = roberta_model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            #Converting the labels to one hot to sync with same shape as logits\n",
        "            b_labels1=torch.nn.functional.one_hot(b_labels, num_classes=2)\n",
        "            loss1 = cross_entropy1(logits, b_labels)\n",
        "       # print(\"loss1:\",loss1)\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss1.item()\n",
        "\n",
        "         #Converting for predictions by applying sigmoid to logits\n",
        "        pred_logits_sigmoid=torch.sigmoid(logits)\n",
        "        y_pred=torch.round(pred_logits_sigmoid)\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits_pred = y_pred.detach().cpu().numpy()\n",
        "        label_ids1 = b_labels.to('cpu').numpy()\n",
        "        logits=logits.detach().cpu().numpy()\n",
        "        #For confusion matrix and classification report to work we need same dimensions.\n",
        "        label_ids = b_labels1.to('cpu').numpy()\n",
        "        pred_logits_sigmoid = pred_logits_sigmoid.detach().cpu().numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids1)\n",
        "\n",
        "         #print(predictions)\n",
        "        predictions=np.argmax(logits_pred, axis=1)\n",
        "        y_test=np.argmax(label_ids,axis=1)\n",
        "        predlist.extend(predictions)\n",
        "        lbllist.extend(y_test)\n",
        "        #Accumulating the sigmoid positive logits for precision recall curve\n",
        "        total_logits.extend(pred_logits_sigmoid[:,1])\n",
        "        total_f1_score += f1_score(predictions,y_test, average = 'macro')\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    #f1 score\n",
        "\n",
        "    avg_f1_score =total_f1_score/len(validation_dataloader)\n",
        "    print(\"  F1 score: {0:.2f}\".format(avg_f1_score))\n",
        "\n",
        "     #classification report\n",
        "    print(classification_report(lbllist, predlist))  \n",
        "\n",
        "    #confusion matrix\n",
        "    cm = confusion_matrix(lbllist,predlist)\n",
        "    # constant for classes\n",
        "    print(cm)\n",
        "    #mcc score\n",
        "    print(\"Mathews Correlation coefficient score for this epoch is :\",round(matthews_corrcoef(lbllist, predlist),2))\n",
        "    #Precision recall curve plot\n",
        "    lr_precision, lr_recall, thresholds = precision_recall_curve(lbllist,total_logits)\n",
        "    lr_f1, lr_auc = f1_score( lbllist,predlist), auc(lr_recall, lr_precision)\n",
        "    print('Model validation score: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
        "    baseline = lbllist.count(1) / len(lbllist)\n",
        "    plt.plot([0, 1], [baseline, baseline], linestyle='--', label='baseline')\n",
        "    plt.plot(lr_recall, lr_precision, marker='.', label='Roberta model evaluation')\n",
        "    # axis labels\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    # show the legend\n",
        "    plt.legend()\n",
        "    # show the plot\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuyWXQKcnqjk",
        "outputId": "ebbd3136-33ac-4612-bd7b-1693234f7787"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1]"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predlist=[]\n",
        "predlist.extend(predictions)\n",
        "predlist\n",
        "#predlist.(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06PH50GKgou1",
        "outputId": "84750f23-3deb-4cb7-eb1c-2a7a86df054c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 1],\n",
              "       [0, 0]])"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "pred=[]\n",
        "label =[]\n",
        "label.append(y_test)\n",
        "#label = [a.squeeze().tolist() for a in label]\n",
        "label1=np.argmax(label,axis=1)\n",
        "pred.append(predictions)\n",
        "#pred = [a.squeeze().tolist() for a in pred]\n",
        "pred=np.argmax(pred,axis=1)\n",
        "confusion_matrix(label1, pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLzESgGsE-LM"
      },
      "outputs": [],
      "source": [
        "del roberta_model\n",
        "del optimizer\n",
        "del scheduler\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DirM6N22kFDX"
      },
      "source": [
        "XLNET"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import XLNetConfig,AutoConfig\n",
        "\n",
        "configuration = XLNetConfig.from_pretrained('xlnet-base-cased',output_attentions=False,output_hidden_states=False,num_labels=2)\n",
        "configuration.hidden_dropout_prob = 0.2\n",
        "configuration.attention_probs_dropout_prob = 0.2"
      ],
      "metadata": {
        "id": "c7bK06XArKFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBf_YlMvkIHx",
        "outputId": "258e3fe1-a632-461f-c8bd-f28248e18b65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "WARNING:transformers.modeling_utils:Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Base model s loaded\n"
          ]
        }
      ],
      "source": [
        "#XLNET\n",
        "xlnet_model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", # 12-layer, 768-hidden, 12-heads, 125M parameters RoBERTa using the BERT-base architecture\n",
        "                                                                   # num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                                                                                    # You can increase this for multi-class tasks.   \n",
        "                                                                   # output_attentions = False, # Whether the model returns attentions weights.\n",
        "                                                                  #  output_hidden_states = False # Whether the model returns all hidden-states.\n",
        "                                                              config =configuration\n",
        "                                                                )\n",
        "xlnet_tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "xlnet_model.cuda()\n",
        "\n",
        "\n",
        "print(' Base model s loaded')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfJMRQAUlTfl",
        "outputId": "67570449-2b4e-4a4e-8ad7-0984365f54d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized XLNET:  ['drinks', 'were', 'bad', '', ',', 'the', 'hot', 'chocolate', 'was', 'water', 'ed', 'down', 'and', 'the', 'la', 'tte', 'had', 'a', 'burnt', 'taste', 'to', 'it', '', '.', 'the', 'food', 'was', 'also', 'poor', 'quality', '', ',', 'but', 'the', 'service', 'was', 'the', 'worst', 'part', '', ',', 'their', '', 'cashier', 'was', 'very', 'rude', '', '.']\n",
            "Token IDs XLNET:  [7841, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 775, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2151, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Print the text split into tokens.\n",
        "print('Tokenized XLNET: ', xlnet_tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the text mapped to token ids.\n",
        "print('Token IDs XLNET: ', xlnet_tokenizer.convert_tokens_to_ids(roberta_tokenizer.tokenize(sentences[0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7kvumvMlZaq",
        "outputId": "c0ccd681-0d4b-4ae6-9c34-9292b6d94aea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Will Your Hometown Be Taking In Obamas Refugees? Heres The List Of Cities Where Theyre Being Transplanted \n",
            "XLNET: ['Will', 'Your', 'Home', 'town', 'Be', 'Taking', 'In', 'Obama', '', 's', 'Refugees', '?', 'Here', '', 's', 'The', 'List', 'Of', 'Cities', 'Where', 'They', '', 're', 'Being', 'Trans', 'plant', 'ed', '', '']\n"
          ]
        }
      ],
      "source": [
        "sequence = \"\"\"Will Your Hometown Be Taking In Obamas Refugees? Heres The List Of Cities Where Theyre Being Transplanted \"\"\"\n",
        "print(\"\"\"Will Your Hometown Be Taking In Obamas Refugees? Heres The List Of Cities Where Theyre Being Transplanted \"\"\")\n",
        "xlnet_tokenized_sequence=xlnet_tokenizer.tokenize(sequence)\n",
        "print(\"XLNET:\",xlnet_tokenized_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ThjNujalnC2",
        "outputId": "ac29329e-1ab5-4441-e9ee-67f18c0aae05"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('<pad>', 5)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "xlnet_tokenizer.pad_token,xlnet_tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTB5F8pklx_5"
      },
      "outputs": [],
      "source": [
        "token_lens = []\n",
        "\n",
        "for txt in sentences:\n",
        "    tokens = xlnet_tokenizer.encode(txt,truncation=True, max_length=512)\n",
        "    token_lens.append(len(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "4v-TZz0Suehl",
        "outputId": "a1dab50e-abe6-4013-c91d-80161eec972e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.histplot(token_lens)\n",
        "plt.xlim([0, 600]);\n",
        "plt.xlabel('Token count');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcfEjGZo6KSE",
        "outputId": "8e66ef94-888c-4e05-a647-9bf021c534a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  this was the worst experience i ve ever had a casual coffee light fare place . the server disappeared for minutes , just talking to his friend by the window as my girlfriend and i sat dumbfounded that this dude had the nerve to do that on the job . we re trying to make eye contact , but clearly getting paid to talk to his bud was more important to him . my girlfriend went up to the counter once the server disappeared into the back for another minutes what is this guy doing ? and asked if she should order food up there or something . the girl at the counter gives her a weird look and just says i ll get your server . when they arrive from the back , they look over at our table and have a laugh . yeah , leaving us hanging for half a goddamn hour at a place with only two other customers is not funny but in retrospect , your collective incompetence and false sense of entitlement certainly was . the food was okay . for a place called toast , i d figured the bread would be better , but it was just cold le bus . additionally , i m sure the andouille in my special was just a link of the pre packaged offering from trader joe s cut into four pieces . for unapologetic mediocrity will not be happening again . avoid this place like the plague . i almost didn t leave a tip , and honestly i shouldn t have . i felt the buyer s remorse all day . what a disgrace .\n",
            "Token IDs XLNET: tensor([   52,    30,    18,  2598,   656,    17,   150,    17,   189,   545,\n",
            "           54,    24, 10245,  2877,   697, 12040,   250,    17,     9,    18,\n",
            "         3441,  6239,    28,   641,    17,    19,   125,  1792,    22,    45,\n",
            "         1233,    37,    18,  2078,    34,    94,  8224,    21,    17,   150,\n",
            "         2162, 14240, 13538,    29,    52, 23196,    54,    18,  9629,    22,\n",
            "          112,    29,    31,    18,   625,    17,     9,    80,    17,    88,\n",
            "          619,    22,   144,  1715,  1056,    17,    19,    57,  2379,   723,\n",
            "         1373,    22,  1034,    22,    45,    17, 13352,    30,    70,   400,\n",
            "           22,   103,    17,     9,    94,  8224,   388,    76,    22,    18,\n",
            "         2610,   497,    18,  3441,  6239,    91,    18,   126,    28,   245,\n",
            "          641,   113,    27,    52,  2035,   690,    17,    82,    21,   442,\n",
            "          108,    85,   170,   374,   626,    76,   105,    49,   359,    17,\n",
            "            9,    18,  1615,    38,    18,  2610,  1849,    62,    24,  8189,\n",
            "          338,    21,   125,   349,    17,   150,    17,   215,   133,    73,\n",
            "         3441,    17,     9,    90,    63,  4623,    40,    18,   126,    17,\n",
            "           19,    63,   338,    95,    38,   120,  1324,    21,    47,    24,\n",
            "         5361,    17,     9, 11752,    17,    19,  1511,   211,  6020,    28,\n",
            "          455,    24, 30615,  1671,    38,    24,   250,    33,   114,    87,\n",
            "           86,  1391,    27,    50,  5787,    57,    25, 31601,    17,    19,\n",
            "           73,  5765, 29839,    21,  4417,  1186,    20, 25614,  2209,    30,\n",
            "           17,     9,    18,   626,    30,  4968,    17,     9,    28,    24,\n",
            "          250,   271, 15294,    17,    19,    17,   150,    17,    66,  8047,\n",
            "           18,  6111,    74,    39,   352,    17,    19,    57,    36,    30,\n",
            "          125,  1946,    17,   529,  2400,    17,     9, 21314,    17,    19,\n",
            "           17,   150,    17,    98,   512,    18,    21,  3498,  7587,    25,\n",
            "           94,   632,    30,   125,    24,  1730,    20,    18,     4,     3])\n",
            "labels: tensor([0, 0, 0,  ..., 1, 1, 1])\n"
          ]
        }
      ],
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "xlnet_input_ids =[]\n",
        "xlnet_attention_masks=[]\n",
        "sentence_ids = []\n",
        "counter = 0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    xlnet_encoded_dict = xlnet_tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 260,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt', \n",
        "                        truncation = True    # Return pytorch tensors.\n",
        "                   )\n",
        "    # Add the encoded sentence to the list.    \n",
        "    xlnet_input_ids.append(xlnet_encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    xlnet_attention_masks.append(xlnet_encoded_dict['attention_mask'])\n",
        "    \n",
        "    # collecting sentence_ids\n",
        "    sentence_ids.append(counter)\n",
        "    counter  = counter + 1\n",
        "    \n",
        "    \n",
        "    \n",
        "# Convert the lists into tensors.\n",
        "\n",
        "xlnet_input_ids = torch.cat(xlnet_input_ids, dim=0)\n",
        "xlnet_attention_masks = torch.cat(xlnet_attention_masks, dim=0)\n",
        "labels = torch.tensor(df_review_merge.label.values)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[1])\n",
        "print('Token IDs XLNET:', xlnet_input_ids[1])\n",
        "print('labels:', labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlvAh90Xpw34"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "# function to seed the script globally\n",
        "torch.manual_seed(1)\n",
        "xlnet_dataset = TensorDataset(xlnet_input_ids, xlnet_attention_masks, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNg983NXqDu0",
        "outputId": "7327a5c5-73e6-4350-be32-2a40cdd2e31f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "486,766 training samples\n",
            "121,692 validation samples\n"
          ]
        }
      ],
      "source": [
        "# Create a 80-20 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.8 * len(xlnet_dataset))\n",
        "val_size = len(xlnet_dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(xlnet_dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ldu4yTtcqWqM"
      },
      "outputs": [],
      "source": [
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXmjY2t9qeoU",
        "outputId": "4353cd4b-524a-47f6-c103-13fb3ecb565f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({1: 422366, 0: 64400})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "import collections as c\n",
        "train_classes = [df_review_merge.label[i] for i in train_dataset.indices]\n",
        "c.Counter(train_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uwAqFPhqnjH",
        "outputId": "81dc8e0f-a753-46b4-e865-be078c4f8799"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 64400 422366]\n",
            "[1.55279503e-05 2.36761482e-06]\n",
            "tensor([2.3676e-06, 2.3676e-06, 2.3676e-06,  ..., 2.3676e-06, 2.3676e-06,\n",
            "        2.3676e-06], dtype=torch.float64)\n",
            "38322\n",
            "<torch.utils.data.sampler.WeightedRandomSampler object at 0x7fec7e0d7310>\n"
          ]
        }
      ],
      "source": [
        "#Using weighted random sampler for class imbalance\n",
        "import numpy as np \n",
        "\n",
        "y_train_indices = train_dataset.indices\n",
        "\n",
        "y_train = [target[i] for i in y_train_indices]\n",
        "\n",
        "class_sample_count = np.array(\n",
        "    [len(np.where(y_train == t)[0]) for t in np.unique(y_train)])\n",
        "print(class_sample_count)\n",
        "weight = 1. / (class_sample_count)\n",
        "print(weight)\n",
        "samples_weight = np.array([weight[t] for t in y_train])\n",
        "samples_weight = torch.from_numpy(samples_weight)\n",
        "print(samples_weight)\n",
        "sampler = torch.utils.data.sampler.WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
        "print(next(iter(sampler)))\n",
        "print(sampler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5C4djohUqwmQ"
      },
      "outputs": [],
      "source": [
        "#Run if we are using weighted random sampler\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32 or 64.\n",
        "batch_size = 32\n",
        "gradient_accumulations=10\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = sampler, # Select batches randomly\n",
        "            batch_size = batch_size, # Trains with this batch size.\n",
        "            num_workers=2,\n",
        "            pin_memory = True\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size, # Evaluate with this batch size.\n",
        "            num_workers =2,\n",
        "            pin_memory = True\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJmvIxfaq524",
        "outputId": "20b343a6-6fb1-476e-d45d-a749aa743946"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels batch shape: <built-in method values of Tensor object at 0x7febff2c9e90>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([14, 18])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "#Testing the label counts in training data after applying weighted random sampler\n",
        "import collections as c\n",
        "train_labels = next(iter(train_dataloader))\n",
        "#print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels[0].values}\")\n",
        "#img = train_features[0].squeeze()\n",
        "label = train_labels[2]\n",
        "torch.bincount(label)\n",
        "#print(label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lSBpVVeOy28",
        "outputId": "9d760163-21ff-4029-ec43-d094eb3cd052"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15212"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "len(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmcVziFhrHe8",
        "outputId": "996550ea-df91-4362-f08b-6d4f1a6b05c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Xlnet model has 210 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "transformer.mask_emb                                     (1, 1, 768)\n",
            "transformer.word_embedding.weight                       (32000, 768)\n",
            "transformer.layer.0.rel_attn.q                          (768, 12, 64)\n",
            "transformer.layer.0.rel_attn.k                          (768, 12, 64)\n",
            "transformer.layer.0.rel_attn.v                          (768, 12, 64)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "transformer.layer.0.rel_attn.o                          (768, 12, 64)\n",
            "transformer.layer.0.rel_attn.r                          (768, 12, 64)\n",
            "transformer.layer.0.rel_attn.r_r_bias                       (12, 64)\n",
            "transformer.layer.0.rel_attn.r_s_bias                       (12, 64)\n",
            "transformer.layer.0.rel_attn.r_w_bias                       (12, 64)\n",
            "transformer.layer.0.rel_attn.seg_embed                   (2, 12, 64)\n",
            "transformer.layer.0.rel_attn.layer_norm.weight                (768,)\n",
            "transformer.layer.0.rel_attn.layer_norm.bias                  (768,)\n",
            "transformer.layer.0.ff.layer_norm.weight                      (768,)\n",
            "transformer.layer.0.ff.layer_norm.bias                        (768,)\n",
            "transformer.layer.0.ff.layer_1.weight                    (3072, 768)\n",
            "transformer.layer.0.ff.layer_1.bias                          (3072,)\n",
            "transformer.layer.0.ff.layer_2.weight                    (768, 3072)\n",
            "transformer.layer.0.ff.layer_2.bias                           (768,)\n",
            "transformer.layer.1.rel_attn.q                          (768, 12, 64)\n",
            "transformer.layer.1.rel_attn.k                          (768, 12, 64)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "sequence_summary.summary.weight                           (768, 768)\n",
            "sequence_summary.summary.bias                                 (768,)\n",
            "logits_proj.weight                                          (2, 768)\n",
            "logits_proj.bias                                                (2,)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(xlnet_model.named_parameters())\n",
        "\n",
        "print('The Xlnet model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NDzmXICrSgQ"
      },
      "outputs": [],
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in xlnet_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in xlnet_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5, eps = 1e-8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ry-Ux449rdN4"
      },
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "epochs = 1\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-w7dzl6Irudn",
        "outputId": "024152e5-70e0-4f57-a054-2eef6a1782a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 1 ========\n",
            "Training...\n",
            "  Batch    40  of  15,212.    Elapsed: 0:00:53.\n",
            "  Batch    80  of  15,212.    Elapsed: 0:01:45.\n",
            "  Batch   120  of  15,212.    Elapsed: 0:02:37.\n",
            "  Batch   160  of  15,212.    Elapsed: 0:03:28.\n",
            "  Batch   200  of  15,212.    Elapsed: 0:04:20.\n",
            "  Batch   240  of  15,212.    Elapsed: 0:05:12.\n",
            "  Batch   280  of  15,212.    Elapsed: 0:06:04.\n",
            "  Batch   320  of  15,212.    Elapsed: 0:06:56.\n",
            "  Batch   360  of  15,212.    Elapsed: 0:07:47.\n",
            "  Batch   400  of  15,212.    Elapsed: 0:08:39.\n",
            "  Batch   440  of  15,212.    Elapsed: 0:09:31.\n",
            "  Batch   480  of  15,212.    Elapsed: 0:10:23.\n",
            "  Batch   520  of  15,212.    Elapsed: 0:11:15.\n",
            "  Batch   560  of  15,212.    Elapsed: 0:12:06.\n",
            "  Batch   600  of  15,212.    Elapsed: 0:12:58.\n",
            "  Batch   640  of  15,212.    Elapsed: 0:13:50.\n",
            "  Batch   680  of  15,212.    Elapsed: 0:14:42.\n",
            "  Batch   720  of  15,212.    Elapsed: 0:15:34.\n",
            "  Batch   760  of  15,212.    Elapsed: 0:16:25.\n",
            "  Batch   800  of  15,212.    Elapsed: 0:17:17.\n",
            "  Batch   840  of  15,212.    Elapsed: 0:18:09.\n",
            "  Batch   880  of  15,212.    Elapsed: 0:19:01.\n",
            "  Batch   920  of  15,212.    Elapsed: 0:19:53.\n",
            "  Batch   960  of  15,212.    Elapsed: 0:20:44.\n",
            "  Batch 1,000  of  15,212.    Elapsed: 0:21:36.\n",
            "  Batch 1,040  of  15,212.    Elapsed: 0:22:28.\n",
            "  Batch 1,080  of  15,212.    Elapsed: 0:23:20.\n",
            "  Batch 1,120  of  15,212.    Elapsed: 0:24:12.\n",
            "  Batch 1,160  of  15,212.    Elapsed: 0:25:04.\n",
            "  Batch 1,200  of  15,212.    Elapsed: 0:25:55.\n",
            "  Batch 1,240  of  15,212.    Elapsed: 0:26:47.\n",
            "  Batch 1,280  of  15,212.    Elapsed: 0:27:39.\n",
            "  Batch 1,320  of  15,212.    Elapsed: 0:28:31.\n",
            "  Batch 1,360  of  15,212.    Elapsed: 0:29:23.\n",
            "  Batch 1,400  of  15,212.    Elapsed: 0:30:15.\n",
            "  Batch 1,440  of  15,212.    Elapsed: 0:31:06.\n",
            "  Batch 1,480  of  15,212.    Elapsed: 0:31:58.\n",
            "  Batch 1,520  of  15,212.    Elapsed: 0:32:50.\n",
            "  Batch 1,560  of  15,212.    Elapsed: 0:33:42.\n",
            "  Batch 1,600  of  15,212.    Elapsed: 0:34:34.\n",
            "  Batch 1,640  of  15,212.    Elapsed: 0:35:25.\n",
            "  Batch 1,680  of  15,212.    Elapsed: 0:36:17.\n",
            "  Batch 1,720  of  15,212.    Elapsed: 0:37:09.\n",
            "  Batch 1,760  of  15,212.    Elapsed: 0:38:01.\n",
            "  Batch 1,800  of  15,212.    Elapsed: 0:38:53.\n",
            "  Batch 1,840  of  15,212.    Elapsed: 0:39:44.\n",
            "  Batch 1,880  of  15,212.    Elapsed: 0:40:36.\n",
            "  Batch 1,920  of  15,212.    Elapsed: 0:41:28.\n",
            "  Batch 1,960  of  15,212.    Elapsed: 0:42:20.\n",
            "  Batch 2,000  of  15,212.    Elapsed: 0:43:12.\n",
            "  Batch 2,040  of  15,212.    Elapsed: 0:44:03.\n",
            "  Batch 2,080  of  15,212.    Elapsed: 0:44:55.\n",
            "  Batch 2,120  of  15,212.    Elapsed: 0:45:47.\n",
            "  Batch 2,160  of  15,212.    Elapsed: 0:46:39.\n",
            "  Batch 2,200  of  15,212.    Elapsed: 0:47:31.\n",
            "  Batch 2,240  of  15,212.    Elapsed: 0:48:22.\n",
            "  Batch 2,280  of  15,212.    Elapsed: 0:49:14.\n",
            "  Batch 2,320  of  15,212.    Elapsed: 0:50:06.\n",
            "  Batch 2,360  of  15,212.    Elapsed: 0:50:58.\n",
            "  Batch 2,400  of  15,212.    Elapsed: 0:51:50.\n",
            "  Batch 2,440  of  15,212.    Elapsed: 0:52:41.\n",
            "  Batch 2,480  of  15,212.    Elapsed: 0:53:33.\n",
            "  Batch 2,520  of  15,212.    Elapsed: 0:54:25.\n",
            "  Batch 2,560  of  15,212.    Elapsed: 0:55:17.\n",
            "  Batch 2,600  of  15,212.    Elapsed: 0:56:08.\n",
            "  Batch 2,640  of  15,212.    Elapsed: 0:57:00.\n",
            "  Batch 2,680  of  15,212.    Elapsed: 0:57:52.\n",
            "  Batch 2,720  of  15,212.    Elapsed: 0:58:44.\n",
            "  Batch 2,760  of  15,212.    Elapsed: 0:59:36.\n",
            "  Batch 2,800  of  15,212.    Elapsed: 1:00:27.\n",
            "  Batch 2,840  of  15,212.    Elapsed: 1:01:19.\n",
            "  Batch 2,880  of  15,212.    Elapsed: 1:02:11.\n",
            "  Batch 2,920  of  15,212.    Elapsed: 1:03:03.\n",
            "  Batch 2,960  of  15,212.    Elapsed: 1:03:54.\n",
            "  Batch 3,000  of  15,212.    Elapsed: 1:04:46.\n",
            "  Batch 3,040  of  15,212.    Elapsed: 1:05:38.\n",
            "  Batch 3,080  of  15,212.    Elapsed: 1:06:30.\n",
            "  Batch 3,120  of  15,212.    Elapsed: 1:07:22.\n",
            "  Batch 3,160  of  15,212.    Elapsed: 1:08:13.\n",
            "  Batch 3,200  of  15,212.    Elapsed: 1:09:05.\n",
            "  Batch 3,240  of  15,212.    Elapsed: 1:09:57.\n",
            "  Batch 3,280  of  15,212.    Elapsed: 1:10:49.\n",
            "  Batch 3,320  of  15,212.    Elapsed: 1:11:41.\n",
            "  Batch 3,360  of  15,212.    Elapsed: 1:12:32.\n",
            "  Batch 3,400  of  15,212.    Elapsed: 1:13:24.\n",
            "  Batch 3,440  of  15,212.    Elapsed: 1:14:16.\n",
            "  Batch 3,480  of  15,212.    Elapsed: 1:15:08.\n",
            "  Batch 3,520  of  15,212.    Elapsed: 1:15:59.\n",
            "  Batch 3,560  of  15,212.    Elapsed: 1:16:51.\n",
            "  Batch 3,600  of  15,212.    Elapsed: 1:17:43.\n",
            "  Batch 3,640  of  15,212.    Elapsed: 1:18:35.\n",
            "  Batch 3,680  of  15,212.    Elapsed: 1:19:27.\n",
            "  Batch 3,720  of  15,212.    Elapsed: 1:20:18.\n",
            "  Batch 3,760  of  15,212.    Elapsed: 1:21:10.\n",
            "  Batch 3,800  of  15,212.    Elapsed: 1:22:02.\n",
            "  Batch 3,840  of  15,212.    Elapsed: 1:22:54.\n",
            "  Batch 3,880  of  15,212.    Elapsed: 1:23:46.\n",
            "  Batch 3,920  of  15,212.    Elapsed: 1:24:37.\n",
            "  Batch 3,960  of  15,212.    Elapsed: 1:25:29.\n",
            "  Batch 4,000  of  15,212.    Elapsed: 1:26:21.\n",
            "  Batch 4,040  of  15,212.    Elapsed: 1:27:13.\n",
            "  Batch 4,080  of  15,212.    Elapsed: 1:28:05.\n",
            "  Batch 4,120  of  15,212.    Elapsed: 1:28:56.\n",
            "  Batch 4,160  of  15,212.    Elapsed: 1:29:48.\n",
            "  Batch 4,200  of  15,212.    Elapsed: 1:30:40.\n",
            "  Batch 4,240  of  15,212.    Elapsed: 1:31:32.\n",
            "  Batch 4,280  of  15,212.    Elapsed: 1:32:24.\n",
            "  Batch 4,320  of  15,212.    Elapsed: 1:33:15.\n",
            "  Batch 4,360  of  15,212.    Elapsed: 1:34:07.\n",
            "  Batch 4,400  of  15,212.    Elapsed: 1:34:59.\n",
            "  Batch 4,440  of  15,212.    Elapsed: 1:35:51.\n",
            "  Batch 4,480  of  15,212.    Elapsed: 1:36:43.\n",
            "  Batch 4,520  of  15,212.    Elapsed: 1:37:34.\n",
            "  Batch 4,560  of  15,212.    Elapsed: 1:38:26.\n",
            "  Batch 4,600  of  15,212.    Elapsed: 1:39:18.\n",
            "  Batch 4,640  of  15,212.    Elapsed: 1:40:10.\n",
            "  Batch 4,680  of  15,212.    Elapsed: 1:41:01.\n",
            "  Batch 4,720  of  15,212.    Elapsed: 1:41:53.\n",
            "  Batch 4,760  of  15,212.    Elapsed: 1:42:45.\n",
            "  Batch 4,800  of  15,212.    Elapsed: 1:43:37.\n",
            "  Batch 4,840  of  15,212.    Elapsed: 1:44:29.\n",
            "  Batch 4,880  of  15,212.    Elapsed: 1:45:20.\n",
            "  Batch 4,920  of  15,212.    Elapsed: 1:46:12.\n",
            "  Batch 4,960  of  15,212.    Elapsed: 1:47:04.\n",
            "  Batch 5,000  of  15,212.    Elapsed: 1:47:56.\n",
            "  Batch 5,040  of  15,212.    Elapsed: 1:48:48.\n",
            "  Batch 5,080  of  15,212.    Elapsed: 1:49:39.\n",
            "  Batch 5,120  of  15,212.    Elapsed: 1:50:31.\n",
            "  Batch 5,160  of  15,212.    Elapsed: 1:51:23.\n",
            "  Batch 5,200  of  15,212.    Elapsed: 1:52:15.\n",
            "  Batch 5,240  of  15,212.    Elapsed: 1:53:06.\n",
            "  Batch 5,280  of  15,212.    Elapsed: 1:53:58.\n",
            "  Batch 5,320  of  15,212.    Elapsed: 1:54:50.\n",
            "  Batch 5,360  of  15,212.    Elapsed: 1:55:42.\n",
            "  Batch 5,400  of  15,212.    Elapsed: 1:56:33.\n",
            "  Batch 5,440  of  15,212.    Elapsed: 1:57:25.\n",
            "  Batch 5,480  of  15,212.    Elapsed: 1:58:17.\n",
            "  Batch 5,520  of  15,212.    Elapsed: 1:59:09.\n",
            "  Batch 5,560  of  15,212.    Elapsed: 2:00:01.\n",
            "  Batch 5,600  of  15,212.    Elapsed: 2:00:53.\n",
            "  Batch 5,640  of  15,212.    Elapsed: 2:01:44.\n",
            "  Batch 5,680  of  15,212.    Elapsed: 2:02:36.\n",
            "  Batch 5,720  of  15,212.    Elapsed: 2:03:28.\n",
            "  Batch 5,760  of  15,212.    Elapsed: 2:04:20.\n",
            "  Batch 5,800  of  15,212.    Elapsed: 2:05:11.\n",
            "  Batch 5,840  of  15,212.    Elapsed: 2:06:03.\n",
            "  Batch 5,880  of  15,212.    Elapsed: 2:06:55.\n",
            "  Batch 5,920  of  15,212.    Elapsed: 2:07:47.\n",
            "  Batch 5,960  of  15,212.    Elapsed: 2:08:39.\n",
            "  Batch 6,000  of  15,212.    Elapsed: 2:09:30.\n",
            "  Batch 6,040  of  15,212.    Elapsed: 2:10:22.\n",
            "  Batch 6,080  of  15,212.    Elapsed: 2:11:14.\n",
            "  Batch 6,120  of  15,212.    Elapsed: 2:12:06.\n",
            "  Batch 6,160  of  15,212.    Elapsed: 2:12:58.\n",
            "  Batch 6,200  of  15,212.    Elapsed: 2:13:49.\n",
            "  Batch 6,240  of  15,212.    Elapsed: 2:14:41.\n",
            "  Batch 6,280  of  15,212.    Elapsed: 2:15:33.\n",
            "  Batch 6,320  of  15,212.    Elapsed: 2:16:25.\n",
            "  Batch 6,360  of  15,212.    Elapsed: 2:17:17.\n",
            "  Batch 6,400  of  15,212.    Elapsed: 2:18:08.\n",
            "  Batch 6,440  of  15,212.    Elapsed: 2:19:00.\n",
            "  Batch 6,480  of  15,212.    Elapsed: 2:19:52.\n",
            "  Batch 6,520  of  15,212.    Elapsed: 2:20:44.\n",
            "  Batch 6,560  of  15,212.    Elapsed: 2:21:36.\n",
            "  Batch 6,600  of  15,212.    Elapsed: 2:22:27.\n",
            "  Batch 6,640  of  15,212.    Elapsed: 2:23:19.\n",
            "  Batch 6,680  of  15,212.    Elapsed: 2:24:11.\n",
            "  Batch 6,720  of  15,212.    Elapsed: 2:25:03.\n",
            "  Batch 6,760  of  15,212.    Elapsed: 2:25:54.\n",
            "  Batch 6,800  of  15,212.    Elapsed: 2:26:46.\n",
            "  Batch 6,840  of  15,212.    Elapsed: 2:27:38.\n",
            "  Batch 6,880  of  15,212.    Elapsed: 2:28:30.\n",
            "  Batch 6,920  of  15,212.    Elapsed: 2:29:22.\n",
            "  Batch 6,960  of  15,212.    Elapsed: 2:30:14.\n",
            "  Batch 7,000  of  15,212.    Elapsed: 2:31:05.\n",
            "  Batch 7,040  of  15,212.    Elapsed: 2:31:57.\n",
            "  Batch 7,080  of  15,212.    Elapsed: 2:32:49.\n",
            "  Batch 7,120  of  15,212.    Elapsed: 2:33:41.\n",
            "  Batch 7,160  of  15,212.    Elapsed: 2:34:32.\n",
            "  Batch 7,200  of  15,212.    Elapsed: 2:35:24.\n",
            "  Batch 7,240  of  15,212.    Elapsed: 2:36:16.\n",
            "  Batch 7,280  of  15,212.    Elapsed: 2:37:08.\n",
            "  Batch 7,320  of  15,212.    Elapsed: 2:38:00.\n",
            "  Batch 7,360  of  15,212.    Elapsed: 2:38:51.\n",
            "  Batch 7,400  of  15,212.    Elapsed: 2:39:43.\n",
            "  Batch 7,440  of  15,212.    Elapsed: 2:40:35.\n",
            "  Batch 7,480  of  15,212.    Elapsed: 2:41:27.\n",
            "  Batch 7,520  of  15,212.    Elapsed: 2:42:19.\n",
            "  Batch 7,560  of  15,212.    Elapsed: 2:43:10.\n",
            "  Batch 7,600  of  15,212.    Elapsed: 2:44:02.\n",
            "  Batch 7,640  of  15,212.    Elapsed: 2:44:54.\n",
            "  Batch 7,680  of  15,212.    Elapsed: 2:45:46.\n",
            "  Batch 7,720  of  15,212.    Elapsed: 2:46:37.\n",
            "  Batch 7,760  of  15,212.    Elapsed: 2:47:29.\n",
            "  Batch 7,800  of  15,212.    Elapsed: 2:48:21.\n",
            "  Batch 7,840  of  15,212.    Elapsed: 2:49:13.\n",
            "  Batch 7,880  of  15,212.    Elapsed: 2:50:04.\n",
            "  Batch 7,920  of  15,212.    Elapsed: 2:50:56.\n",
            "  Batch 7,960  of  15,212.    Elapsed: 2:51:48.\n",
            "  Batch 8,000  of  15,212.    Elapsed: 2:52:40.\n",
            "  Batch 8,040  of  15,212.    Elapsed: 2:53:32.\n",
            "  Batch 8,080  of  15,212.    Elapsed: 2:54:23.\n",
            "  Batch 8,120  of  15,212.    Elapsed: 2:55:15.\n",
            "  Batch 8,160  of  15,212.    Elapsed: 2:56:07.\n",
            "  Batch 8,200  of  15,212.    Elapsed: 2:56:59.\n",
            "  Batch 8,240  of  15,212.    Elapsed: 2:57:51.\n",
            "  Batch 8,280  of  15,212.    Elapsed: 2:58:42.\n",
            "  Batch 8,320  of  15,212.    Elapsed: 2:59:34.\n",
            "  Batch 8,360  of  15,212.    Elapsed: 3:00:26.\n",
            "  Batch 8,400  of  15,212.    Elapsed: 3:01:18.\n",
            "  Batch 8,440  of  15,212.    Elapsed: 3:02:10.\n",
            "  Batch 8,480  of  15,212.    Elapsed: 3:03:01.\n",
            "  Batch 8,520  of  15,212.    Elapsed: 3:03:53.\n",
            "  Batch 8,560  of  15,212.    Elapsed: 3:04:45.\n",
            "  Batch 8,600  of  15,212.    Elapsed: 3:05:37.\n",
            "  Batch 8,640  of  15,212.    Elapsed: 3:06:29.\n",
            "  Batch 8,680  of  15,212.    Elapsed: 3:07:20.\n",
            "  Batch 8,720  of  15,212.    Elapsed: 3:08:12.\n",
            "  Batch 8,760  of  15,212.    Elapsed: 3:09:04.\n",
            "  Batch 8,800  of  15,212.    Elapsed: 3:09:56.\n",
            "  Batch 8,840  of  15,212.    Elapsed: 3:10:48.\n",
            "  Batch 8,880  of  15,212.    Elapsed: 3:11:39.\n",
            "  Batch 8,920  of  15,212.    Elapsed: 3:12:31.\n",
            "  Batch 8,960  of  15,212.    Elapsed: 3:13:23.\n",
            "  Batch 9,000  of  15,212.    Elapsed: 3:14:15.\n",
            "  Batch 9,040  of  15,212.    Elapsed: 3:15:06.\n",
            "  Batch 9,080  of  15,212.    Elapsed: 3:15:58.\n",
            "  Batch 9,120  of  15,212.    Elapsed: 3:16:50.\n",
            "  Batch 9,160  of  15,212.    Elapsed: 3:17:42.\n",
            "  Batch 9,200  of  15,212.    Elapsed: 3:18:34.\n",
            "  Batch 9,240  of  15,212.    Elapsed: 3:19:25.\n",
            "  Batch 9,280  of  15,212.    Elapsed: 3:20:17.\n",
            "  Batch 9,320  of  15,212.    Elapsed: 3:21:09.\n",
            "  Batch 9,360  of  15,212.    Elapsed: 3:22:01.\n",
            "  Batch 9,400  of  15,212.    Elapsed: 3:22:53.\n",
            "  Batch 9,440  of  15,212.    Elapsed: 3:23:44.\n",
            "  Batch 9,480  of  15,212.    Elapsed: 3:24:36.\n",
            "  Batch 9,520  of  15,212.    Elapsed: 3:25:28.\n",
            "  Batch 9,560  of  15,212.    Elapsed: 3:26:20.\n",
            "  Batch 9,600  of  15,212.    Elapsed: 3:27:12.\n",
            "  Batch 9,640  of  15,212.    Elapsed: 3:28:03.\n",
            "  Batch 9,680  of  15,212.    Elapsed: 3:28:55.\n",
            "  Batch 9,720  of  15,212.    Elapsed: 3:29:47.\n",
            "  Batch 9,760  of  15,212.    Elapsed: 3:30:39.\n",
            "  Batch 9,800  of  15,212.    Elapsed: 3:31:30.\n",
            "  Batch 9,840  of  15,212.    Elapsed: 3:32:22.\n",
            "  Batch 9,880  of  15,212.    Elapsed: 3:33:14.\n",
            "  Batch 9,920  of  15,212.    Elapsed: 3:34:06.\n",
            "  Batch 9,960  of  15,212.    Elapsed: 3:34:58.\n",
            "  Batch 10,000  of  15,212.    Elapsed: 3:35:49.\n",
            "  Batch 10,040  of  15,212.    Elapsed: 3:36:41.\n",
            "  Batch 10,080  of  15,212.    Elapsed: 3:37:33.\n",
            "  Batch 10,120  of  15,212.    Elapsed: 3:38:25.\n",
            "  Batch 10,160  of  15,212.    Elapsed: 3:39:16.\n",
            "  Batch 10,200  of  15,212.    Elapsed: 3:40:08.\n",
            "  Batch 10,240  of  15,212.    Elapsed: 3:41:00.\n",
            "  Batch 10,280  of  15,212.    Elapsed: 3:41:52.\n",
            "  Batch 10,320  of  15,212.    Elapsed: 3:42:44.\n",
            "  Batch 10,360  of  15,212.    Elapsed: 3:43:35.\n",
            "  Batch 10,400  of  15,212.    Elapsed: 3:44:27.\n",
            "  Batch 10,440  of  15,212.    Elapsed: 3:45:19.\n",
            "  Batch 10,480  of  15,212.    Elapsed: 3:46:11.\n",
            "  Batch 10,520  of  15,212.    Elapsed: 3:47:03.\n",
            "  Batch 10,560  of  15,212.    Elapsed: 3:47:54.\n",
            "  Batch 10,600  of  15,212.    Elapsed: 3:48:46.\n",
            "  Batch 10,640  of  15,212.    Elapsed: 3:49:38.\n",
            "  Batch 10,680  of  15,212.    Elapsed: 3:50:30.\n",
            "  Batch 10,720  of  15,212.    Elapsed: 3:51:21.\n",
            "  Batch 10,760  of  15,212.    Elapsed: 3:52:13.\n",
            "  Batch 10,800  of  15,212.    Elapsed: 3:53:05.\n",
            "  Batch 10,840  of  15,212.    Elapsed: 3:53:57.\n",
            "  Batch 10,880  of  15,212.    Elapsed: 3:54:49.\n",
            "  Batch 10,920  of  15,212.    Elapsed: 3:55:40.\n",
            "  Batch 10,960  of  15,212.    Elapsed: 3:56:32.\n",
            "  Batch 11,000  of  15,212.    Elapsed: 3:57:24.\n",
            "  Batch 11,040  of  15,212.    Elapsed: 3:58:16.\n",
            "  Batch 11,080  of  15,212.    Elapsed: 3:59:08.\n",
            "  Batch 11,120  of  15,212.    Elapsed: 3:59:59.\n",
            "  Batch 11,160  of  15,212.    Elapsed: 4:00:51.\n",
            "  Batch 11,200  of  15,212.    Elapsed: 4:01:43.\n",
            "  Batch 11,240  of  15,212.    Elapsed: 4:02:35.\n",
            "  Batch 11,280  of  15,212.    Elapsed: 4:03:26.\n",
            "  Batch 11,320  of  15,212.    Elapsed: 4:04:18.\n",
            "  Batch 11,360  of  15,212.    Elapsed: 4:05:10.\n",
            "  Batch 11,400  of  15,212.    Elapsed: 4:06:02.\n",
            "  Batch 11,440  of  15,212.    Elapsed: 4:06:54.\n",
            "  Batch 11,480  of  15,212.    Elapsed: 4:07:45.\n",
            "  Batch 11,520  of  15,212.    Elapsed: 4:08:37.\n",
            "  Batch 11,560  of  15,212.    Elapsed: 4:09:29.\n",
            "  Batch 11,600  of  15,212.    Elapsed: 4:10:21.\n",
            "  Batch 11,640  of  15,212.    Elapsed: 4:11:12.\n",
            "  Batch 11,680  of  15,212.    Elapsed: 4:12:04.\n",
            "  Batch 11,720  of  15,212.    Elapsed: 4:12:56.\n",
            "  Batch 11,760  of  15,212.    Elapsed: 4:13:48.\n",
            "  Batch 11,800  of  15,212.    Elapsed: 4:14:40.\n",
            "  Batch 11,840  of  15,212.    Elapsed: 4:15:31.\n",
            "  Batch 11,880  of  15,212.    Elapsed: 4:16:23.\n",
            "  Batch 11,920  of  15,212.    Elapsed: 4:17:15.\n",
            "  Batch 11,960  of  15,212.    Elapsed: 4:18:07.\n",
            "  Batch 12,000  of  15,212.    Elapsed: 4:18:59.\n",
            "  Batch 12,040  of  15,212.    Elapsed: 4:19:50.\n",
            "  Batch 12,080  of  15,212.    Elapsed: 4:20:42.\n",
            "  Batch 12,120  of  15,212.    Elapsed: 4:21:34.\n",
            "  Batch 12,160  of  15,212.    Elapsed: 4:22:26.\n",
            "  Batch 12,200  of  15,212.    Elapsed: 4:23:18.\n",
            "  Batch 12,240  of  15,212.    Elapsed: 4:24:09.\n",
            "  Batch 12,280  of  15,212.    Elapsed: 4:25:01.\n",
            "  Batch 12,320  of  15,212.    Elapsed: 4:25:53.\n",
            "  Batch 12,360  of  15,212.    Elapsed: 4:26:45.\n",
            "  Batch 12,400  of  15,212.    Elapsed: 4:27:37.\n",
            "  Batch 12,440  of  15,212.    Elapsed: 4:28:28.\n",
            "  Batch 12,480  of  15,212.    Elapsed: 4:29:20.\n",
            "  Batch 12,520  of  15,212.    Elapsed: 4:30:12.\n",
            "  Batch 12,560  of  15,212.    Elapsed: 4:31:04.\n",
            "  Batch 12,600  of  15,212.    Elapsed: 4:31:55.\n",
            "  Batch 12,640  of  15,212.    Elapsed: 4:32:47.\n",
            "  Batch 12,680  of  15,212.    Elapsed: 4:33:39.\n",
            "  Batch 12,720  of  15,212.    Elapsed: 4:34:31.\n",
            "  Batch 12,760  of  15,212.    Elapsed: 4:35:23.\n",
            "  Batch 12,800  of  15,212.    Elapsed: 4:36:14.\n",
            "  Batch 12,840  of  15,212.    Elapsed: 4:37:06.\n",
            "  Batch 12,880  of  15,212.    Elapsed: 4:37:58.\n",
            "  Batch 12,920  of  15,212.    Elapsed: 4:38:50.\n",
            "  Batch 12,960  of  15,212.    Elapsed: 4:39:41.\n",
            "  Batch 13,000  of  15,212.    Elapsed: 4:40:33.\n",
            "  Batch 13,040  of  15,212.    Elapsed: 4:41:25.\n",
            "  Batch 13,080  of  15,212.    Elapsed: 4:42:17.\n",
            "  Batch 13,120  of  15,212.    Elapsed: 4:43:08.\n",
            "  Batch 13,160  of  15,212.    Elapsed: 4:44:00.\n",
            "  Batch 13,200  of  15,212.    Elapsed: 4:44:52.\n",
            "  Batch 13,240  of  15,212.    Elapsed: 4:45:44.\n",
            "  Batch 13,280  of  15,212.    Elapsed: 4:46:35.\n",
            "  Batch 13,320  of  15,212.    Elapsed: 4:47:27.\n",
            "  Batch 13,360  of  15,212.    Elapsed: 4:48:19.\n",
            "  Batch 13,400  of  15,212.    Elapsed: 4:49:11.\n",
            "  Batch 13,440  of  15,212.    Elapsed: 4:50:02.\n",
            "  Batch 13,480  of  15,212.    Elapsed: 4:50:54.\n",
            "  Batch 13,520  of  15,212.    Elapsed: 4:51:46.\n",
            "  Batch 13,560  of  15,212.    Elapsed: 4:52:38.\n",
            "  Batch 13,600  of  15,212.    Elapsed: 4:53:30.\n",
            "  Batch 13,640  of  15,212.    Elapsed: 4:54:21.\n",
            "  Batch 13,680  of  15,212.    Elapsed: 4:55:13.\n",
            "  Batch 13,720  of  15,212.    Elapsed: 4:56:05.\n",
            "  Batch 13,760  of  15,212.    Elapsed: 4:56:57.\n",
            "  Batch 13,800  of  15,212.    Elapsed: 4:57:49.\n",
            "  Batch 13,840  of  15,212.    Elapsed: 4:58:40.\n",
            "  Batch 13,880  of  15,212.    Elapsed: 4:59:32.\n",
            "  Batch 13,920  of  15,212.    Elapsed: 5:00:24.\n",
            "  Batch 13,960  of  15,212.    Elapsed: 5:01:16.\n",
            "  Batch 14,000  of  15,212.    Elapsed: 5:02:08.\n",
            "  Batch 14,040  of  15,212.    Elapsed: 5:02:59.\n",
            "  Batch 14,080  of  15,212.    Elapsed: 5:03:51.\n",
            "  Batch 14,120  of  15,212.    Elapsed: 5:04:43.\n",
            "  Batch 14,160  of  15,212.    Elapsed: 5:05:35.\n",
            "  Batch 14,200  of  15,212.    Elapsed: 5:06:27.\n",
            "  Batch 14,240  of  15,212.    Elapsed: 5:07:18.\n",
            "  Batch 14,280  of  15,212.    Elapsed: 5:08:10.\n",
            "  Batch 14,320  of  15,212.    Elapsed: 5:09:02.\n",
            "  Batch 14,360  of  15,212.    Elapsed: 5:09:54.\n",
            "  Batch 14,400  of  15,212.    Elapsed: 5:10:46.\n",
            "  Batch 14,440  of  15,212.    Elapsed: 5:11:37.\n",
            "  Batch 14,480  of  15,212.    Elapsed: 5:12:29.\n",
            "  Batch 14,520  of  15,212.    Elapsed: 5:13:21.\n",
            "  Batch 14,560  of  15,212.    Elapsed: 5:14:13.\n",
            "  Batch 14,600  of  15,212.    Elapsed: 5:15:04.\n",
            "  Batch 14,640  of  15,212.    Elapsed: 5:15:56.\n",
            "  Batch 14,680  of  15,212.    Elapsed: 5:16:48.\n",
            "  Batch 14,720  of  15,212.    Elapsed: 5:17:40.\n",
            "  Batch 14,760  of  15,212.    Elapsed: 5:18:32.\n",
            "  Batch 14,800  of  15,212.    Elapsed: 5:19:23.\n",
            "  Batch 14,840  of  15,212.    Elapsed: 5:20:15.\n",
            "  Batch 14,880  of  15,212.    Elapsed: 5:21:07.\n",
            "  Batch 14,920  of  15,212.    Elapsed: 5:21:59.\n",
            "  Batch 14,960  of  15,212.    Elapsed: 5:22:50.\n",
            "  Batch 15,000  of  15,212.    Elapsed: 5:23:42.\n",
            "  Batch 15,040  of  15,212.    Elapsed: 5:24:34.\n",
            "  Batch 15,080  of  15,212.    Elapsed: 5:25:26.\n",
            "  Batch 15,120  of  15,212.    Elapsed: 5:26:18.\n",
            "  Batch 15,160  of  15,212.    Elapsed: 5:27:09.\n",
            "  Batch 15,200  of  15,212.    Elapsed: 5:28:01.\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 5:28:16\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.72\n",
            "  F1 score: 0.60\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.63      0.37     16039\n",
            "           1       0.93      0.73      0.82    105653\n",
            "\n",
            "    accuracy                           0.72    121692\n",
            "   macro avg       0.60      0.68      0.59    121692\n",
            "weighted avg       0.84      0.72      0.76    121692\n",
            "\n",
            "[[10140  5899]\n",
            " [28495 77158]]\n",
            "Mathews Correlation coefficient score for this epoch is : 0.26\n",
            "Model validation score: f1=0.818 auc=0.947\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Validation Loss: 0.55\n",
            "  Validation took: 1:20:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 6:48:16 (h:mm:ss)\n"
          ]
        }
      ],
      "source": [
        "#using weighted random sampler\n",
        "from sklearn.metrics import classification_report,auc,confusion_matrix,f1_score,precision_recall_curve,plot_precision_recall_curve,matthews_corrcoef\n",
        "import random\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "scaler = GradScaler()\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    xlnet_model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        xlnet_model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        with autocast():\n",
        "            loss, logits = xlnet_model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "            b_labels1=torch.nn.functional.one_hot(b_labels, num_classes=2)\n",
        "            #Calculating weights\n",
        "            #positive=torch.sum(b_labels1, dim=0)\n",
        "           # negative=len(b_labels1)-positive\n",
        "            #negative\n",
        "            #pos_weight  = positive / negative\n",
        "            #criterion.pos_weight = pos_weight\n",
        "            loss1 = loss_fn(logits,b_labels1).to(device)\n",
        "           # print(\"loss:\",loss1)\n",
        "            loss1 = loss1 / gradient_accumulations\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss1.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        scaler.scale(loss1).backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "       # torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm*scaler.get_scale())\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        if ((step + 1) % gradient_accumulations == 0):\n",
        "             scaler.step(optimizer)\n",
        "       # Updates the scale for next iteration.\n",
        "             scaler.update()\n",
        "        # Update the learning rate.\n",
        "             scheduler.step()       \n",
        "             optimizer.zero_grad()\n",
        "            # xlnet_model.zero_grad()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    xlnet_model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    total_f1_score =0\n",
        "    predlist =[]\n",
        "    lbllist =[]\n",
        "    total_logits=[]\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            (loss, logits) = xlnet_model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            #Converting the labels to one hot to sync with same shape as logits\n",
        "            b_labels1=torch.nn.functional.one_hot(b_labels, num_classes=2)\n",
        "            loss1 = loss_fn(logits, b_labels1)\n",
        "       # print(\"loss1:\",loss1)\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss1.item()\n",
        "\n",
        "         #Converting for predictions by applying sigmoid to logits\n",
        "        pred_logits_sigmoid=torch.sigmoid(logits)\n",
        "        y_pred=torch.round(pred_logits_sigmoid)\n",
        "        \n",
        "        # Move logits and labels to CPU\n",
        "        logits_pred = y_pred.detach().cpu().numpy()\n",
        "        label_ids1 = b_labels.to('cpu').numpy()\n",
        "        logits=logits.detach().cpu().numpy()\n",
        "        #For confusion matrix and classification report to work we need same dimensions.\n",
        "        label_ids = b_labels1.to('cpu').numpy()\n",
        "        pred_logits_sigmoid=pred_logits_sigmoid.detach().cpu().numpy()\n",
        "     \n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids1)\n",
        "\n",
        "         #print(predictions)\n",
        "       # predictions=np.argmax(logits_pred, axis=1)\n",
        "        predictions =np.argmax(logits_pred,axis=1)\n",
        "        y_test=np.argmax(label_ids,axis=1)\n",
        "        predlist.extend(predictions)\n",
        "        lbllist.extend(y_test)\n",
        "        #Accumulating the sigmoid positive logits for precision recall curve\n",
        "        total_logits.extend(pred_logits_sigmoid[:,1])\n",
        "        #total_logits.extend(pred_logits_sigmoid)\n",
        "        total_f1_score += f1_score(predlist,lbllist, average = 'macro')\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    #f1 score\n",
        "\n",
        "    avg_f1_score =total_f1_score/len(validation_dataloader)\n",
        "    print(\"  F1 score: {0:.2f}\".format(avg_f1_score))\n",
        "\n",
        "     #classification report\n",
        "    print(classification_report(lbllist, predlist))  \n",
        "\n",
        "    #confusion matrix\n",
        "    cm = confusion_matrix(lbllist,predlist)\n",
        "    # constant for classes\n",
        "    print(cm)\n",
        "    #mcc score\n",
        "    print(\"Mathews Correlation coefficient score for this epoch is :\",round(matthews_corrcoef(lbllist, predlist),2))\n",
        "    #Precision recall curve plot\n",
        "    lr_precision, lr_recall, thresholds = precision_recall_curve(lbllist,total_logits)\n",
        "    lr_f1, lr_auc = f1_score( lbllist,predlist), auc(lr_recall, lr_precision)\n",
        "    print('Model validation score: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
        "    baseline = lbllist.count(1) / len(lbllist)\n",
        "    plt.plot([0, 1], [baseline, baseline], linestyle='--', label='baseline')\n",
        "    plt.plot(lr_recall, lr_precision, marker='.', label='Xlnet model evaluation')\n",
        "    # axis labels\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    # show the legend\n",
        "    plt.legend()\n",
        "    # show the plot\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gslb6tZrsNe_"
      },
      "outputs": [],
      "source": [
        "del xlnet_model\n",
        "del optimizer\n",
        "del scheduler\n",
        "torch.cuda.empty_cache()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}