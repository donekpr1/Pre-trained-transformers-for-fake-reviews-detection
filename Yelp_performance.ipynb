{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-qcbi5fUd42"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEeevkbCM6q9",
        "outputId": "13555628-5b7e-4667-a4bf-d8e619ccfbfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 217 ms, sys: 54.6 ms, total: 272 ms\n",
            "Wall time: 331 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "df1= pd.read_csv(\"/content/sample_data/metadata\",delimiter='\\t',encoding='utf-8',names=['User-id','restaurant-id','rating','label','date'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFt2kxM8XkTM"
      },
      "outputs": [],
      "source": [
        "df_review= pd.read_csv(\"/content/sample_data/reviewContent\",delimiter='\\t',names=['User-id','restaurant-id','date','review'],header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fogUjOJdt3Y",
        "outputId": "739a614d-b356-43b6-a582-0c8ee87b0e9e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(608458, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df_review.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ii5GtdsmXA7p"
      },
      "outputs": [],
      "source": [
        "df_review_merge=pd.merge(df1,df_review,how='inner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "pWRvPelmDzW2",
        "outputId": "2b4a154d-d0ed-4f35-e1f5-57b9b2083104"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   User-id  restaurant-id  rating  label        date  \\\n",
              "0     5044              0     1.0     -1  2014-11-16   \n",
              "1     5045              0     1.0     -1  2014-09-08   \n",
              "2     5046              0     3.0     -1  2013-10-06   \n",
              "3     5047              0     5.0     -1  2014-11-30   \n",
              "4     5048              0     5.0     -1  2014-08-28   \n",
              "\n",
              "                                              review  \n",
              "0  Drinks were bad, the hot chocolate was watered...  \n",
              "1  This was the worst experience I've ever had a ...  \n",
              "2  This is located on the site of the old Spruce ...  \n",
              "3  I enjoyed coffee and breakfast twice at Toast ...  \n",
              "4  I love Toast! The food choices are fantastic -...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e2330e24-cfde-4ec6-95ba-349793c2c053\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User-id</th>\n",
              "      <th>restaurant-id</th>\n",
              "      <th>rating</th>\n",
              "      <th>label</th>\n",
              "      <th>date</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5044</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>2014-11-16</td>\n",
              "      <td>Drinks were bad, the hot chocolate was watered...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5045</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>2014-09-08</td>\n",
              "      <td>This was the worst experience I've ever had a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5046</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>2013-10-06</td>\n",
              "      <td>This is located on the site of the old Spruce ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5047</td>\n",
              "      <td>0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>2014-11-30</td>\n",
              "      <td>I enjoyed coffee and breakfast twice at Toast ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5048</td>\n",
              "      <td>0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>2014-08-28</td>\n",
              "      <td>I love Toast! The food choices are fantastic -...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e2330e24-cfde-4ec6-95ba-349793c2c053')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e2330e24-cfde-4ec6-95ba-349793c2c053 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e2330e24-cfde-4ec6-95ba-349793c2c053');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df_review_merge.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_review_merge['review'] = [i.replace(\"&amp;amp;\", '').replace(\"\\'\",'') for i in df_review_merge['review']]"
      ],
      "metadata": {
        "id": "iWpZlmCAsvF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67-8mEUJXhvo",
        "outputId": "a67ae53d-4a7b-406b-ec2e-e724f2986f21"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(608458, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "df_review_merge.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wI0cjJjjfxD6",
        "outputId": "942cd0f7-3d89-439e-a4c8-2a1662c23758"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 1    528019\n",
              "-1     80439\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df_review_merge.label.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yx3kvU-rrRBo"
      },
      "outputs": [],
      "source": [
        "df_review_merge['label']=df_review_merge['label'].replace(-1,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "84kNINwJD3hR",
        "outputId": "9cf28cb9-5932-4d64-92d9-eb704277eb0f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   User-id  restaurant-id  rating  label        date  \\\n",
              "0     5044              0     1.0      0  2014-11-16   \n",
              "1     5045              0     1.0      0  2014-09-08   \n",
              "2     5046              0     3.0      0  2013-10-06   \n",
              "3     5047              0     5.0      0  2014-11-30   \n",
              "4     5048              0     5.0      0  2014-08-28   \n",
              "\n",
              "                                              review  \n",
              "0  Drinks were bad, the hot chocolate was watered...  \n",
              "1  This was the worst experience Ive ever had a c...  \n",
              "2  This is located on the site of the old Spruce ...  \n",
              "3  I enjoyed coffee and breakfast twice at Toast ...  \n",
              "4  I love Toast! The food choices are fantastic -...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0ad49d04-4252-4879-b35f-783bf2313dea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User-id</th>\n",
              "      <th>restaurant-id</th>\n",
              "      <th>rating</th>\n",
              "      <th>label</th>\n",
              "      <th>date</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5044</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2014-11-16</td>\n",
              "      <td>Drinks were bad, the hot chocolate was watered...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5045</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2014-09-08</td>\n",
              "      <td>This was the worst experience Ive ever had a c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5046</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2013-10-06</td>\n",
              "      <td>This is located on the site of the old Spruce ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5047</td>\n",
              "      <td>0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2014-11-30</td>\n",
              "      <td>I enjoyed coffee and breakfast twice at Toast ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5048</td>\n",
              "      <td>0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2014-08-28</td>\n",
              "      <td>I love Toast! The food choices are fantastic -...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0ad49d04-4252-4879-b35f-783bf2313dea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0ad49d04-4252-4879-b35f-783bf2313dea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0ad49d04-4252-4879-b35f-783bf2313dea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df_review_merge.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXizd8W7ixiM",
        "outputId": "7762741c-c922-4277-9a87-5c13cb31b618"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#removing character references amp that is not useful for the model\n",
        "df_review_merge['review'] = [i.replace(\"&amp;amp;\", '').replace(\"\\'\",'') for i in df_review_merge['review']]"
      ],
      "metadata": {
        "id": "moxBPJaW2jO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata"
      ],
      "metadata": {
        "id": "sRhp8i1d4l38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjLl8CmCTlAM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6237f28f-deca-4008-908d-9d3496c31d12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4min 51s, sys: 1.37 s, total: 4min 53s\n",
            "Wall time: 4min 52s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "def clean_text(txt):\n",
        "    \"\"\"\"\"\n",
        "    cleans the input text in the following steps\n",
        "    1- replace contractions\n",
        "    2- removing punctuation\n",
        "    3- spliting into words\n",
        "    4- removing stopwords\n",
        "    5- removing leftover punctuations\n",
        "    \"\"\"\"\"\n",
        "    contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "    def _get_contractions(contraction_dict):\n",
        "        contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
        "        return contraction_dict, contraction_re\n",
        "\n",
        "    def replace_contractions(text):\n",
        "        contractions, contractions_re = _get_contractions(contraction_dict)\n",
        "        def replace(match):\n",
        "            return contractions[match.group(0)]\n",
        "        return contractions_re.sub(replace, text)\n",
        "\n",
        "    # replace contractions and convert to lowercase\n",
        "    txt = txt.lower()\n",
        "    #txt = replace_contractions(txt)\n",
        "    \n",
        "    #remove punctuations\n",
        "    txt  = \"\".join([char for char in txt if char not in string.punctuation])\n",
        "    txt = re.sub('[0-9]+', '', txt)\n",
        "    txt= re.sub(r\"([.,!?])\", r\" \\1 \",txt)\n",
        "    txt = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", txt)\n",
        "    txt = re.sub(r'[^\\x00-\\x7f]',r'', txt)#remove non ascii\n",
        "    txt = unicodedata.normalize('NFKD', txt).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    txt = re.sub(' +', ' ', txt).strip() #remove leading and trailing white spaces\n",
        "    re.sub(\"(.)\\\\1{2,}\", \"\\\\1\",txt) #removing duplicate values like youuuuu to you\n",
        "  \n",
        "    \n",
        "    # split into words\n",
        "    words = word_tokenize(txt)\n",
        "    \n",
        "    # remove stopwords\n",
        "    #stop_words = set(stopwords.words('english'))\n",
        "   # words = [w for w in words if not w in stop_words]\n",
        "    \n",
        "    # removing leftover punctuations\n",
        "   # words = [word for word in words if word.isalpha()]\n",
        "\n",
        "   # lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "  #  def lemmatize_text(txt):\n",
        "  #      return [lemmatizer.lemmatize(w) for w in words]\n",
        "\n",
        "    cleaned_text = ' '.join(words)\n",
        "    return cleaned_text\n",
        "    \n",
        "df_review_merge['review'] = df_review_merge['review'].apply(lambda txt: clean_text(txt))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnEvINiDDgfD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "36d3bf1e-ee78-48a7-cc43-01fa8d342523"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'drinks were bad the hot chocolate was watered down and the latte had a burnt taste to it the food was also poor quality but the service was the worst part their cashier was very rude'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "df_review_merge['review'].iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3K-n8LScYCc",
        "outputId": "6f5cb9dd-433d-4abe-f4d6-787ac7a2bde1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    528019\n",
              "0     80439\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "df_review_merge.label.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWf5TBSKXz6z"
      },
      "outputs": [],
      "source": [
        "sentences=df_review_merge.review.values\n",
        "target=df_review_merge.label.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9dxoP0tzYgS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "257d6c43-1bb5-4f7a-8353-c4235be42cbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUXn5wkP0KiH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae03a829-bde3-48a2-fe3d-301048486844"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.2"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "1.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEiLuSHJYBGM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59722905-6feb-46c8-d73b-619ffd26a395"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers==3.0.0 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (21.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (0.0.53)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (0.8.0rc4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (3.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (4.64.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (0.1.97)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.0) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==3.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSSvqX0QYLRH"
      },
      "outputs": [],
      "source": [
        "#!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_LKzDsEkCK-",
        "outputId": "2da257b5-ca11-4124-ae6e-0c44cae3a673"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.97)\n"
          ]
        }
      ],
      "source": [
        "!pip install Sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYmqqbSRrV_S"
      },
      "outputs": [],
      "source": [
        "#!pip install -q torch==1.4.0 -f https://download.pytorch.org/whl/cu101/torch_stable.html\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaConfig,AutoConfig\n",
        "\n",
        "configuration = RobertaConfig.from_pretrained('roberta-base',output_attentions=False,output_hidden_states=False,num_labels=2)\n",
        "configuration.hidden_dropout_prob = 0.2\n",
        "configuration.attention_probs_dropout_prob = 0.1\n"
      ],
      "metadata": {
        "id": "BLnLYIznl6Si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-B0OUEA4jxWg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a28e5cf8-70be-470b-920c-1c8df08c9b06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "WARNING:transformers.modeling_utils:Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "#Loading pre trained models\n",
        "from transformers import (\n",
        "    BertForSequenceClassification,\n",
        "#     TFBertForSequenceClassification, \n",
        "                          BertTokenizer,\n",
        "#                           TFRobertaForSequenceClassification,\n",
        "                          RobertaForSequenceClassification,\n",
        "                          RobertaTokenizer,\n",
        "                          XLNetForSequenceClassification,\n",
        "                          XLNetTokenizer,\n",
        "                         AdamW)\n",
        "roberta_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", # 12-layer, 768-hidden, 12-heads, 125M parameters RoBERTa using the BERT-base architecture\n",
        "                                                                  # num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                                                                                    # You can increase this for multi-class tasks.   \n",
        "                                                                 #  output_attentions = False, # Whether the model returns attentions weights.\n",
        "                                                                 # output_hidden_states = False, #, # Whether the model returns all hidden-states.\n",
        "                                                                   config =configuration\n",
        "                                                                )\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "roberta_model.cuda()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aujKyQnlk4gF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4c030df-63a1-4f74-8824-8f352ff02ced"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Original:  drinks were bad the hot chocolate was watered down and the latte had a burnt taste to it the food was also poor quality but the service was the worst part their cashier was very rude\n",
            "Tokenized RoBERT:  ['dr', 'inks', 'Ġwere', 'Ġbad', 'Ġthe', 'Ġhot', 'Ġchocolate', 'Ġwas', 'Ġwatered', 'Ġdown', 'Ġand', 'Ġthe', 'Ġlat', 'te', 'Ġhad', 'Ġa', 'Ġburnt', 'Ġtaste', 'Ġto', 'Ġit', 'Ġthe', 'Ġfood', 'Ġwas', 'Ġalso', 'Ġpoor', 'Ġquality', 'Ġbut', 'Ġthe', 'Ġservice', 'Ġwas', 'Ġthe', 'Ġworst', 'Ġpart', 'Ġtheir', 'Ġcash', 'ier', 'Ġwas', 'Ġvery', 'Ġrude']\n",
            "Token IDs RoBERTa:  [10232, 12935, 58, 1099, 5, 2131, 7548, 21, 36408, 159, 8, 5, 16619, 859, 56, 10, 18698, 5840, 7, 24, 5, 689, 21, 67, 2129, 1318, 53, 5, 544, 21, 5, 2373, 233, 49, 1055, 906, 21, 182, 21820]\n"
          ]
        }
      ],
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the text split into tokens.\n",
        "print('Tokenized RoBERT: ', roberta_tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the text mapped to token ids.\n",
        "print('Token IDs RoBERTa: ', roberta_tokenizer.convert_tokens_to_ids(roberta_tokenizer.tokenize(sentences[0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbzP1QUuWUMi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5fc5668-795e-464d-aa6c-b549ecf168cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RoBERTa: ['Will', 'ĠYour', 'ĠH', 'ometown', 'ĠBe', 'ĠTaking', 'ĠIn', 'ĠObama', 'âĢ', 'Ļ', 's', 'ĠRefugees', '?', 'ĠHere', 'âĢ', 'Ļ', 's', 'ĠThe', 'ĠList', 'ĠOf', 'ĠCities', 'ĠWhere', 'ĠThey', 'âĢ', 'Ļ', 're', 'ĠBeing', 'ĠTrans', 'pl', 'anted', 'ĠðŁ', 'ı', 'Ļ', 'ï¸ı']\n"
          ]
        }
      ],
      "source": [
        "sequence = \"\"\"Will Your Hometown Be Taking In Obama’s Refugees? Here’s The List Of Cities Where They’re Being Transplanted 🏙️\"\"\"\n",
        "roberta_tokenized_sequence = roberta_tokenizer.tokenize(sequence)\n",
        "print(\"RoBERTa:\", roberta_tokenized_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0b0Cs2zaWjfb"
      },
      "outputs": [],
      "source": [
        "token_lens = []\n",
        "\n",
        "for txt in sentences:\n",
        "    tokens = roberta_tokenizer.encode(txt,truncation=True, max_length=512)\n",
        "    token_lens.append(len(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "aCbxpk7ojP0j",
        "outputId": "f0a536a0-8eff-443f-82ea-bb93f4160e98"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.histplot(token_lens)\n",
        "plt.xlim([0, 600]);\n",
        "plt.xlabel('Token count');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZamXIfgk-Du",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46d5f940-650c-4b1c-bd09-e25a338cdcb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  drinks were bad the hot chocolate was watered down and the latte had a burnt taste to it the food was also poor quality but the service was the worst part their cashier was very rude\n",
            "Token IDs: tensor([    0, 10232, 12935,    58,  1099,     5,  2131,  7548,    21, 36408,\n",
            "          159,     8,     5, 16619,   859,    56,    10, 18698,  5840,     7,\n",
            "           24,     5,   689,    21,    67,  2129,  1318,    53,     5,   544,\n",
            "           21,     5,  2373,   233,    49,  1055,   906,    21,   182, 21820,\n",
            "            2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1])\n",
            "labels: tensor([0, 0, 0,  ..., 1, 1, 1])\n"
          ]
        }
      ],
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "roberta_input_ids = []\n",
        "roberta_attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    roberta_encoded_dict = roberta_tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 256,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',\n",
        "                        truncation=True    # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    roberta_input_ids.append(roberta_encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    roberta_attention_masks.append(roberta_encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "roberta_input_ids = torch.cat(roberta_input_ids, dim=0)\n",
        "roberta_attention_masks = torch.cat(roberta_attention_masks, dim=0)\n",
        "labels = torch.tensor(df_review_merge.label.values)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', roberta_input_ids[0])\n",
        "print('labels:', labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWmN6cjTyePb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cd5b841-1a9b-4ed5-ca7f-ae7ee3e07226"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "486,766 training samples\n",
            "121,692 validation samples\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(roberta_input_ids, roberta_attention_masks, labels)\n",
        "\n",
        "# Create a 80-20 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtETllfvyjmy",
        "outputId": "095b0757-2888-483f-9140-61da1b03274d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "486766"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNe3YowrynHY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0928ef8-b35e-444d-da9c-a2ef3233698b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({1: 422485, 0: 64281})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "import collections as c\n",
        "train_classes = [df_review_merge.label[i] for i in train_dataset.indices]\n",
        "c.Counter(train_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0bdj-ntyril",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fe803a5-ebcf-43e9-e694-b67f62507a80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 64281 422485]\n",
            "[1.55566964e-05 2.36694794e-06]\n",
            "tensor([2.3669e-06, 2.3669e-06, 2.3669e-06,  ..., 2.3669e-06, 2.3669e-06,\n",
            "        2.3669e-06], dtype=torch.float64)\n",
            "227551\n",
            "<torch.utils.data.sampler.WeightedRandomSampler object at 0x7f309e47e2d0>\n"
          ]
        }
      ],
      "source": [
        "#Using weighted random sampler for class imbalance\n",
        "import numpy as np \n",
        "\n",
        "y_train_indices = train_dataset.indices\n",
        "\n",
        "y_train = [target[i] for i in y_train_indices]\n",
        "\n",
        "class_sample_count = np.array(\n",
        "    [len(np.where(y_train == t)[0]) for t in np.unique(y_train)])\n",
        "print(class_sample_count)\n",
        "weight = 1. / (class_sample_count)\n",
        "print(weight)\n",
        "samples_weight = np.array([weight[t] for t in y_train])\n",
        "samples_weight = torch.from_numpy(samples_weight)\n",
        "print(samples_weight)\n",
        "sampler = torch.utils.data.sampler.WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
        "print(next(iter(sampler)))\n",
        "print(sampler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJcpIWH1WJOW"
      },
      "outputs": [],
      "source": [
        "labels_unique,counts=np.unique(y_train,return_counts=True)\n",
        "counts\n",
        "class_weights=[sum(counts)/c for c in counts]\n",
        "class_weights\n",
        "example_weights=[class_weights[e] for e in y_train]\n",
        "#len(torch.example_weights)\n",
        "#sampler = torch.utils.data.sampler.WeightedRandomSampler(example_weights.type('torch.DoubleTensor'), len(example_weights))\n",
        "#sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wKaZFPZ8Yix",
        "outputId": "90d9ce30-e297-4a59-db79-620b822ecd6a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "np.unique(df_review_merge.label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9qKHsARntz7"
      },
      "outputs": [],
      "source": [
        "#Run if we are using weighted random sampler\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32 or 64.\n",
        "batch_size = 64\n",
        "gradient_accumulations=10\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = sampler, # Select batches randomly\n",
        "            batch_size = batch_size, # Trains with this batch size.\n",
        "            num_workers=2,\n",
        "            pin_memory = True\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size, # Evaluate with this batch size.\n",
        "            num_workers =2,\n",
        "            pin_memory = True\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpqN1Rsdi6cL"
      },
      "outputs": [],
      "source": [
        "#Run if we are using only random sampler for train data \n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32 or 64.\n",
        "batch_size = 64\n",
        "gradient_accumulations=10\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size, # Trains with this batch size.\n",
        "            num_workers=2,\n",
        "            pin_memory = True\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size, # Evaluate with this batch size.\n",
        "            num_workers =2,\n",
        "            pin_memory = True\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r35PDVsBdAN7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69467d00-81c3-415d-c689-962dbb009453"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels batch shape: <built-in method values of Tensor object at 0x7f2f826bc290>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([33, 31])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "#Testing the label counts in training data after applying weighted random sampler\n",
        "import collections as c\n",
        "train_labels = next(iter(train_dataloader))\n",
        "#print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels[0].values}\")\n",
        "#img = train_features[0].squeeze()\n",
        "label = train_labels[2]\n",
        "torch.bincount(label)\n",
        "#print(label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84LiNyT63LxF",
        "outputId": "8d9f6b62-499d-4b33-d092-9a78b9317bc9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7606"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "len(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDw6uSrMw6eI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b3c9207-98a4-4054-87da-c1bd180eceb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(422485.)\n",
            "tensor(64281.)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1521)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "#2nd method using pos weight for imbalance subsitute in loss function\n",
        "y_train=torch.Tensor(y_train)\n",
        "num_positives = torch.sum(y_train, dim=0)\n",
        "print(num_positives)\n",
        "num_negatives = len(train_dataset) - num_positives\n",
        "print(num_negatives)\n",
        "pos_weight  = num_negatives / num_positives\n",
        "pos_weight\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGMurnwan6GB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb2ff4e7-3a3e-4038-bb7b-365fa501d07b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Roberta model has 203 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "roberta.embeddings.word_embeddings.weight               (50265, 768)\n",
            "roberta.embeddings.position_embeddings.weight             (514, 768)\n",
            "roberta.embeddings.token_type_embeddings.weight             (1, 768)\n",
            "roberta.embeddings.LayerNorm.weight                           (768,)\n",
            "roberta.embeddings.LayerNorm.bias                             (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "roberta.encoder.layer.0.attention.self.query.weight       (768, 768)\n",
            "roberta.encoder.layer.0.attention.self.query.bias             (768,)\n",
            "roberta.encoder.layer.0.attention.self.key.weight         (768, 768)\n",
            "roberta.encoder.layer.0.attention.self.key.bias               (768,)\n",
            "roberta.encoder.layer.0.attention.self.value.weight       (768, 768)\n",
            "roberta.encoder.layer.0.attention.self.value.bias             (768,)\n",
            "roberta.encoder.layer.0.attention.output.dense.weight     (768, 768)\n",
            "roberta.encoder.layer.0.attention.output.dense.bias           (768,)\n",
            "roberta.encoder.layer.0.attention.output.LayerNorm.weight       (768,)\n",
            "roberta.encoder.layer.0.attention.output.LayerNorm.bias       (768,)\n",
            "roberta.encoder.layer.0.intermediate.dense.weight        (3072, 768)\n",
            "roberta.encoder.layer.0.intermediate.dense.bias              (3072,)\n",
            "roberta.encoder.layer.0.output.dense.weight              (768, 3072)\n",
            "roberta.encoder.layer.0.output.dense.bias                     (768,)\n",
            "roberta.encoder.layer.0.output.LayerNorm.weight               (768,)\n",
            "roberta.encoder.layer.0.output.LayerNorm.bias                 (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "classifier.dense.weight                                   (768, 768)\n",
            "classifier.dense.bias                                         (768,)\n",
            "classifier.out_proj.weight                                  (2, 768)\n",
            "classifier.out_proj.bias                                        (2,)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(roberta_model.named_parameters())\n",
        "\n",
        "print('The Roberta model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5U8SUYo5n9XY"
      },
      "outputs": [],
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in roberta_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in roberta_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5, eps = 1e-8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XR50r-utoALX"
      },
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "epochs = 3\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbeXUTPooC5s"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txhcOtv3RTyp"
      },
      "outputs": [],
      "source": [
        "from torch.cuda.amp import GradScaler, autocast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2u9XFEsoGnB"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHC5I3hdaAF4"
      },
      "outputs": [],
      "source": [
        "#Loss function with weight\n",
        "def loss_fn1(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)(outputs, targets.float())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcgOGvJIdad7"
      },
      "outputs": [],
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets.float())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCWlp9jZoL5v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2258f28c-6631-4f56-8b45-fe5e524318c6"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of  7,606.    Elapsed: 0:00:58.\n",
            "  Batch    80  of  7,606.    Elapsed: 0:01:55.\n",
            "  Batch   120  of  7,606.    Elapsed: 0:02:52.\n",
            "  Batch   160  of  7,606.    Elapsed: 0:03:49.\n",
            "  Batch   200  of  7,606.    Elapsed: 0:04:46.\n",
            "  Batch   240  of  7,606.    Elapsed: 0:05:43.\n",
            "  Batch   280  of  7,606.    Elapsed: 0:06:40.\n",
            "  Batch   320  of  7,606.    Elapsed: 0:07:37.\n",
            "  Batch   360  of  7,606.    Elapsed: 0:08:34.\n",
            "  Batch   400  of  7,606.    Elapsed: 0:09:31.\n",
            "  Batch   440  of  7,606.    Elapsed: 0:10:28.\n",
            "  Batch   480  of  7,606.    Elapsed: 0:11:25.\n",
            "  Batch   520  of  7,606.    Elapsed: 0:12:22.\n",
            "  Batch   560  of  7,606.    Elapsed: 0:13:19.\n",
            "  Batch   600  of  7,606.    Elapsed: 0:14:16.\n",
            "  Batch   640  of  7,606.    Elapsed: 0:15:13.\n",
            "  Batch   680  of  7,606.    Elapsed: 0:16:10.\n",
            "  Batch   720  of  7,606.    Elapsed: 0:17:07.\n",
            "  Batch   760  of  7,606.    Elapsed: 0:18:04.\n",
            "  Batch   800  of  7,606.    Elapsed: 0:19:01.\n",
            "  Batch   840  of  7,606.    Elapsed: 0:19:58.\n",
            "  Batch   880  of  7,606.    Elapsed: 0:20:55.\n",
            "  Batch   920  of  7,606.    Elapsed: 0:21:52.\n",
            "  Batch   960  of  7,606.    Elapsed: 0:22:49.\n",
            "  Batch 1,000  of  7,606.    Elapsed: 0:23:46.\n",
            "  Batch 1,040  of  7,606.    Elapsed: 0:24:43.\n",
            "  Batch 1,080  of  7,606.    Elapsed: 0:25:40.\n",
            "  Batch 1,120  of  7,606.    Elapsed: 0:26:37.\n",
            "  Batch 1,160  of  7,606.    Elapsed: 0:27:35.\n",
            "  Batch 1,200  of  7,606.    Elapsed: 0:28:32.\n",
            "  Batch 1,240  of  7,606.    Elapsed: 0:29:29.\n",
            "  Batch 1,280  of  7,606.    Elapsed: 0:30:26.\n",
            "  Batch 1,320  of  7,606.    Elapsed: 0:31:23.\n",
            "  Batch 1,360  of  7,606.    Elapsed: 0:32:20.\n",
            "  Batch 1,400  of  7,606.    Elapsed: 0:33:17.\n",
            "  Batch 1,440  of  7,606.    Elapsed: 0:34:14.\n",
            "  Batch 1,480  of  7,606.    Elapsed: 0:35:11.\n",
            "  Batch 1,520  of  7,606.    Elapsed: 0:36:08.\n",
            "  Batch 1,560  of  7,606.    Elapsed: 0:37:05.\n",
            "  Batch 1,600  of  7,606.    Elapsed: 0:38:02.\n",
            "  Batch 1,640  of  7,606.    Elapsed: 0:38:59.\n",
            "  Batch 1,680  of  7,606.    Elapsed: 0:39:56.\n",
            "  Batch 1,720  of  7,606.    Elapsed: 0:40:53.\n",
            "  Batch 1,760  of  7,606.    Elapsed: 0:41:50.\n",
            "  Batch 1,800  of  7,606.    Elapsed: 0:42:47.\n",
            "  Batch 1,840  of  7,606.    Elapsed: 0:43:44.\n",
            "  Batch 1,880  of  7,606.    Elapsed: 0:44:41.\n",
            "  Batch 1,920  of  7,606.    Elapsed: 0:45:38.\n",
            "  Batch 1,960  of  7,606.    Elapsed: 0:46:35.\n",
            "  Batch 2,000  of  7,606.    Elapsed: 0:47:32.\n",
            "  Batch 2,040  of  7,606.    Elapsed: 0:48:29.\n",
            "  Batch 2,080  of  7,606.    Elapsed: 0:49:26.\n",
            "  Batch 2,120  of  7,606.    Elapsed: 0:50:23.\n",
            "  Batch 2,160  of  7,606.    Elapsed: 0:51:20.\n",
            "  Batch 2,200  of  7,606.    Elapsed: 0:52:17.\n",
            "  Batch 2,240  of  7,606.    Elapsed: 0:53:14.\n",
            "  Batch 2,280  of  7,606.    Elapsed: 0:54:11.\n",
            "  Batch 2,320  of  7,606.    Elapsed: 0:55:08.\n",
            "  Batch 2,360  of  7,606.    Elapsed: 0:56:05.\n",
            "  Batch 2,400  of  7,606.    Elapsed: 0:57:02.\n",
            "  Batch 2,440  of  7,606.    Elapsed: 0:57:59.\n",
            "  Batch 2,480  of  7,606.    Elapsed: 0:58:56.\n",
            "  Batch 2,520  of  7,606.    Elapsed: 0:59:53.\n",
            "  Batch 2,560  of  7,606.    Elapsed: 1:00:50.\n",
            "  Batch 2,600  of  7,606.    Elapsed: 1:01:47.\n",
            "  Batch 2,640  of  7,606.    Elapsed: 1:02:44.\n",
            "  Batch 2,680  of  7,606.    Elapsed: 1:03:41.\n",
            "  Batch 2,720  of  7,606.    Elapsed: 1:04:38.\n",
            "  Batch 2,760  of  7,606.    Elapsed: 1:05:35.\n",
            "  Batch 2,800  of  7,606.    Elapsed: 1:06:33.\n",
            "  Batch 2,840  of  7,606.    Elapsed: 1:07:30.\n",
            "  Batch 2,880  of  7,606.    Elapsed: 1:08:27.\n",
            "  Batch 2,920  of  7,606.    Elapsed: 1:09:24.\n",
            "  Batch 2,960  of  7,606.    Elapsed: 1:10:21.\n",
            "  Batch 3,000  of  7,606.    Elapsed: 1:11:18.\n",
            "  Batch 3,040  of  7,606.    Elapsed: 1:12:15.\n",
            "  Batch 3,080  of  7,606.    Elapsed: 1:13:12.\n",
            "  Batch 3,120  of  7,606.    Elapsed: 1:14:09.\n",
            "  Batch 3,160  of  7,606.    Elapsed: 1:15:06.\n",
            "  Batch 3,200  of  7,606.    Elapsed: 1:16:03.\n",
            "  Batch 3,240  of  7,606.    Elapsed: 1:17:00.\n",
            "  Batch 3,280  of  7,606.    Elapsed: 1:17:57.\n",
            "  Batch 3,320  of  7,606.    Elapsed: 1:18:54.\n",
            "  Batch 3,360  of  7,606.    Elapsed: 1:19:51.\n",
            "  Batch 3,400  of  7,606.    Elapsed: 1:20:48.\n",
            "  Batch 3,440  of  7,606.    Elapsed: 1:21:45.\n",
            "  Batch 3,480  of  7,606.    Elapsed: 1:22:42.\n",
            "  Batch 3,520  of  7,606.    Elapsed: 1:23:39.\n",
            "  Batch 3,560  of  7,606.    Elapsed: 1:24:36.\n",
            "  Batch 3,600  of  7,606.    Elapsed: 1:25:33.\n",
            "  Batch 3,640  of  7,606.    Elapsed: 1:26:30.\n",
            "  Batch 3,680  of  7,606.    Elapsed: 1:27:27.\n",
            "  Batch 3,720  of  7,606.    Elapsed: 1:28:24.\n",
            "  Batch 3,760  of  7,606.    Elapsed: 1:29:21.\n",
            "  Batch 3,800  of  7,606.    Elapsed: 1:30:18.\n",
            "  Batch 3,840  of  7,606.    Elapsed: 1:31:15.\n",
            "  Batch 3,880  of  7,606.    Elapsed: 1:32:12.\n",
            "  Batch 3,920  of  7,606.    Elapsed: 1:33:09.\n",
            "  Batch 3,960  of  7,606.    Elapsed: 1:34:06.\n",
            "  Batch 4,000  of  7,606.    Elapsed: 1:35:03.\n",
            "  Batch 4,040  of  7,606.    Elapsed: 1:36:00.\n",
            "  Batch 4,080  of  7,606.    Elapsed: 1:36:57.\n",
            "  Batch 4,120  of  7,606.    Elapsed: 1:37:54.\n",
            "  Batch 4,160  of  7,606.    Elapsed: 1:38:51.\n",
            "  Batch 4,200  of  7,606.    Elapsed: 1:39:48.\n",
            "  Batch 4,240  of  7,606.    Elapsed: 1:40:45.\n",
            "  Batch 4,280  of  7,606.    Elapsed: 1:41:42.\n",
            "  Batch 4,320  of  7,606.    Elapsed: 1:42:40.\n",
            "  Batch 4,360  of  7,606.    Elapsed: 1:43:37.\n",
            "  Batch 4,400  of  7,606.    Elapsed: 1:44:34.\n",
            "  Batch 4,440  of  7,606.    Elapsed: 1:45:31.\n",
            "  Batch 4,480  of  7,606.    Elapsed: 1:46:28.\n",
            "  Batch 4,520  of  7,606.    Elapsed: 1:47:25.\n",
            "  Batch 4,560  of  7,606.    Elapsed: 1:48:22.\n",
            "  Batch 4,600  of  7,606.    Elapsed: 1:49:19.\n",
            "  Batch 4,640  of  7,606.    Elapsed: 1:50:16.\n",
            "  Batch 4,680  of  7,606.    Elapsed: 1:51:13.\n",
            "  Batch 4,720  of  7,606.    Elapsed: 1:52:10.\n",
            "  Batch 4,760  of  7,606.    Elapsed: 1:53:07.\n",
            "  Batch 4,800  of  7,606.    Elapsed: 1:54:04.\n",
            "  Batch 4,840  of  7,606.    Elapsed: 1:55:01.\n",
            "  Batch 4,880  of  7,606.    Elapsed: 1:55:58.\n",
            "  Batch 4,920  of  7,606.    Elapsed: 1:56:55.\n",
            "  Batch 4,960  of  7,606.    Elapsed: 1:57:52.\n",
            "  Batch 5,000  of  7,606.    Elapsed: 1:58:49.\n",
            "  Batch 5,040  of  7,606.    Elapsed: 1:59:46.\n",
            "  Batch 5,080  of  7,606.    Elapsed: 2:00:43.\n",
            "  Batch 5,120  of  7,606.    Elapsed: 2:01:40.\n",
            "  Batch 5,160  of  7,606.    Elapsed: 2:02:37.\n",
            "  Batch 5,200  of  7,606.    Elapsed: 2:03:34.\n",
            "  Batch 5,240  of  7,606.    Elapsed: 2:04:31.\n",
            "  Batch 5,280  of  7,606.    Elapsed: 2:05:28.\n",
            "  Batch 5,320  of  7,606.    Elapsed: 2:06:25.\n",
            "  Batch 5,360  of  7,606.    Elapsed: 2:07:22.\n",
            "  Batch 5,400  of  7,606.    Elapsed: 2:08:19.\n",
            "  Batch 5,440  of  7,606.    Elapsed: 2:09:16.\n",
            "  Batch 5,480  of  7,606.    Elapsed: 2:10:13.\n",
            "  Batch 5,520  of  7,606.    Elapsed: 2:11:10.\n",
            "  Batch 5,560  of  7,606.    Elapsed: 2:12:08.\n",
            "  Batch 5,600  of  7,606.    Elapsed: 2:13:05.\n",
            "  Batch 5,640  of  7,606.    Elapsed: 2:14:02.\n",
            "  Batch 5,680  of  7,606.    Elapsed: 2:14:59.\n",
            "  Batch 5,720  of  7,606.    Elapsed: 2:15:56.\n",
            "  Batch 5,760  of  7,606.    Elapsed: 2:16:53.\n",
            "  Batch 5,800  of  7,606.    Elapsed: 2:17:50.\n",
            "  Batch 5,840  of  7,606.    Elapsed: 2:18:47.\n",
            "  Batch 5,880  of  7,606.    Elapsed: 2:19:44.\n",
            "  Batch 5,920  of  7,606.    Elapsed: 2:20:41.\n",
            "  Batch 5,960  of  7,606.    Elapsed: 2:21:38.\n",
            "  Batch 6,000  of  7,606.    Elapsed: 2:22:35.\n",
            "  Batch 6,040  of  7,606.    Elapsed: 2:23:32.\n",
            "  Batch 6,080  of  7,606.    Elapsed: 2:24:29.\n",
            "  Batch 6,120  of  7,606.    Elapsed: 2:25:26.\n",
            "  Batch 6,160  of  7,606.    Elapsed: 2:26:23.\n",
            "  Batch 6,200  of  7,606.    Elapsed: 2:27:20.\n",
            "  Batch 6,240  of  7,606.    Elapsed: 2:28:17.\n",
            "  Batch 6,280  of  7,606.    Elapsed: 2:29:14.\n",
            "  Batch 6,320  of  7,606.    Elapsed: 2:30:11.\n",
            "  Batch 6,360  of  7,606.    Elapsed: 2:31:08.\n",
            "  Batch 6,400  of  7,606.    Elapsed: 2:32:05.\n",
            "  Batch 6,440  of  7,606.    Elapsed: 2:33:02.\n",
            "  Batch 6,480  of  7,606.    Elapsed: 2:33:59.\n",
            "  Batch 6,520  of  7,606.    Elapsed: 2:34:56.\n",
            "  Batch 6,560  of  7,606.    Elapsed: 2:35:53.\n",
            "  Batch 6,600  of  7,606.    Elapsed: 2:36:50.\n",
            "  Batch 6,640  of  7,606.    Elapsed: 2:37:47.\n",
            "  Batch 6,680  of  7,606.    Elapsed: 2:38:44.\n",
            "  Batch 6,720  of  7,606.    Elapsed: 2:39:41.\n",
            "  Batch 6,760  of  7,606.    Elapsed: 2:40:39.\n",
            "  Batch 6,800  of  7,606.    Elapsed: 2:41:36.\n",
            "  Batch 6,840  of  7,606.    Elapsed: 2:42:33.\n",
            "  Batch 6,880  of  7,606.    Elapsed: 2:43:30.\n",
            "  Batch 6,920  of  7,606.    Elapsed: 2:44:27.\n",
            "  Batch 6,960  of  7,606.    Elapsed: 2:45:24.\n",
            "  Batch 7,000  of  7,606.    Elapsed: 2:46:21.\n",
            "  Batch 7,040  of  7,606.    Elapsed: 2:47:18.\n",
            "  Batch 7,080  of  7,606.    Elapsed: 2:48:15.\n",
            "  Batch 7,120  of  7,606.    Elapsed: 2:49:12.\n",
            "  Batch 7,160  of  7,606.    Elapsed: 2:50:09.\n",
            "  Batch 7,200  of  7,606.    Elapsed: 2:51:06.\n",
            "  Batch 7,240  of  7,606.    Elapsed: 2:52:03.\n",
            "  Batch 7,280  of  7,606.    Elapsed: 2:53:00.\n",
            "  Batch 7,320  of  7,606.    Elapsed: 2:53:57.\n",
            "  Batch 7,360  of  7,606.    Elapsed: 2:54:54.\n",
            "  Batch 7,400  of  7,606.    Elapsed: 2:55:51.\n",
            "  Batch 7,440  of  7,606.    Elapsed: 2:56:48.\n",
            "  Batch 7,480  of  7,606.    Elapsed: 2:57:45.\n",
            "  Batch 7,520  of  7,606.    Elapsed: 2:58:42.\n",
            "  Batch 7,560  of  7,606.    Elapsed: 2:59:39.\n",
            "  Batch 7,600  of  7,606.    Elapsed: 3:00:36.\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 3:00:44\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.57\n",
            "  F1 score: 0.51\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.82      0.33     16158\n",
            "           1       0.95      0.52      0.68    105534\n",
            "\n",
            "    accuracy                           0.56    121692\n",
            "   macro avg       0.58      0.67      0.50    121692\n",
            "weighted avg       0.85      0.56      0.63    121692\n",
            "\n",
            "[[13329  2829]\n",
            " [50251 55283]]\n",
            "Mathews Correlation coefficient score for this epoch is : 0.24\n",
            "Model validation score: f1=0.676 auc=0.948\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Validation Loss: 0.70\n",
            "  Validation took: 0:15:47\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of  7,606.    Elapsed: 0:00:57.\n",
            "  Batch    80  of  7,606.    Elapsed: 0:01:54.\n",
            "  Batch   120  of  7,606.    Elapsed: 0:02:51.\n",
            "  Batch   160  of  7,606.    Elapsed: 0:03:48.\n",
            "  Batch   200  of  7,606.    Elapsed: 0:04:45.\n",
            "  Batch   240  of  7,606.    Elapsed: 0:05:43.\n",
            "  Batch   280  of  7,606.    Elapsed: 0:06:40.\n",
            "  Batch   320  of  7,606.    Elapsed: 0:07:37.\n",
            "  Batch   360  of  7,606.    Elapsed: 0:08:34.\n",
            "  Batch   400  of  7,606.    Elapsed: 0:09:31.\n",
            "  Batch   440  of  7,606.    Elapsed: 0:10:28.\n",
            "  Batch   480  of  7,606.    Elapsed: 0:11:25.\n",
            "  Batch   520  of  7,606.    Elapsed: 0:12:22.\n",
            "  Batch   560  of  7,606.    Elapsed: 0:13:19.\n",
            "  Batch   600  of  7,606.    Elapsed: 0:14:16.\n",
            "  Batch   640  of  7,606.    Elapsed: 0:15:13.\n",
            "  Batch   680  of  7,606.    Elapsed: 0:16:10.\n",
            "  Batch   720  of  7,606.    Elapsed: 0:17:07.\n",
            "  Batch   760  of  7,606.    Elapsed: 0:18:04.\n",
            "  Batch   800  of  7,606.    Elapsed: 0:19:01.\n",
            "  Batch   840  of  7,606.    Elapsed: 0:19:58.\n",
            "  Batch   880  of  7,606.    Elapsed: 0:20:55.\n",
            "  Batch   920  of  7,606.    Elapsed: 0:21:52.\n",
            "  Batch   960  of  7,606.    Elapsed: 0:22:49.\n",
            "  Batch 1,000  of  7,606.    Elapsed: 0:23:46.\n",
            "  Batch 1,040  of  7,606.    Elapsed: 0:24:43.\n",
            "  Batch 1,080  of  7,606.    Elapsed: 0:25:40.\n",
            "  Batch 1,120  of  7,606.    Elapsed: 0:26:37.\n",
            "  Batch 1,160  of  7,606.    Elapsed: 0:27:34.\n",
            "  Batch 1,200  of  7,606.    Elapsed: 0:28:31.\n",
            "  Batch 1,240  of  7,606.    Elapsed: 0:29:28.\n",
            "  Batch 1,280  of  7,606.    Elapsed: 0:30:25.\n",
            "  Batch 1,320  of  7,606.    Elapsed: 0:31:22.\n",
            "  Batch 1,360  of  7,606.    Elapsed: 0:32:19.\n",
            "  Batch 1,400  of  7,606.    Elapsed: 0:33:16.\n",
            "  Batch 1,440  of  7,606.    Elapsed: 0:34:13.\n",
            "  Batch 1,480  of  7,606.    Elapsed: 0:35:10.\n",
            "  Batch 1,520  of  7,606.    Elapsed: 0:36:07.\n",
            "  Batch 1,560  of  7,606.    Elapsed: 0:37:04.\n",
            "  Batch 1,600  of  7,606.    Elapsed: 0:38:02.\n",
            "  Batch 1,640  of  7,606.    Elapsed: 0:38:59.\n",
            "  Batch 1,680  of  7,606.    Elapsed: 0:39:56.\n",
            "  Batch 1,720  of  7,606.    Elapsed: 0:40:53.\n",
            "  Batch 1,760  of  7,606.    Elapsed: 0:41:50.\n",
            "  Batch 1,800  of  7,606.    Elapsed: 0:42:47.\n",
            "  Batch 1,840  of  7,606.    Elapsed: 0:43:44.\n",
            "  Batch 1,880  of  7,606.    Elapsed: 0:44:41.\n",
            "  Batch 1,920  of  7,606.    Elapsed: 0:45:38.\n",
            "  Batch 1,960  of  7,606.    Elapsed: 0:46:35.\n",
            "  Batch 2,000  of  7,606.    Elapsed: 0:47:32.\n",
            "  Batch 2,040  of  7,606.    Elapsed: 0:48:29.\n",
            "  Batch 2,080  of  7,606.    Elapsed: 0:49:26.\n",
            "  Batch 2,120  of  7,606.    Elapsed: 0:50:23.\n",
            "  Batch 2,160  of  7,606.    Elapsed: 0:51:20.\n",
            "  Batch 2,200  of  7,606.    Elapsed: 0:52:17.\n",
            "  Batch 2,240  of  7,606.    Elapsed: 0:53:14.\n",
            "  Batch 2,280  of  7,606.    Elapsed: 0:54:11.\n",
            "  Batch 2,320  of  7,606.    Elapsed: 0:55:08.\n",
            "  Batch 2,360  of  7,606.    Elapsed: 0:56:05.\n",
            "  Batch 2,400  of  7,606.    Elapsed: 0:57:02.\n",
            "  Batch 2,440  of  7,606.    Elapsed: 0:57:59.\n",
            "  Batch 2,480  of  7,606.    Elapsed: 0:58:56.\n",
            "  Batch 2,520  of  7,606.    Elapsed: 0:59:53.\n",
            "  Batch 2,560  of  7,606.    Elapsed: 1:00:50.\n",
            "  Batch 2,600  of  7,606.    Elapsed: 1:01:47.\n",
            "  Batch 2,640  of  7,606.    Elapsed: 1:02:44.\n",
            "  Batch 2,680  of  7,606.    Elapsed: 1:03:41.\n",
            "  Batch 2,720  of  7,606.    Elapsed: 1:04:38.\n",
            "  Batch 2,760  of  7,606.    Elapsed: 1:05:35.\n",
            "  Batch 2,800  of  7,606.    Elapsed: 1:06:32.\n",
            "  Batch 2,840  of  7,606.    Elapsed: 1:07:30.\n",
            "  Batch 2,880  of  7,606.    Elapsed: 1:08:27.\n",
            "  Batch 2,920  of  7,606.    Elapsed: 1:09:24.\n",
            "  Batch 2,960  of  7,606.    Elapsed: 1:10:21.\n",
            "  Batch 3,000  of  7,606.    Elapsed: 1:11:18.\n",
            "  Batch 3,040  of  7,606.    Elapsed: 1:12:15.\n",
            "  Batch 3,080  of  7,606.    Elapsed: 1:13:12.\n",
            "  Batch 3,120  of  7,606.    Elapsed: 1:14:09.\n",
            "  Batch 3,160  of  7,606.    Elapsed: 1:15:06.\n",
            "  Batch 3,200  of  7,606.    Elapsed: 1:16:03.\n",
            "  Batch 3,240  of  7,606.    Elapsed: 1:17:00.\n",
            "  Batch 3,280  of  7,606.    Elapsed: 1:17:57.\n",
            "  Batch 3,320  of  7,606.    Elapsed: 1:18:54.\n",
            "  Batch 3,360  of  7,606.    Elapsed: 1:19:51.\n",
            "  Batch 3,400  of  7,606.    Elapsed: 1:20:48.\n",
            "  Batch 3,440  of  7,606.    Elapsed: 1:21:45.\n",
            "  Batch 3,480  of  7,606.    Elapsed: 1:22:42.\n",
            "  Batch 3,520  of  7,606.    Elapsed: 1:23:39.\n",
            "  Batch 3,560  of  7,606.    Elapsed: 1:24:36.\n",
            "  Batch 3,600  of  7,606.    Elapsed: 1:25:33.\n",
            "  Batch 3,640  of  7,606.    Elapsed: 1:26:30.\n",
            "  Batch 3,680  of  7,606.    Elapsed: 1:27:27.\n",
            "  Batch 3,720  of  7,606.    Elapsed: 1:28:24.\n",
            "  Batch 3,760  of  7,606.    Elapsed: 1:29:21.\n",
            "  Batch 3,800  of  7,606.    Elapsed: 1:30:18.\n",
            "  Batch 3,840  of  7,606.    Elapsed: 1:31:15.\n",
            "  Batch 3,880  of  7,606.    Elapsed: 1:32:12.\n",
            "  Batch 3,920  of  7,606.    Elapsed: 1:33:09.\n",
            "  Batch 3,960  of  7,606.    Elapsed: 1:34:06.\n",
            "  Batch 4,000  of  7,606.    Elapsed: 1:35:03.\n",
            "  Batch 4,040  of  7,606.    Elapsed: 1:36:00.\n",
            "  Batch 4,080  of  7,606.    Elapsed: 1:36:57.\n",
            "  Batch 4,120  of  7,606.    Elapsed: 1:37:54.\n",
            "  Batch 4,160  of  7,606.    Elapsed: 1:38:52.\n",
            "  Batch 4,200  of  7,606.    Elapsed: 1:39:49.\n",
            "  Batch 4,240  of  7,606.    Elapsed: 1:40:46.\n",
            "  Batch 4,280  of  7,606.    Elapsed: 1:41:43.\n",
            "  Batch 4,320  of  7,606.    Elapsed: 1:42:40.\n",
            "  Batch 4,360  of  7,606.    Elapsed: 1:43:37.\n",
            "  Batch 4,400  of  7,606.    Elapsed: 1:44:34.\n",
            "  Batch 4,440  of  7,606.    Elapsed: 1:45:31.\n",
            "  Batch 4,480  of  7,606.    Elapsed: 1:46:28.\n",
            "  Batch 4,520  of  7,606.    Elapsed: 1:47:25.\n",
            "  Batch 4,560  of  7,606.    Elapsed: 1:48:22.\n",
            "  Batch 4,600  of  7,606.    Elapsed: 1:49:19.\n",
            "  Batch 4,640  of  7,606.    Elapsed: 1:50:16.\n",
            "  Batch 4,680  of  7,606.    Elapsed: 1:51:13.\n",
            "  Batch 4,720  of  7,606.    Elapsed: 1:52:10.\n",
            "  Batch 4,760  of  7,606.    Elapsed: 1:53:07.\n",
            "  Batch 4,800  of  7,606.    Elapsed: 1:54:04.\n",
            "  Batch 4,840  of  7,606.    Elapsed: 1:55:01.\n",
            "  Batch 4,880  of  7,606.    Elapsed: 1:55:58.\n",
            "  Batch 4,920  of  7,606.    Elapsed: 1:56:55.\n",
            "  Batch 4,960  of  7,606.    Elapsed: 1:57:52.\n",
            "  Batch 5,000  of  7,606.    Elapsed: 1:58:49.\n",
            "  Batch 5,040  of  7,606.    Elapsed: 1:59:46.\n",
            "  Batch 5,080  of  7,606.    Elapsed: 2:00:43.\n",
            "  Batch 5,120  of  7,606.    Elapsed: 2:01:40.\n",
            "  Batch 5,160  of  7,606.    Elapsed: 2:02:37.\n",
            "  Batch 5,200  of  7,606.    Elapsed: 2:03:34.\n",
            "  Batch 5,240  of  7,606.    Elapsed: 2:04:31.\n",
            "  Batch 5,280  of  7,606.    Elapsed: 2:05:28.\n",
            "  Batch 5,320  of  7,606.    Elapsed: 2:06:25.\n",
            "  Batch 5,360  of  7,606.    Elapsed: 2:07:22.\n",
            "  Batch 5,400  of  7,606.    Elapsed: 2:08:19.\n",
            "  Batch 5,440  of  7,606.    Elapsed: 2:09:16.\n",
            "  Batch 5,480  of  7,606.    Elapsed: 2:10:13.\n",
            "  Batch 5,520  of  7,606.    Elapsed: 2:11:10.\n",
            "  Batch 5,560  of  7,606.    Elapsed: 2:12:07.\n",
            "  Batch 5,600  of  7,606.    Elapsed: 2:13:04.\n",
            "  Batch 5,640  of  7,606.    Elapsed: 2:14:01.\n",
            "  Batch 5,680  of  7,606.    Elapsed: 2:14:59.\n",
            "  Batch 5,720  of  7,606.    Elapsed: 2:15:56.\n",
            "  Batch 5,760  of  7,606.    Elapsed: 2:16:53.\n",
            "  Batch 5,800  of  7,606.    Elapsed: 2:17:50.\n",
            "  Batch 5,840  of  7,606.    Elapsed: 2:18:47.\n",
            "  Batch 5,880  of  7,606.    Elapsed: 2:19:44.\n",
            "  Batch 5,920  of  7,606.    Elapsed: 2:20:41.\n",
            "  Batch 5,960  of  7,606.    Elapsed: 2:21:38.\n",
            "  Batch 6,000  of  7,606.    Elapsed: 2:22:35.\n",
            "  Batch 6,040  of  7,606.    Elapsed: 2:23:32.\n",
            "  Batch 6,080  of  7,606.    Elapsed: 2:24:29.\n",
            "  Batch 6,120  of  7,606.    Elapsed: 2:25:26.\n",
            "  Batch 6,160  of  7,606.    Elapsed: 2:26:23.\n",
            "  Batch 6,200  of  7,606.    Elapsed: 2:27:20.\n",
            "  Batch 6,240  of  7,606.    Elapsed: 2:28:17.\n",
            "  Batch 6,280  of  7,606.    Elapsed: 2:29:14.\n",
            "  Batch 6,320  of  7,606.    Elapsed: 2:30:11.\n",
            "  Batch 6,360  of  7,606.    Elapsed: 2:31:08.\n",
            "  Batch 6,400  of  7,606.    Elapsed: 2:32:05.\n",
            "  Batch 6,440  of  7,606.    Elapsed: 2:33:02.\n",
            "  Batch 6,480  of  7,606.    Elapsed: 2:33:59.\n",
            "  Batch 6,520  of  7,606.    Elapsed: 2:34:56.\n",
            "  Batch 6,560  of  7,606.    Elapsed: 2:35:53.\n",
            "  Batch 6,600  of  7,606.    Elapsed: 2:36:50.\n",
            "  Batch 6,640  of  7,606.    Elapsed: 2:37:47.\n",
            "  Batch 6,680  of  7,606.    Elapsed: 2:38:44.\n",
            "  Batch 6,720  of  7,606.    Elapsed: 2:39:41.\n",
            "  Batch 6,760  of  7,606.    Elapsed: 2:40:38.\n",
            "  Batch 6,800  of  7,606.    Elapsed: 2:41:35.\n",
            "  Batch 6,840  of  7,606.    Elapsed: 2:42:32.\n",
            "  Batch 6,880  of  7,606.    Elapsed: 2:43:29.\n",
            "  Batch 6,920  of  7,606.    Elapsed: 2:44:26.\n",
            "  Batch 6,960  of  7,606.    Elapsed: 2:45:23.\n",
            "  Batch 7,000  of  7,606.    Elapsed: 2:46:20.\n",
            "  Batch 7,040  of  7,606.    Elapsed: 2:47:17.\n",
            "  Batch 7,080  of  7,606.    Elapsed: 2:48:14.\n",
            "  Batch 7,120  of  7,606.    Elapsed: 2:49:11.\n",
            "  Batch 7,160  of  7,606.    Elapsed: 2:50:08.\n",
            "  Batch 7,200  of  7,606.    Elapsed: 2:51:05.\n",
            "  Batch 7,240  of  7,606.    Elapsed: 2:52:02.\n",
            "  Batch 7,280  of  7,606.    Elapsed: 2:52:59.\n",
            "  Batch 7,320  of  7,606.    Elapsed: 2:53:56.\n",
            "  Batch 7,360  of  7,606.    Elapsed: 2:54:53.\n",
            "  Batch 7,400  of  7,606.    Elapsed: 2:55:50.\n",
            "  Batch 7,440  of  7,606.    Elapsed: 2:56:48.\n",
            "  Batch 7,480  of  7,606.    Elapsed: 2:57:45.\n",
            "  Batch 7,520  of  7,606.    Elapsed: 2:58:42.\n",
            "  Batch 7,560  of  7,606.    Elapsed: 2:59:39.\n",
            "  Batch 7,600  of  7,606.    Elapsed: 3:00:36.\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 3:00:44\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.59\n",
            "  F1 score: 0.52\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.22      0.80      0.34     16158\n",
            "           1       0.95      0.55      0.70    105534\n",
            "\n",
            "    accuracy                           0.59    121692\n",
            "   macro avg       0.58      0.68      0.52    121692\n",
            "weighted avg       0.85      0.59      0.65    121692\n",
            "\n",
            "[[12993  3165]\n",
            " [47174 58360]]\n",
            "Mathews Correlation coefficient score for this epoch is : 0.24\n",
            "Model validation score: f1=0.699 auc=0.949\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Validation Loss: 0.66\n",
            "  Validation took: 0:15:46\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of  7,606.    Elapsed: 0:00:57.\n",
            "  Batch    80  of  7,606.    Elapsed: 0:01:54.\n",
            "  Batch   120  of  7,606.    Elapsed: 0:02:51.\n",
            "  Batch   160  of  7,606.    Elapsed: 0:03:48.\n",
            "  Batch   200  of  7,606.    Elapsed: 0:04:45.\n",
            "  Batch   240  of  7,606.    Elapsed: 0:05:42.\n",
            "  Batch   280  of  7,606.    Elapsed: 0:06:40.\n",
            "  Batch   320  of  7,606.    Elapsed: 0:07:37.\n",
            "  Batch   360  of  7,606.    Elapsed: 0:08:34.\n",
            "  Batch   400  of  7,606.    Elapsed: 0:09:31.\n",
            "  Batch   440  of  7,606.    Elapsed: 0:10:28.\n",
            "  Batch   480  of  7,606.    Elapsed: 0:11:25.\n",
            "  Batch   520  of  7,606.    Elapsed: 0:12:22.\n",
            "  Batch   560  of  7,606.    Elapsed: 0:13:19.\n",
            "  Batch   600  of  7,606.    Elapsed: 0:14:16.\n",
            "  Batch   640  of  7,606.    Elapsed: 0:15:13.\n",
            "  Batch   680  of  7,606.    Elapsed: 0:16:10.\n",
            "  Batch   720  of  7,606.    Elapsed: 0:17:07.\n",
            "  Batch   760  of  7,606.    Elapsed: 0:18:04.\n",
            "  Batch   800  of  7,606.    Elapsed: 0:19:01.\n",
            "  Batch   840  of  7,606.    Elapsed: 0:19:58.\n",
            "  Batch   880  of  7,606.    Elapsed: 0:20:55.\n",
            "  Batch   920  of  7,606.    Elapsed: 0:21:52.\n",
            "  Batch   960  of  7,606.    Elapsed: 0:22:49.\n",
            "  Batch 1,000  of  7,606.    Elapsed: 0:23:46.\n",
            "  Batch 1,040  of  7,606.    Elapsed: 0:24:43.\n",
            "  Batch 1,080  of  7,606.    Elapsed: 0:25:40.\n",
            "  Batch 1,120  of  7,606.    Elapsed: 0:26:37.\n",
            "  Batch 1,160  of  7,606.    Elapsed: 0:27:34.\n",
            "  Batch 1,200  of  7,606.    Elapsed: 0:28:31.\n",
            "  Batch 1,240  of  7,606.    Elapsed: 0:29:28.\n",
            "  Batch 1,280  of  7,606.    Elapsed: 0:30:25.\n",
            "  Batch 1,320  of  7,606.    Elapsed: 0:31:22.\n",
            "  Batch 1,360  of  7,606.    Elapsed: 0:32:19.\n",
            "  Batch 1,400  of  7,606.    Elapsed: 0:33:16.\n",
            "  Batch 1,440  of  7,606.    Elapsed: 0:34:13.\n",
            "  Batch 1,480  of  7,606.    Elapsed: 0:35:10.\n",
            "  Batch 1,520  of  7,606.    Elapsed: 0:36:07.\n",
            "  Batch 1,560  of  7,606.    Elapsed: 0:37:04.\n",
            "  Batch 1,600  of  7,606.    Elapsed: 0:38:01.\n",
            "  Batch 1,640  of  7,606.    Elapsed: 0:38:58.\n",
            "  Batch 1,680  of  7,606.    Elapsed: 0:39:55.\n",
            "  Batch 1,720  of  7,606.    Elapsed: 0:40:53.\n",
            "  Batch 1,760  of  7,606.    Elapsed: 0:41:50.\n",
            "  Batch 1,800  of  7,606.    Elapsed: 0:42:47.\n",
            "  Batch 1,840  of  7,606.    Elapsed: 0:43:44.\n",
            "  Batch 1,880  of  7,606.    Elapsed: 0:44:41.\n",
            "  Batch 1,920  of  7,606.    Elapsed: 0:45:38.\n",
            "  Batch 1,960  of  7,606.    Elapsed: 0:46:35.\n",
            "  Batch 2,000  of  7,606.    Elapsed: 0:47:32.\n",
            "  Batch 2,040  of  7,606.    Elapsed: 0:48:29.\n",
            "  Batch 2,080  of  7,606.    Elapsed: 0:49:26.\n",
            "  Batch 2,120  of  7,606.    Elapsed: 0:50:23.\n",
            "  Batch 2,160  of  7,606.    Elapsed: 0:51:20.\n",
            "  Batch 2,200  of  7,606.    Elapsed: 0:52:17.\n",
            "  Batch 2,240  of  7,606.    Elapsed: 0:53:14.\n",
            "  Batch 2,280  of  7,606.    Elapsed: 0:54:11.\n",
            "  Batch 2,320  of  7,606.    Elapsed: 0:55:08.\n",
            "  Batch 2,360  of  7,606.    Elapsed: 0:56:05.\n",
            "  Batch 2,400  of  7,606.    Elapsed: 0:57:02.\n",
            "  Batch 2,440  of  7,606.    Elapsed: 0:57:59.\n",
            "  Batch 2,480  of  7,606.    Elapsed: 0:58:56.\n",
            "  Batch 2,520  of  7,606.    Elapsed: 0:59:53.\n",
            "  Batch 2,560  of  7,606.    Elapsed: 1:00:50.\n",
            "  Batch 2,600  of  7,606.    Elapsed: 1:01:47.\n",
            "  Batch 2,640  of  7,606.    Elapsed: 1:02:44.\n",
            "  Batch 2,680  of  7,606.    Elapsed: 1:03:41.\n",
            "  Batch 2,720  of  7,606.    Elapsed: 1:04:38.\n",
            "  Batch 2,760  of  7,606.    Elapsed: 1:05:35.\n",
            "  Batch 2,800  of  7,606.    Elapsed: 1:06:32.\n",
            "  Batch 2,840  of  7,606.    Elapsed: 1:07:29.\n",
            "  Batch 2,880  of  7,606.    Elapsed: 1:08:26.\n",
            "  Batch 2,920  of  7,606.    Elapsed: 1:09:23.\n",
            "  Batch 2,960  of  7,606.    Elapsed: 1:10:20.\n",
            "  Batch 3,000  of  7,606.    Elapsed: 1:11:17.\n",
            "  Batch 3,040  of  7,606.    Elapsed: 1:12:14.\n",
            "  Batch 3,080  of  7,606.    Elapsed: 1:13:11.\n",
            "  Batch 3,120  of  7,606.    Elapsed: 1:14:08.\n",
            "  Batch 3,160  of  7,606.    Elapsed: 1:15:05.\n",
            "  Batch 3,200  of  7,606.    Elapsed: 1:16:02.\n",
            "  Batch 3,240  of  7,606.    Elapsed: 1:16:59.\n",
            "  Batch 3,280  of  7,606.    Elapsed: 1:17:56.\n",
            "  Batch 3,320  of  7,606.    Elapsed: 1:18:53.\n",
            "  Batch 3,360  of  7,606.    Elapsed: 1:19:51.\n",
            "  Batch 3,400  of  7,606.    Elapsed: 1:20:48.\n",
            "  Batch 3,440  of  7,606.    Elapsed: 1:21:45.\n",
            "  Batch 3,480  of  7,606.    Elapsed: 1:22:42.\n",
            "  Batch 3,520  of  7,606.    Elapsed: 1:23:39.\n",
            "  Batch 3,560  of  7,606.    Elapsed: 1:24:36.\n",
            "  Batch 3,600  of  7,606.    Elapsed: 1:25:33.\n",
            "  Batch 3,640  of  7,606.    Elapsed: 1:26:30.\n",
            "  Batch 3,680  of  7,606.    Elapsed: 1:27:27.\n",
            "  Batch 3,720  of  7,606.    Elapsed: 1:28:24.\n",
            "  Batch 3,760  of  7,606.    Elapsed: 1:29:21.\n",
            "  Batch 3,800  of  7,606.    Elapsed: 1:30:18.\n",
            "  Batch 3,840  of  7,606.    Elapsed: 1:31:15.\n",
            "  Batch 3,880  of  7,606.    Elapsed: 1:32:12.\n",
            "  Batch 3,920  of  7,606.    Elapsed: 1:33:09.\n",
            "  Batch 3,960  of  7,606.    Elapsed: 1:34:06.\n",
            "  Batch 4,000  of  7,606.    Elapsed: 1:35:03.\n",
            "  Batch 4,040  of  7,606.    Elapsed: 1:36:00.\n",
            "  Batch 4,080  of  7,606.    Elapsed: 1:36:57.\n",
            "  Batch 4,120  of  7,606.    Elapsed: 1:37:54.\n",
            "  Batch 4,160  of  7,606.    Elapsed: 1:38:51.\n",
            "  Batch 4,200  of  7,606.    Elapsed: 1:39:48.\n",
            "  Batch 4,240  of  7,606.    Elapsed: 1:40:45.\n",
            "  Batch 4,280  of  7,606.    Elapsed: 1:41:42.\n",
            "  Batch 4,320  of  7,606.    Elapsed: 1:42:39.\n",
            "  Batch 4,360  of  7,606.    Elapsed: 1:43:36.\n",
            "  Batch 4,400  of  7,606.    Elapsed: 1:44:33.\n",
            "  Batch 4,440  of  7,606.    Elapsed: 1:45:30.\n",
            "  Batch 4,480  of  7,606.    Elapsed: 1:46:27.\n",
            "  Batch 4,520  of  7,606.    Elapsed: 1:47:24.\n",
            "  Batch 4,560  of  7,606.    Elapsed: 1:48:21.\n",
            "  Batch 4,600  of  7,606.    Elapsed: 1:49:18.\n",
            "  Batch 4,640  of  7,606.    Elapsed: 1:50:15.\n",
            "  Batch 4,680  of  7,606.    Elapsed: 1:51:12.\n",
            "  Batch 4,720  of  7,606.    Elapsed: 1:52:09.\n",
            "  Batch 4,760  of  7,606.    Elapsed: 1:53:07.\n",
            "  Batch 4,800  of  7,606.    Elapsed: 1:54:04.\n",
            "  Batch 4,840  of  7,606.    Elapsed: 1:55:01.\n",
            "  Batch 4,880  of  7,606.    Elapsed: 1:55:58.\n",
            "  Batch 4,920  of  7,606.    Elapsed: 1:56:55.\n",
            "  Batch 4,960  of  7,606.    Elapsed: 1:57:52.\n",
            "  Batch 5,000  of  7,606.    Elapsed: 1:58:49.\n",
            "  Batch 5,040  of  7,606.    Elapsed: 1:59:46.\n",
            "  Batch 5,080  of  7,606.    Elapsed: 2:00:43.\n",
            "  Batch 5,120  of  7,606.    Elapsed: 2:01:40.\n",
            "  Batch 5,160  of  7,606.    Elapsed: 2:02:37.\n",
            "  Batch 5,200  of  7,606.    Elapsed: 2:03:34.\n",
            "  Batch 5,240  of  7,606.    Elapsed: 2:04:31.\n",
            "  Batch 5,280  of  7,606.    Elapsed: 2:05:28.\n",
            "  Batch 5,320  of  7,606.    Elapsed: 2:06:25.\n",
            "  Batch 5,360  of  7,606.    Elapsed: 2:07:22.\n",
            "  Batch 5,400  of  7,606.    Elapsed: 2:08:19.\n",
            "  Batch 5,440  of  7,606.    Elapsed: 2:09:16.\n",
            "  Batch 5,480  of  7,606.    Elapsed: 2:10:13.\n",
            "  Batch 5,520  of  7,606.    Elapsed: 2:11:10.\n",
            "  Batch 5,560  of  7,606.    Elapsed: 2:12:07.\n",
            "  Batch 5,600  of  7,606.    Elapsed: 2:13:04.\n",
            "  Batch 5,640  of  7,606.    Elapsed: 2:14:01.\n",
            "  Batch 5,680  of  7,606.    Elapsed: 2:14:58.\n",
            "  Batch 5,720  of  7,606.    Elapsed: 2:15:55.\n",
            "  Batch 5,760  of  7,606.    Elapsed: 2:16:52.\n",
            "  Batch 5,800  of  7,606.    Elapsed: 2:17:49.\n",
            "  Batch 5,840  of  7,606.    Elapsed: 2:18:46.\n",
            "  Batch 5,880  of  7,606.    Elapsed: 2:19:43.\n",
            "  Batch 5,920  of  7,606.    Elapsed: 2:20:40.\n",
            "  Batch 5,960  of  7,606.    Elapsed: 2:21:37.\n",
            "  Batch 6,000  of  7,606.    Elapsed: 2:22:34.\n",
            "  Batch 6,040  of  7,606.    Elapsed: 2:23:32.\n",
            "  Batch 6,080  of  7,606.    Elapsed: 2:24:29.\n",
            "  Batch 6,120  of  7,606.    Elapsed: 2:25:26.\n",
            "  Batch 6,160  of  7,606.    Elapsed: 2:26:23.\n",
            "  Batch 6,200  of  7,606.    Elapsed: 2:27:20.\n",
            "  Batch 6,240  of  7,606.    Elapsed: 2:28:17.\n",
            "  Batch 6,280  of  7,606.    Elapsed: 2:29:14.\n",
            "  Batch 6,320  of  7,606.    Elapsed: 2:30:11.\n",
            "  Batch 6,360  of  7,606.    Elapsed: 2:31:08.\n",
            "  Batch 6,400  of  7,606.    Elapsed: 2:32:05.\n",
            "  Batch 6,440  of  7,606.    Elapsed: 2:33:02.\n",
            "  Batch 6,480  of  7,606.    Elapsed: 2:33:59.\n",
            "  Batch 6,520  of  7,606.    Elapsed: 2:34:56.\n",
            "  Batch 6,560  of  7,606.    Elapsed: 2:35:53.\n",
            "  Batch 6,600  of  7,606.    Elapsed: 2:36:50.\n",
            "  Batch 6,640  of  7,606.    Elapsed: 2:37:47.\n",
            "  Batch 6,680  of  7,606.    Elapsed: 2:38:44.\n",
            "  Batch 6,720  of  7,606.    Elapsed: 2:39:41.\n",
            "  Batch 6,760  of  7,606.    Elapsed: 2:40:38.\n",
            "  Batch 6,800  of  7,606.    Elapsed: 2:41:35.\n",
            "  Batch 6,840  of  7,606.    Elapsed: 2:42:32.\n",
            "  Batch 6,880  of  7,606.    Elapsed: 2:43:29.\n",
            "  Batch 6,920  of  7,606.    Elapsed: 2:44:26.\n",
            "  Batch 6,960  of  7,606.    Elapsed: 2:45:23.\n",
            "  Batch 7,000  of  7,606.    Elapsed: 2:46:20.\n",
            "  Batch 7,040  of  7,606.    Elapsed: 2:47:17.\n",
            "  Batch 7,080  of  7,606.    Elapsed: 2:48:14.\n",
            "  Batch 7,120  of  7,606.    Elapsed: 2:49:11.\n",
            "  Batch 7,160  of  7,606.    Elapsed: 2:50:08.\n",
            "  Batch 7,200  of  7,606.    Elapsed: 2:51:05.\n",
            "  Batch 7,240  of  7,606.    Elapsed: 2:52:02.\n",
            "  Batch 7,280  of  7,606.    Elapsed: 2:53:00.\n",
            "  Batch 7,320  of  7,606.    Elapsed: 2:53:57.\n",
            "  Batch 7,360  of  7,606.    Elapsed: 2:54:54.\n",
            "  Batch 7,400  of  7,606.    Elapsed: 2:55:51.\n",
            "  Batch 7,440  of  7,606.    Elapsed: 2:56:48.\n",
            "  Batch 7,480  of  7,606.    Elapsed: 2:57:45.\n",
            "  Batch 7,520  of  7,606.    Elapsed: 2:58:42.\n",
            "  Batch 7,560  of  7,606.    Elapsed: 2:59:39.\n",
            "  Batch 7,600  of  7,606.    Elapsed: 3:00:36.\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 3:00:44\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.56\n",
            "  F1 score: 0.51\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.83      0.33     16158\n",
            "           1       0.95      0.52      0.67    105534\n",
            "\n",
            "    accuracy                           0.56    121692\n",
            "   macro avg       0.58      0.68      0.50    121692\n",
            "weighted avg       0.85      0.56      0.63    121692\n",
            "\n",
            "[[13386  2772]\n",
            " [50478 55056]]\n",
            "Mathews Correlation coefficient score for this epoch is : 0.24\n",
            "Model validation score: f1=0.674 auc=0.949\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Validation Loss: 0.68\n",
            "  Validation took: 0:15:48\n",
            "\n",
            "Training complete!\n",
            "Total training took 9:49:32 (h:mm:ss)\n"
          ]
        }
      ],
      "source": [
        "#using weighted random sampler\n",
        "from sklearn.metrics import classification_report,auc,confusion_matrix,f1_score,precision_recall_curve,plot_precision_recall_curve,matthews_corrcoef\n",
        "import random\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "scaler = GradScaler()\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    roberta_model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        roberta_model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        with autocast():\n",
        "            loss, logits = roberta_model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "            b_labels1=torch.nn.functional.one_hot(b_labels, num_classes=2)\n",
        "            #Calculating weights\n",
        "            #positive=torch.sum(b_labels1, dim=0)\n",
        "           # negative=len(b_labels1)-positive\n",
        "            #negative\n",
        "            #pos_weight  = positive / negative\n",
        "            #criterion.pos_weight = pos_weight\n",
        "            loss1 = loss_fn(logits,b_labels1).to(device)\n",
        "           # print(\"loss:\",loss1)\n",
        "            loss1 = loss1 / gradient_accumulations\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss1.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        scaler.scale(loss1).backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "       # torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm*scaler.get_scale())\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        if ((step + 1) % gradient_accumulations == 0):\n",
        "             scaler.step(optimizer)\n",
        "       # Updates the scale for next iteration.\n",
        "             scaler.update()\n",
        "        # Update the learning rate.\n",
        "             scheduler.step()       \n",
        "             optimizer.zero_grad()\n",
        "            # roberta_model.zero_grad()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    roberta_model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    total_f1_score =0\n",
        "    predlist =[]\n",
        "    lbllist =[]\n",
        "    total_logits=[]\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            (loss, logits) = roberta_model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            #Converting the labels to one hot to sync with same shape as logits\n",
        "            b_labels1=torch.nn.functional.one_hot(b_labels, num_classes=2)\n",
        "            loss1 = loss_fn(logits, b_labels1)\n",
        "       # print(\"loss1:\",loss1)\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss1.item()\n",
        "\n",
        "         #Converting for predictions by applying sigmoid to logits\n",
        "        pred_logits_sigmoid=torch.sigmoid(logits)\n",
        "        y_pred=torch.round(pred_logits_sigmoid)\n",
        "        \n",
        "        # Move logits and labels to CPU\n",
        "        logits_pred = y_pred.detach().cpu().numpy()\n",
        "        label_ids1 = b_labels.to('cpu').numpy()\n",
        "        logits=logits.detach().cpu().numpy()\n",
        "        #For confusion matrix and classification report to work we need same dimensions.\n",
        "        label_ids = b_labels1.to('cpu').numpy()\n",
        "        pred_logits_sigmoid=pred_logits_sigmoid.detach().cpu().numpy()\n",
        "     \n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids1)\n",
        "\n",
        "         #print(predictions)\n",
        "       # predictions=np.argmax(logits_pred, axis=1)\n",
        "        predictions =np.argmax(logits_pred,axis=1)\n",
        "        y_test=np.argmax(label_ids,axis=1)\n",
        "        predlist.extend(predictions)\n",
        "        lbllist.extend(y_test)\n",
        "        #Accumulating the sigmoid positive logits for precision recall curve\n",
        "        total_logits.extend(pred_logits_sigmoid[:,1])\n",
        "        #total_logits.extend(pred_logits_sigmoid)\n",
        "        total_f1_score += f1_score(predlist,lbllist, average = 'macro')\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    #f1 score\n",
        "\n",
        "    avg_f1_score =total_f1_score/len(validation_dataloader)\n",
        "    print(\"  F1 score: {0:.2f}\".format(avg_f1_score))\n",
        "\n",
        "     #classification report\n",
        "    print(classification_report(lbllist, predlist))  \n",
        "\n",
        "    #confusion matrix\n",
        "    cm = confusion_matrix(lbllist,predlist)\n",
        "    # constant for classes\n",
        "    print(cm)\n",
        "    #mcc score\n",
        "    print(\"Mathews Correlation coefficient score for this epoch is :\",round(matthews_corrcoef(lbllist, predlist),2))\n",
        "    #Precision recall curve plot\n",
        "    lr_precision, lr_recall, thresholds = precision_recall_curve(lbllist,total_logits)\n",
        "    lr_f1, lr_auc = f1_score( lbllist,predlist), auc(lr_recall, lr_precision)\n",
        "    print('Model validation score: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
        "    baseline = lbllist.count(1) / len(lbllist)\n",
        "    plt.plot([0, 1], [baseline, baseline], linestyle='--', label='baseline')\n",
        "    plt.plot(lr_recall, lr_precision, marker='.', label='Roberta model evaluation')\n",
        "    # axis labels\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    # show the legend\n",
        "    plt.legend()\n",
        "    # show the plot\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZJG_wq6vXbE",
        "outputId": "0070f375-d389-4513-ead2-08df01065d58"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "121692"
            ]
          },
          "execution_count": 160,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(total_logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wE0g5b4Gv1r-"
      },
      "outputs": [],
      "source": [
        "print(\"Mathews Correlation coefficient score for this epoch is :\",round(matthews_corrcoef(lbllist, predlist),2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JDZoR_yHd10G",
        "outputId": "4521458d-68eb-4ba2-d4e3-d6c3f342e8bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of  7,606.    Elapsed: 0:00:57.\n",
            "  Batch    80  of  7,606.    Elapsed: 0:01:54.\n",
            "  Batch   120  of  7,606.    Elapsed: 0:02:51.\n",
            "  Batch   160  of  7,606.    Elapsed: 0:03:48.\n",
            "  Batch   200  of  7,606.    Elapsed: 0:04:46.\n",
            "  Batch   240  of  7,606.    Elapsed: 0:05:43.\n",
            "  Batch   280  of  7,606.    Elapsed: 0:06:40.\n",
            "  Batch   320  of  7,606.    Elapsed: 0:07:37.\n",
            "  Batch   360  of  7,606.    Elapsed: 0:08:34.\n",
            "  Batch   400  of  7,606.    Elapsed: 0:09:31.\n",
            "  Batch   440  of  7,606.    Elapsed: 0:10:28.\n",
            "  Batch   480  of  7,606.    Elapsed: 0:11:25.\n",
            "  Batch   520  of  7,606.    Elapsed: 0:12:22.\n",
            "  Batch   560  of  7,606.    Elapsed: 0:13:19.\n",
            "  Batch   600  of  7,606.    Elapsed: 0:14:16.\n",
            "  Batch   640  of  7,606.    Elapsed: 0:15:13.\n",
            "  Batch   680  of  7,606.    Elapsed: 0:16:10.\n",
            "  Batch   720  of  7,606.    Elapsed: 0:17:07.\n",
            "  Batch   760  of  7,606.    Elapsed: 0:18:04.\n",
            "  Batch   800  of  7,606.    Elapsed: 0:19:01.\n",
            "  Batch   840  of  7,606.    Elapsed: 0:19:58.\n",
            "  Batch   880  of  7,606.    Elapsed: 0:20:55.\n",
            "  Batch   920  of  7,606.    Elapsed: 0:21:53.\n",
            "  Batch   960  of  7,606.    Elapsed: 0:22:50.\n",
            "  Batch 1,000  of  7,606.    Elapsed: 0:23:47.\n",
            "  Batch 1,040  of  7,606.    Elapsed: 0:24:44.\n",
            "  Batch 1,080  of  7,606.    Elapsed: 0:25:41.\n",
            "  Batch 1,120  of  7,606.    Elapsed: 0:26:38.\n",
            "  Batch 1,160  of  7,606.    Elapsed: 0:27:35.\n",
            "  Batch 1,200  of  7,606.    Elapsed: 0:28:32.\n",
            "  Batch 1,240  of  7,606.    Elapsed: 0:29:29.\n",
            "  Batch 1,280  of  7,606.    Elapsed: 0:30:26.\n",
            "  Batch 1,320  of  7,606.    Elapsed: 0:31:23.\n",
            "  Batch 1,360  of  7,606.    Elapsed: 0:32:20.\n",
            "  Batch 1,400  of  7,606.    Elapsed: 0:33:17.\n",
            "  Batch 1,440  of  7,606.    Elapsed: 0:34:14.\n",
            "  Batch 1,480  of  7,606.    Elapsed: 0:35:11.\n",
            "  Batch 1,520  of  7,606.    Elapsed: 0:36:08.\n",
            "  Batch 1,560  of  7,606.    Elapsed: 0:37:05.\n",
            "  Batch 1,600  of  7,606.    Elapsed: 0:38:02.\n",
            "  Batch 1,640  of  7,606.    Elapsed: 0:38:59.\n",
            "  Batch 1,680  of  7,606.    Elapsed: 0:39:57.\n",
            "  Batch 1,720  of  7,606.    Elapsed: 0:40:54.\n",
            "  Batch 1,760  of  7,606.    Elapsed: 0:41:51.\n",
            "  Batch 1,800  of  7,606.    Elapsed: 0:42:48.\n",
            "  Batch 1,840  of  7,606.    Elapsed: 0:43:45.\n",
            "  Batch 1,880  of  7,606.    Elapsed: 0:44:42.\n",
            "  Batch 1,920  of  7,606.    Elapsed: 0:45:39.\n",
            "  Batch 1,960  of  7,606.    Elapsed: 0:46:36.\n",
            "  Batch 2,000  of  7,606.    Elapsed: 0:47:33.\n",
            "  Batch 2,040  of  7,606.    Elapsed: 0:48:30.\n",
            "  Batch 2,080  of  7,606.    Elapsed: 0:49:27.\n",
            "  Batch 2,120  of  7,606.    Elapsed: 0:50:24.\n",
            "  Batch 2,160  of  7,606.    Elapsed: 0:51:21.\n",
            "  Batch 2,200  of  7,606.    Elapsed: 0:52:18.\n",
            "  Batch 2,240  of  7,606.    Elapsed: 0:53:15.\n",
            "  Batch 2,280  of  7,606.    Elapsed: 0:54:12.\n",
            "  Batch 2,320  of  7,606.    Elapsed: 0:55:09.\n",
            "  Batch 2,360  of  7,606.    Elapsed: 0:56:06.\n",
            "  Batch 2,400  of  7,606.    Elapsed: 0:57:03.\n",
            "  Batch 2,440  of  7,606.    Elapsed: 0:58:01.\n",
            "  Batch 2,480  of  7,606.    Elapsed: 0:58:58.\n",
            "  Batch 2,520  of  7,606.    Elapsed: 0:59:55.\n",
            "  Batch 2,560  of  7,606.    Elapsed: 1:00:52.\n",
            "  Batch 2,600  of  7,606.    Elapsed: 1:01:49.\n",
            "  Batch 2,640  of  7,606.    Elapsed: 1:02:46.\n",
            "  Batch 2,680  of  7,606.    Elapsed: 1:03:43.\n",
            "  Batch 2,720  of  7,606.    Elapsed: 1:04:40.\n",
            "  Batch 2,760  of  7,606.    Elapsed: 1:05:37.\n",
            "  Batch 2,800  of  7,606.    Elapsed: 1:06:34.\n",
            "  Batch 2,840  of  7,606.    Elapsed: 1:07:31.\n",
            "  Batch 2,880  of  7,606.    Elapsed: 1:08:28.\n",
            "  Batch 2,920  of  7,606.    Elapsed: 1:09:25.\n",
            "  Batch 2,960  of  7,606.    Elapsed: 1:10:22.\n",
            "  Batch 3,000  of  7,606.    Elapsed: 1:11:19.\n",
            "  Batch 3,040  of  7,606.    Elapsed: 1:12:16.\n",
            "  Batch 3,080  of  7,606.    Elapsed: 1:13:13.\n",
            "  Batch 3,120  of  7,606.    Elapsed: 1:14:10.\n",
            "  Batch 3,160  of  7,606.    Elapsed: 1:15:07.\n",
            "  Batch 3,200  of  7,606.    Elapsed: 1:16:04.\n",
            "  Batch 3,240  of  7,606.    Elapsed: 1:17:02.\n",
            "  Batch 3,280  of  7,606.    Elapsed: 1:17:59.\n",
            "  Batch 3,320  of  7,606.    Elapsed: 1:18:56.\n",
            "  Batch 3,360  of  7,606.    Elapsed: 1:19:53.\n",
            "  Batch 3,400  of  7,606.    Elapsed: 1:20:50.\n",
            "  Batch 3,440  of  7,606.    Elapsed: 1:21:47.\n",
            "  Batch 3,480  of  7,606.    Elapsed: 1:22:44.\n",
            "  Batch 3,520  of  7,606.    Elapsed: 1:23:41.\n",
            "  Batch 3,560  of  7,606.    Elapsed: 1:24:38.\n",
            "  Batch 3,600  of  7,606.    Elapsed: 1:25:35.\n",
            "  Batch 3,640  of  7,606.    Elapsed: 1:26:32.\n",
            "  Batch 3,680  of  7,606.    Elapsed: 1:27:29.\n",
            "  Batch 3,720  of  7,606.    Elapsed: 1:28:26.\n",
            "  Batch 3,760  of  7,606.    Elapsed: 1:29:23.\n",
            "  Batch 3,800  of  7,606.    Elapsed: 1:30:20.\n",
            "  Batch 3,840  of  7,606.    Elapsed: 1:31:17.\n",
            "  Batch 3,880  of  7,606.    Elapsed: 1:32:14.\n",
            "  Batch 3,920  of  7,606.    Elapsed: 1:33:11.\n",
            "  Batch 3,960  of  7,606.    Elapsed: 1:34:08.\n",
            "  Batch 4,000  of  7,606.    Elapsed: 1:35:05.\n",
            "  Batch 4,040  of  7,606.    Elapsed: 1:36:02.\n",
            "  Batch 4,080  of  7,606.    Elapsed: 1:37:00.\n",
            "  Batch 4,120  of  7,606.    Elapsed: 1:37:57.\n",
            "  Batch 4,160  of  7,606.    Elapsed: 1:38:54.\n",
            "  Batch 4,200  of  7,606.    Elapsed: 1:39:51.\n",
            "  Batch 4,240  of  7,606.    Elapsed: 1:40:48.\n",
            "  Batch 4,280  of  7,606.    Elapsed: 1:41:45.\n",
            "  Batch 4,320  of  7,606.    Elapsed: 1:42:42.\n",
            "  Batch 4,360  of  7,606.    Elapsed: 1:43:39.\n",
            "  Batch 4,400  of  7,606.    Elapsed: 1:44:36.\n",
            "  Batch 4,440  of  7,606.    Elapsed: 1:45:33.\n",
            "  Batch 4,480  of  7,606.    Elapsed: 1:46:30.\n",
            "  Batch 4,520  of  7,606.    Elapsed: 1:47:27.\n",
            "  Batch 4,560  of  7,606.    Elapsed: 1:48:24.\n",
            "  Batch 4,600  of  7,606.    Elapsed: 1:49:21.\n",
            "  Batch 4,640  of  7,606.    Elapsed: 1:50:18.\n",
            "  Batch 4,680  of  7,606.    Elapsed: 1:51:15.\n",
            "  Batch 4,720  of  7,606.    Elapsed: 1:52:12.\n",
            "  Batch 4,760  of  7,606.    Elapsed: 1:53:09.\n",
            "  Batch 4,800  of  7,606.    Elapsed: 1:54:06.\n",
            "  Batch 4,840  of  7,606.    Elapsed: 1:55:03.\n",
            "  Batch 4,880  of  7,606.    Elapsed: 1:56:01.\n",
            "  Batch 4,920  of  7,606.    Elapsed: 1:56:58.\n",
            "  Batch 4,960  of  7,606.    Elapsed: 1:57:55.\n",
            "  Batch 5,000  of  7,606.    Elapsed: 1:58:52.\n",
            "  Batch 5,040  of  7,606.    Elapsed: 1:59:49.\n",
            "  Batch 5,080  of  7,606.    Elapsed: 2:00:46.\n",
            "  Batch 5,120  of  7,606.    Elapsed: 2:01:43.\n",
            "  Batch 5,160  of  7,606.    Elapsed: 2:02:40.\n",
            "  Batch 5,200  of  7,606.    Elapsed: 2:03:37.\n",
            "  Batch 5,240  of  7,606.    Elapsed: 2:04:34.\n",
            "  Batch 5,280  of  7,606.    Elapsed: 2:05:31.\n",
            "  Batch 5,320  of  7,606.    Elapsed: 2:06:28.\n",
            "  Batch 5,360  of  7,606.    Elapsed: 2:07:25.\n",
            "  Batch 5,400  of  7,606.    Elapsed: 2:08:22.\n",
            "  Batch 5,440  of  7,606.    Elapsed: 2:09:19.\n",
            "  Batch 5,480  of  7,606.    Elapsed: 2:10:16.\n",
            "  Batch 5,520  of  7,606.    Elapsed: 2:11:13.\n",
            "  Batch 5,560  of  7,606.    Elapsed: 2:12:10.\n",
            "  Batch 5,600  of  7,606.    Elapsed: 2:13:07.\n",
            "  Batch 5,640  of  7,606.    Elapsed: 2:14:05.\n",
            "  Batch 5,680  of  7,606.    Elapsed: 2:15:02.\n",
            "  Batch 5,720  of  7,606.    Elapsed: 2:15:59.\n",
            "  Batch 5,760  of  7,606.    Elapsed: 2:16:56.\n",
            "  Batch 5,800  of  7,606.    Elapsed: 2:17:53.\n",
            "  Batch 5,840  of  7,606.    Elapsed: 2:18:50.\n",
            "  Batch 5,880  of  7,606.    Elapsed: 2:19:47.\n",
            "  Batch 5,920  of  7,606.    Elapsed: 2:20:44.\n",
            "  Batch 5,960  of  7,606.    Elapsed: 2:21:41.\n",
            "  Batch 6,000  of  7,606.    Elapsed: 2:22:38.\n",
            "  Batch 6,040  of  7,606.    Elapsed: 2:23:35.\n",
            "  Batch 6,080  of  7,606.    Elapsed: 2:24:32.\n",
            "  Batch 6,120  of  7,606.    Elapsed: 2:25:29.\n",
            "  Batch 6,160  of  7,606.    Elapsed: 2:26:26.\n",
            "  Batch 6,200  of  7,606.    Elapsed: 2:27:23.\n",
            "  Batch 6,240  of  7,606.    Elapsed: 2:28:20.\n",
            "  Batch 6,280  of  7,606.    Elapsed: 2:29:17.\n",
            "  Batch 6,320  of  7,606.    Elapsed: 2:30:14.\n",
            "  Batch 6,360  of  7,606.    Elapsed: 2:31:11.\n",
            "  Batch 6,400  of  7,606.    Elapsed: 2:32:08.\n",
            "  Batch 6,440  of  7,606.    Elapsed: 2:33:06.\n",
            "  Batch 6,480  of  7,606.    Elapsed: 2:34:03.\n",
            "  Batch 6,520  of  7,606.    Elapsed: 2:35:00.\n",
            "  Batch 6,560  of  7,606.    Elapsed: 2:35:57.\n",
            "  Batch 6,600  of  7,606.    Elapsed: 2:36:54.\n",
            "  Batch 6,640  of  7,606.    Elapsed: 2:37:51.\n",
            "  Batch 6,680  of  7,606.    Elapsed: 2:38:48.\n",
            "  Batch 6,720  of  7,606.    Elapsed: 2:39:45.\n",
            "  Batch 6,760  of  7,606.    Elapsed: 2:40:42.\n",
            "  Batch 6,800  of  7,606.    Elapsed: 2:41:39.\n",
            "  Batch 6,840  of  7,606.    Elapsed: 2:42:36.\n",
            "  Batch 6,880  of  7,606.    Elapsed: 2:43:33.\n",
            "  Batch 6,920  of  7,606.    Elapsed: 2:44:30.\n",
            "  Batch 6,960  of  7,606.    Elapsed: 2:45:27.\n",
            "  Batch 7,000  of  7,606.    Elapsed: 2:46:24.\n",
            "  Batch 7,040  of  7,606.    Elapsed: 2:47:21.\n",
            "  Batch 7,080  of  7,606.    Elapsed: 2:48:18.\n",
            "  Batch 7,120  of  7,606.    Elapsed: 2:49:15.\n",
            "  Batch 7,160  of  7,606.    Elapsed: 2:50:12.\n",
            "  Batch 7,200  of  7,606.    Elapsed: 2:51:10.\n",
            "  Batch 7,240  of  7,606.    Elapsed: 2:52:07.\n",
            "  Batch 7,280  of  7,606.    Elapsed: 2:53:04.\n",
            "  Batch 7,320  of  7,606.    Elapsed: 2:54:01.\n",
            "  Batch 7,360  of  7,606.    Elapsed: 2:54:58.\n",
            "  Batch 7,400  of  7,606.    Elapsed: 2:55:55.\n",
            "  Batch 7,440  of  7,606.    Elapsed: 2:56:52.\n",
            "  Batch 7,480  of  7,606.    Elapsed: 2:57:49.\n",
            "  Batch 7,520  of  7,606.    Elapsed: 2:58:46.\n",
            "  Batch 7,560  of  7,606.    Elapsed: 2:59:43.\n",
            "  Batch 7,600  of  7,606.    Elapsed: 3:00:40.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 3:00:48\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.87\n",
            "  F1 score: 0.57\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.64      0.36     16168\n",
            "           1       0.93      0.70      0.80    105524\n",
            "\n",
            "    accuracy                           0.69    121692\n",
            "   macro avg       0.59      0.67      0.58    121692\n",
            "weighted avg       0.84      0.69      0.74    121692\n",
            "\n",
            "[[10397  5771]\n",
            " [31879 73645]]\n",
            "Mathews Correlation coefficient score for this epoch is : 0.24\n",
            "Model validation score: f1=0.796 auc=0.942\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Validation Loss: 0.53\n",
            "  Validation took: 0:14:18\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of  7,606.    Elapsed: 0:00:57.\n",
            "  Batch    80  of  7,606.    Elapsed: 0:01:54.\n",
            "  Batch   120  of  7,606.    Elapsed: 0:02:51.\n",
            "  Batch   160  of  7,606.    Elapsed: 0:03:48.\n",
            "  Batch   200  of  7,606.    Elapsed: 0:04:45.\n",
            "  Batch   240  of  7,606.    Elapsed: 0:05:43.\n",
            "  Batch   280  of  7,606.    Elapsed: 0:06:40.\n",
            "  Batch   320  of  7,606.    Elapsed: 0:07:37.\n",
            "  Batch   360  of  7,606.    Elapsed: 0:08:34.\n",
            "  Batch   400  of  7,606.    Elapsed: 0:09:31.\n",
            "  Batch   440  of  7,606.    Elapsed: 0:10:28.\n",
            "  Batch   480  of  7,606.    Elapsed: 0:11:25.\n",
            "  Batch   520  of  7,606.    Elapsed: 0:12:22.\n",
            "  Batch   560  of  7,606.    Elapsed: 0:13:19.\n",
            "  Batch   600  of  7,606.    Elapsed: 0:14:16.\n",
            "  Batch   640  of  7,606.    Elapsed: 0:15:13.\n",
            "  Batch   680  of  7,606.    Elapsed: 0:16:10.\n",
            "  Batch   720  of  7,606.    Elapsed: 0:17:07.\n",
            "  Batch   760  of  7,606.    Elapsed: 0:18:04.\n",
            "  Batch   800  of  7,606.    Elapsed: 0:19:01.\n",
            "  Batch   840  of  7,606.    Elapsed: 0:19:58.\n",
            "  Batch   880  of  7,606.    Elapsed: 0:20:55.\n",
            "  Batch   920  of  7,606.    Elapsed: 0:21:52.\n",
            "  Batch   960  of  7,606.    Elapsed: 0:22:49.\n",
            "  Batch 1,000  of  7,606.    Elapsed: 0:23:47.\n",
            "  Batch 1,040  of  7,606.    Elapsed: 0:24:44.\n",
            "  Batch 1,080  of  7,606.    Elapsed: 0:25:41.\n",
            "  Batch 1,120  of  7,606.    Elapsed: 0:26:38.\n",
            "  Batch 1,160  of  7,606.    Elapsed: 0:27:35.\n",
            "  Batch 1,200  of  7,606.    Elapsed: 0:28:32.\n",
            "  Batch 1,240  of  7,606.    Elapsed: 0:29:29.\n",
            "  Batch 1,280  of  7,606.    Elapsed: 0:30:26.\n",
            "  Batch 1,320  of  7,606.    Elapsed: 0:31:23.\n",
            "  Batch 1,360  of  7,606.    Elapsed: 0:32:20.\n",
            "  Batch 1,400  of  7,606.    Elapsed: 0:33:17.\n",
            "  Batch 1,440  of  7,606.    Elapsed: 0:34:14.\n",
            "  Batch 1,480  of  7,606.    Elapsed: 0:35:11.\n",
            "  Batch 1,520  of  7,606.    Elapsed: 0:36:08.\n",
            "  Batch 1,560  of  7,606.    Elapsed: 0:37:05.\n",
            "  Batch 1,600  of  7,606.    Elapsed: 0:38:02.\n",
            "  Batch 1,640  of  7,606.    Elapsed: 0:38:59.\n",
            "  Batch 1,680  of  7,606.    Elapsed: 0:39:56.\n",
            "  Batch 1,720  of  7,606.    Elapsed: 0:40:53.\n",
            "  Batch 1,760  of  7,606.    Elapsed: 0:41:50.\n",
            "  Batch 1,800  of  7,606.    Elapsed: 0:42:48.\n",
            "  Batch 1,840  of  7,606.    Elapsed: 0:43:45.\n",
            "  Batch 1,880  of  7,606.    Elapsed: 0:44:42.\n",
            "  Batch 1,920  of  7,606.    Elapsed: 0:45:39.\n",
            "  Batch 1,960  of  7,606.    Elapsed: 0:46:36.\n",
            "  Batch 2,000  of  7,606.    Elapsed: 0:47:33.\n",
            "  Batch 2,040  of  7,606.    Elapsed: 0:48:30.\n",
            "  Batch 2,080  of  7,606.    Elapsed: 0:49:27.\n",
            "  Batch 2,120  of  7,606.    Elapsed: 0:50:24.\n",
            "  Batch 2,160  of  7,606.    Elapsed: 0:51:21.\n",
            "  Batch 2,200  of  7,606.    Elapsed: 0:52:18.\n",
            "  Batch 2,240  of  7,606.    Elapsed: 0:53:15.\n",
            "  Batch 2,280  of  7,606.    Elapsed: 0:54:12.\n",
            "  Batch 2,320  of  7,606.    Elapsed: 0:55:09.\n",
            "  Batch 2,360  of  7,606.    Elapsed: 0:56:06.\n",
            "  Batch 2,400  of  7,606.    Elapsed: 0:57:03.\n",
            "  Batch 2,440  of  7,606.    Elapsed: 0:58:00.\n",
            "  Batch 2,480  of  7,606.    Elapsed: 0:58:57.\n",
            "  Batch 2,520  of  7,606.    Elapsed: 0:59:54.\n",
            "  Batch 2,560  of  7,606.    Elapsed: 1:00:51.\n",
            "  Batch 2,600  of  7,606.    Elapsed: 1:01:49.\n",
            "  Batch 2,640  of  7,606.    Elapsed: 1:02:46.\n",
            "  Batch 2,680  of  7,606.    Elapsed: 1:03:43.\n",
            "  Batch 2,720  of  7,606.    Elapsed: 1:04:40.\n",
            "  Batch 2,760  of  7,606.    Elapsed: 1:05:37.\n",
            "  Batch 2,800  of  7,606.    Elapsed: 1:06:34.\n",
            "  Batch 2,840  of  7,606.    Elapsed: 1:07:31.\n",
            "  Batch 2,880  of  7,606.    Elapsed: 1:08:28.\n",
            "  Batch 2,920  of  7,606.    Elapsed: 1:09:25.\n",
            "  Batch 2,960  of  7,606.    Elapsed: 1:10:22.\n",
            "  Batch 3,000  of  7,606.    Elapsed: 1:11:19.\n",
            "  Batch 3,040  of  7,606.    Elapsed: 1:12:16.\n",
            "  Batch 3,080  of  7,606.    Elapsed: 1:13:13.\n",
            "  Batch 3,120  of  7,606.    Elapsed: 1:14:10.\n",
            "  Batch 3,160  of  7,606.    Elapsed: 1:15:07.\n",
            "  Batch 3,200  of  7,606.    Elapsed: 1:16:04.\n",
            "  Batch 3,240  of  7,606.    Elapsed: 1:17:01.\n",
            "  Batch 3,280  of  7,606.    Elapsed: 1:17:58.\n",
            "  Batch 3,320  of  7,606.    Elapsed: 1:18:55.\n",
            "  Batch 3,360  of  7,606.    Elapsed: 1:19:53.\n",
            "  Batch 3,400  of  7,606.    Elapsed: 1:20:50.\n",
            "  Batch 3,440  of  7,606.    Elapsed: 1:21:47.\n",
            "  Batch 3,480  of  7,606.    Elapsed: 1:22:44.\n",
            "  Batch 3,520  of  7,606.    Elapsed: 1:23:41.\n",
            "  Batch 3,560  of  7,606.    Elapsed: 1:24:38.\n",
            "  Batch 3,600  of  7,606.    Elapsed: 1:25:35.\n",
            "  Batch 3,640  of  7,606.    Elapsed: 1:26:32.\n",
            "  Batch 3,680  of  7,606.    Elapsed: 1:27:29.\n",
            "  Batch 3,720  of  7,606.    Elapsed: 1:28:26.\n",
            "  Batch 3,760  of  7,606.    Elapsed: 1:29:23.\n",
            "  Batch 3,800  of  7,606.    Elapsed: 1:30:20.\n",
            "  Batch 3,840  of  7,606.    Elapsed: 1:31:17.\n",
            "  Batch 3,880  of  7,606.    Elapsed: 1:32:14.\n",
            "  Batch 3,920  of  7,606.    Elapsed: 1:33:11.\n",
            "  Batch 3,960  of  7,606.    Elapsed: 1:34:08.\n",
            "  Batch 4,000  of  7,606.    Elapsed: 1:35:05.\n",
            "  Batch 4,040  of  7,606.    Elapsed: 1:36:02.\n",
            "  Batch 4,080  of  7,606.    Elapsed: 1:36:59.\n",
            "  Batch 4,120  of  7,606.    Elapsed: 1:37:56.\n",
            "  Batch 4,160  of  7,606.    Elapsed: 1:38:53.\n",
            "  Batch 4,200  of  7,606.    Elapsed: 1:39:50.\n",
            "  Batch 4,240  of  7,606.    Elapsed: 1:40:48.\n",
            "  Batch 4,280  of  7,606.    Elapsed: 1:41:45.\n",
            "  Batch 4,320  of  7,606.    Elapsed: 1:42:42.\n",
            "  Batch 4,360  of  7,606.    Elapsed: 1:43:39.\n",
            "  Batch 4,400  of  7,606.    Elapsed: 1:44:36.\n",
            "  Batch 4,440  of  7,606.    Elapsed: 1:45:33.\n",
            "  Batch 4,480  of  7,606.    Elapsed: 1:46:30.\n",
            "  Batch 4,520  of  7,606.    Elapsed: 1:47:27.\n",
            "  Batch 4,560  of  7,606.    Elapsed: 1:48:24.\n",
            "  Batch 4,600  of  7,606.    Elapsed: 1:49:21.\n",
            "  Batch 4,640  of  7,606.    Elapsed: 1:50:18.\n",
            "  Batch 4,680  of  7,606.    Elapsed: 1:51:15.\n",
            "  Batch 4,720  of  7,606.    Elapsed: 1:52:12.\n",
            "  Batch 4,760  of  7,606.    Elapsed: 1:53:09.\n",
            "  Batch 4,800  of  7,606.    Elapsed: 1:54:06.\n",
            "  Batch 4,840  of  7,606.    Elapsed: 1:55:03.\n",
            "  Batch 4,880  of  7,606.    Elapsed: 1:56:00.\n",
            "  Batch 4,920  of  7,606.    Elapsed: 1:56:57.\n",
            "  Batch 4,960  of  7,606.    Elapsed: 1:57:54.\n",
            "  Batch 5,000  of  7,606.    Elapsed: 1:58:52.\n",
            "  Batch 5,040  of  7,606.    Elapsed: 1:59:49.\n",
            "  Batch 5,080  of  7,606.    Elapsed: 2:00:46.\n",
            "  Batch 5,120  of  7,606.    Elapsed: 2:01:43.\n",
            "  Batch 5,160  of  7,606.    Elapsed: 2:02:40.\n",
            "  Batch 5,200  of  7,606.    Elapsed: 2:03:37.\n",
            "  Batch 5,240  of  7,606.    Elapsed: 2:04:34.\n",
            "  Batch 5,280  of  7,606.    Elapsed: 2:05:31.\n",
            "  Batch 5,320  of  7,606.    Elapsed: 2:06:28.\n",
            "  Batch 5,360  of  7,606.    Elapsed: 2:07:25.\n",
            "  Batch 5,400  of  7,606.    Elapsed: 2:08:22.\n",
            "  Batch 5,440  of  7,606.    Elapsed: 2:09:19.\n",
            "  Batch 5,480  of  7,606.    Elapsed: 2:10:16.\n",
            "  Batch 5,520  of  7,606.    Elapsed: 2:11:13.\n",
            "  Batch 5,560  of  7,606.    Elapsed: 2:12:10.\n",
            "  Batch 5,600  of  7,606.    Elapsed: 2:13:07.\n",
            "  Batch 5,640  of  7,606.    Elapsed: 2:14:04.\n",
            "  Batch 5,680  of  7,606.    Elapsed: 2:15:01.\n",
            "  Batch 5,720  of  7,606.    Elapsed: 2:15:58.\n",
            "  Batch 5,760  of  7,606.    Elapsed: 2:16:55.\n",
            "  Batch 5,800  of  7,606.    Elapsed: 2:17:52.\n",
            "  Batch 5,840  of  7,606.    Elapsed: 2:18:50.\n",
            "  Batch 5,880  of  7,606.    Elapsed: 2:19:47.\n",
            "  Batch 5,920  of  7,606.    Elapsed: 2:20:44.\n",
            "  Batch 5,960  of  7,606.    Elapsed: 2:21:41.\n",
            "  Batch 6,000  of  7,606.    Elapsed: 2:22:38.\n",
            "  Batch 6,040  of  7,606.    Elapsed: 2:23:35.\n",
            "  Batch 6,080  of  7,606.    Elapsed: 2:24:32.\n",
            "  Batch 6,120  of  7,606.    Elapsed: 2:25:29.\n",
            "  Batch 6,160  of  7,606.    Elapsed: 2:26:26.\n",
            "  Batch 6,200  of  7,606.    Elapsed: 2:27:23.\n",
            "  Batch 6,240  of  7,606.    Elapsed: 2:28:20.\n",
            "  Batch 6,280  of  7,606.    Elapsed: 2:29:17.\n",
            "  Batch 6,320  of  7,606.    Elapsed: 2:30:14.\n",
            "  Batch 6,360  of  7,606.    Elapsed: 2:31:11.\n",
            "  Batch 6,400  of  7,606.    Elapsed: 2:32:08.\n",
            "  Batch 6,440  of  7,606.    Elapsed: 2:33:05.\n",
            "  Batch 6,480  of  7,606.    Elapsed: 2:34:02.\n",
            "  Batch 6,520  of  7,606.    Elapsed: 2:34:59.\n",
            "  Batch 6,560  of  7,606.    Elapsed: 2:35:56.\n",
            "  Batch 6,600  of  7,606.    Elapsed: 2:36:53.\n",
            "  Batch 6,640  of  7,606.    Elapsed: 2:37:51.\n",
            "  Batch 6,680  of  7,606.    Elapsed: 2:38:48.\n",
            "  Batch 6,720  of  7,606.    Elapsed: 2:39:45.\n",
            "  Batch 6,760  of  7,606.    Elapsed: 2:40:42.\n",
            "  Batch 6,800  of  7,606.    Elapsed: 2:41:39.\n",
            "  Batch 6,840  of  7,606.    Elapsed: 2:42:36.\n",
            "  Batch 6,880  of  7,606.    Elapsed: 2:43:33.\n",
            "  Batch 6,920  of  7,606.    Elapsed: 2:44:30.\n",
            "  Batch 6,960  of  7,606.    Elapsed: 2:45:27.\n",
            "  Batch 7,000  of  7,606.    Elapsed: 2:46:24.\n",
            "  Batch 7,040  of  7,606.    Elapsed: 2:47:21.\n",
            "  Batch 7,080  of  7,606.    Elapsed: 2:48:18.\n",
            "  Batch 7,120  of  7,606.    Elapsed: 2:49:15.\n",
            "  Batch 7,160  of  7,606.    Elapsed: 2:50:12.\n",
            "  Batch 7,200  of  7,606.    Elapsed: 2:51:09.\n",
            "  Batch 7,240  of  7,606.    Elapsed: 2:52:06.\n",
            "  Batch 7,280  of  7,606.    Elapsed: 2:53:03.\n",
            "  Batch 7,320  of  7,606.    Elapsed: 2:54:00.\n",
            "  Batch 7,360  of  7,606.    Elapsed: 2:54:57.\n",
            "  Batch 7,400  of  7,606.    Elapsed: 2:55:55.\n",
            "  Batch 7,440  of  7,606.    Elapsed: 2:56:52.\n",
            "  Batch 7,480  of  7,606.    Elapsed: 2:57:49.\n",
            "  Batch 7,520  of  7,606.    Elapsed: 2:58:46.\n",
            "  Batch 7,560  of  7,606.    Elapsed: 2:59:43.\n",
            "  Batch 7,600  of  7,606.    Elapsed: 3:00:40.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 3:00:48\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.87\n",
            "  F1 score: 0.55\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.23      0.73      0.35     16168\n",
            "           1       0.94      0.63      0.75    105524\n",
            "\n",
            "    accuracy                           0.64    121692\n",
            "   macro avg       0.58      0.68      0.55    121692\n",
            "weighted avg       0.84      0.64      0.70    121692\n",
            "\n",
            "[[11834  4334]\n",
            " [39554 65970]]\n",
            "Mathews Correlation coefficient score for this epoch is : 0.25\n",
            "Model validation score: f1=0.750 auc=0.945\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Validation Loss: 0.56\n",
            "  Validation took: 0:14:18\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of  7,606.    Elapsed: 0:00:57.\n",
            "  Batch    80  of  7,606.    Elapsed: 0:01:54.\n",
            "  Batch   120  of  7,606.    Elapsed: 0:02:51.\n",
            "  Batch   160  of  7,606.    Elapsed: 0:03:48.\n",
            "  Batch   200  of  7,606.    Elapsed: 0:04:45.\n",
            "  Batch   240  of  7,606.    Elapsed: 0:05:43.\n",
            "  Batch   280  of  7,606.    Elapsed: 0:06:40.\n",
            "  Batch   320  of  7,606.    Elapsed: 0:07:37.\n",
            "  Batch   360  of  7,606.    Elapsed: 0:08:34.\n",
            "  Batch   400  of  7,606.    Elapsed: 0:09:31.\n",
            "  Batch   440  of  7,606.    Elapsed: 0:10:28.\n",
            "  Batch   480  of  7,606.    Elapsed: 0:11:25.\n",
            "  Batch   520  of  7,606.    Elapsed: 0:12:22.\n",
            "  Batch   560  of  7,606.    Elapsed: 0:13:19.\n",
            "  Batch   600  of  7,606.    Elapsed: 0:14:16.\n",
            "  Batch   640  of  7,606.    Elapsed: 0:15:13.\n",
            "  Batch   680  of  7,606.    Elapsed: 0:16:10.\n",
            "  Batch   720  of  7,606.    Elapsed: 0:17:07.\n",
            "  Batch   760  of  7,606.    Elapsed: 0:18:04.\n",
            "  Batch   800  of  7,606.    Elapsed: 0:19:01.\n",
            "  Batch   840  of  7,606.    Elapsed: 0:19:58.\n",
            "  Batch   880  of  7,606.    Elapsed: 0:20:55.\n",
            "  Batch   920  of  7,606.    Elapsed: 0:21:53.\n",
            "  Batch   960  of  7,606.    Elapsed: 0:22:50.\n",
            "  Batch 1,000  of  7,606.    Elapsed: 0:23:47.\n",
            "  Batch 1,040  of  7,606.    Elapsed: 0:24:44.\n",
            "  Batch 1,080  of  7,606.    Elapsed: 0:25:41.\n",
            "  Batch 1,120  of  7,606.    Elapsed: 0:26:38.\n",
            "  Batch 1,160  of  7,606.    Elapsed: 0:27:35.\n",
            "  Batch 1,200  of  7,606.    Elapsed: 0:28:32.\n",
            "  Batch 1,240  of  7,606.    Elapsed: 0:29:29.\n",
            "  Batch 1,280  of  7,606.    Elapsed: 0:30:26.\n",
            "  Batch 1,320  of  7,606.    Elapsed: 0:31:23.\n",
            "  Batch 1,360  of  7,606.    Elapsed: 0:32:20.\n",
            "  Batch 1,400  of  7,606.    Elapsed: 0:33:17.\n",
            "  Batch 1,440  of  7,606.    Elapsed: 0:34:14.\n",
            "  Batch 1,480  of  7,606.    Elapsed: 0:35:11.\n",
            "  Batch 1,520  of  7,606.    Elapsed: 0:36:08.\n",
            "  Batch 1,560  of  7,606.    Elapsed: 0:37:05.\n",
            "  Batch 1,600  of  7,606.    Elapsed: 0:38:02.\n",
            "  Batch 1,640  of  7,606.    Elapsed: 0:38:59.\n",
            "  Batch 1,680  of  7,606.    Elapsed: 0:39:57.\n",
            "  Batch 1,720  of  7,606.    Elapsed: 0:40:54.\n",
            "  Batch 1,760  of  7,606.    Elapsed: 0:41:51.\n",
            "  Batch 1,800  of  7,606.    Elapsed: 0:42:48.\n",
            "  Batch 1,840  of  7,606.    Elapsed: 0:43:45.\n",
            "  Batch 1,880  of  7,606.    Elapsed: 0:44:42.\n",
            "  Batch 1,920  of  7,606.    Elapsed: 0:45:39.\n",
            "  Batch 1,960  of  7,606.    Elapsed: 0:46:36.\n",
            "  Batch 2,000  of  7,606.    Elapsed: 0:47:33.\n",
            "  Batch 2,040  of  7,606.    Elapsed: 0:48:30.\n",
            "  Batch 2,080  of  7,606.    Elapsed: 0:49:27.\n",
            "  Batch 2,120  of  7,606.    Elapsed: 0:50:24.\n",
            "  Batch 2,160  of  7,606.    Elapsed: 0:51:21.\n",
            "  Batch 2,200  of  7,606.    Elapsed: 0:52:18.\n",
            "  Batch 2,240  of  7,606.    Elapsed: 0:53:15.\n",
            "  Batch 2,280  of  7,606.    Elapsed: 0:54:12.\n",
            "  Batch 2,320  of  7,606.    Elapsed: 0:55:09.\n",
            "  Batch 2,360  of  7,606.    Elapsed: 0:56:06.\n",
            "  Batch 2,400  of  7,606.    Elapsed: 0:57:03.\n",
            "  Batch 2,440  of  7,606.    Elapsed: 0:58:00.\n",
            "  Batch 2,480  of  7,606.    Elapsed: 0:58:58.\n",
            "  Batch 2,520  of  7,606.    Elapsed: 0:59:55.\n",
            "  Batch 2,560  of  7,606.    Elapsed: 1:00:52.\n",
            "  Batch 2,600  of  7,606.    Elapsed: 1:01:49.\n",
            "  Batch 2,640  of  7,606.    Elapsed: 1:02:46.\n",
            "  Batch 2,680  of  7,606.    Elapsed: 1:03:43.\n",
            "  Batch 2,720  of  7,606.    Elapsed: 1:04:40.\n",
            "  Batch 2,760  of  7,606.    Elapsed: 1:05:37.\n",
            "  Batch 2,800  of  7,606.    Elapsed: 1:06:34.\n",
            "  Batch 2,840  of  7,606.    Elapsed: 1:07:31.\n",
            "  Batch 2,880  of  7,606.    Elapsed: 1:08:28.\n",
            "  Batch 2,920  of  7,606.    Elapsed: 1:09:25.\n",
            "  Batch 2,960  of  7,606.    Elapsed: 1:10:22.\n",
            "  Batch 3,000  of  7,606.    Elapsed: 1:11:19.\n",
            "  Batch 3,040  of  7,606.    Elapsed: 1:12:16.\n",
            "  Batch 3,080  of  7,606.    Elapsed: 1:13:13.\n",
            "  Batch 3,120  of  7,606.    Elapsed: 1:14:10.\n",
            "  Batch 3,160  of  7,606.    Elapsed: 1:15:07.\n",
            "  Batch 3,200  of  7,606.    Elapsed: 1:16:04.\n",
            "  Batch 3,240  of  7,606.    Elapsed: 1:17:01.\n",
            "  Batch 3,280  of  7,606.    Elapsed: 1:17:59.\n",
            "  Batch 3,320  of  7,606.    Elapsed: 1:18:56.\n",
            "  Batch 3,360  of  7,606.    Elapsed: 1:19:53.\n",
            "  Batch 3,400  of  7,606.    Elapsed: 1:20:50.\n",
            "  Batch 3,440  of  7,606.    Elapsed: 1:21:47.\n",
            "  Batch 3,480  of  7,606.    Elapsed: 1:22:44.\n",
            "  Batch 3,520  of  7,606.    Elapsed: 1:23:41.\n",
            "  Batch 3,560  of  7,606.    Elapsed: 1:24:38.\n",
            "  Batch 3,600  of  7,606.    Elapsed: 1:25:35.\n",
            "  Batch 3,640  of  7,606.    Elapsed: 1:26:32.\n",
            "  Batch 3,680  of  7,606.    Elapsed: 1:27:29.\n",
            "  Batch 3,720  of  7,606.    Elapsed: 1:28:26.\n",
            "  Batch 3,760  of  7,606.    Elapsed: 1:29:23.\n",
            "  Batch 3,800  of  7,606.    Elapsed: 1:30:20.\n",
            "  Batch 3,840  of  7,606.    Elapsed: 1:31:17.\n",
            "  Batch 3,880  of  7,606.    Elapsed: 1:32:14.\n",
            "  Batch 3,920  of  7,606.    Elapsed: 1:33:11.\n",
            "  Batch 3,960  of  7,606.    Elapsed: 1:34:08.\n",
            "  Batch 4,000  of  7,606.    Elapsed: 1:35:06.\n",
            "  Batch 4,040  of  7,606.    Elapsed: 1:36:03.\n",
            "  Batch 4,080  of  7,606.    Elapsed: 1:37:00.\n",
            "  Batch 4,120  of  7,606.    Elapsed: 1:37:57.\n",
            "  Batch 4,160  of  7,606.    Elapsed: 1:38:54.\n",
            "  Batch 4,200  of  7,606.    Elapsed: 1:39:51.\n",
            "  Batch 4,240  of  7,606.    Elapsed: 1:40:48.\n",
            "  Batch 4,280  of  7,606.    Elapsed: 1:41:45.\n",
            "  Batch 4,320  of  7,606.    Elapsed: 1:42:42.\n",
            "  Batch 4,360  of  7,606.    Elapsed: 1:43:39.\n",
            "  Batch 4,400  of  7,606.    Elapsed: 1:44:36.\n",
            "  Batch 4,440  of  7,606.    Elapsed: 1:45:33.\n",
            "  Batch 4,480  of  7,606.    Elapsed: 1:46:30.\n",
            "  Batch 4,520  of  7,606.    Elapsed: 1:47:27.\n",
            "  Batch 4,560  of  7,606.    Elapsed: 1:48:24.\n",
            "  Batch 4,600  of  7,606.    Elapsed: 1:49:21.\n",
            "  Batch 4,640  of  7,606.    Elapsed: 1:50:18.\n",
            "  Batch 4,680  of  7,606.    Elapsed: 1:51:16.\n",
            "  Batch 4,720  of  7,606.    Elapsed: 1:52:13.\n",
            "  Batch 4,760  of  7,606.    Elapsed: 1:53:10.\n",
            "  Batch 4,800  of  7,606.    Elapsed: 1:54:07.\n",
            "  Batch 4,840  of  7,606.    Elapsed: 1:55:04.\n",
            "  Batch 4,880  of  7,606.    Elapsed: 1:56:01.\n",
            "  Batch 4,920  of  7,606.    Elapsed: 1:56:58.\n",
            "  Batch 4,960  of  7,606.    Elapsed: 1:57:55.\n",
            "  Batch 5,000  of  7,606.    Elapsed: 1:58:52.\n",
            "  Batch 5,040  of  7,606.    Elapsed: 1:59:49.\n",
            "  Batch 5,080  of  7,606.    Elapsed: 2:00:46.\n",
            "  Batch 5,120  of  7,606.    Elapsed: 2:01:43.\n",
            "  Batch 5,160  of  7,606.    Elapsed: 2:02:40.\n",
            "  Batch 5,200  of  7,606.    Elapsed: 2:03:37.\n",
            "  Batch 5,240  of  7,606.    Elapsed: 2:04:34.\n",
            "  Batch 5,280  of  7,606.    Elapsed: 2:05:31.\n",
            "  Batch 5,320  of  7,606.    Elapsed: 2:06:28.\n",
            "  Batch 5,360  of  7,606.    Elapsed: 2:07:25.\n",
            "  Batch 5,400  of  7,606.    Elapsed: 2:08:22.\n",
            "  Batch 5,440  of  7,606.    Elapsed: 2:09:20.\n",
            "  Batch 5,480  of  7,606.    Elapsed: 2:10:17.\n",
            "  Batch 5,520  of  7,606.    Elapsed: 2:11:14.\n",
            "  Batch 5,560  of  7,606.    Elapsed: 2:12:11.\n",
            "  Batch 5,600  of  7,606.    Elapsed: 2:13:08.\n",
            "  Batch 5,640  of  7,606.    Elapsed: 2:14:05.\n",
            "  Batch 5,680  of  7,606.    Elapsed: 2:15:02.\n",
            "  Batch 5,720  of  7,606.    Elapsed: 2:15:59.\n",
            "  Batch 5,760  of  7,606.    Elapsed: 2:16:56.\n",
            "  Batch 5,800  of  7,606.    Elapsed: 2:17:53.\n",
            "  Batch 5,840  of  7,606.    Elapsed: 2:18:50.\n",
            "  Batch 5,880  of  7,606.    Elapsed: 2:19:47.\n",
            "  Batch 5,920  of  7,606.    Elapsed: 2:20:44.\n",
            "  Batch 5,960  of  7,606.    Elapsed: 2:21:41.\n",
            "  Batch 6,000  of  7,606.    Elapsed: 2:22:38.\n",
            "  Batch 6,040  of  7,606.    Elapsed: 2:23:35.\n",
            "  Batch 6,080  of  7,606.    Elapsed: 2:24:32.\n",
            "  Batch 6,120  of  7,606.    Elapsed: 2:25:29.\n",
            "  Batch 6,160  of  7,606.    Elapsed: 2:26:27.\n",
            "  Batch 6,200  of  7,606.    Elapsed: 2:27:24.\n",
            "  Batch 6,240  of  7,606.    Elapsed: 2:28:21.\n",
            "  Batch 6,280  of  7,606.    Elapsed: 2:29:18.\n",
            "  Batch 6,320  of  7,606.    Elapsed: 2:30:15.\n",
            "  Batch 6,360  of  7,606.    Elapsed: 2:31:12.\n",
            "  Batch 6,400  of  7,606.    Elapsed: 2:32:09.\n",
            "  Batch 6,440  of  7,606.    Elapsed: 2:33:06.\n",
            "  Batch 6,480  of  7,606.    Elapsed: 2:34:03.\n",
            "  Batch 6,520  of  7,606.    Elapsed: 2:35:00.\n",
            "  Batch 6,560  of  7,606.    Elapsed: 2:35:57.\n",
            "  Batch 6,600  of  7,606.    Elapsed: 2:36:54.\n",
            "  Batch 6,640  of  7,606.    Elapsed: 2:37:51.\n",
            "  Batch 6,680  of  7,606.    Elapsed: 2:38:48.\n",
            "  Batch 6,720  of  7,606.    Elapsed: 2:39:45.\n",
            "  Batch 6,760  of  7,606.    Elapsed: 2:40:42.\n",
            "  Batch 6,800  of  7,606.    Elapsed: 2:41:39.\n",
            "  Batch 6,840  of  7,606.    Elapsed: 2:42:36.\n",
            "  Batch 6,880  of  7,606.    Elapsed: 2:43:34.\n",
            "  Batch 6,920  of  7,606.    Elapsed: 2:44:31.\n",
            "  Batch 6,960  of  7,606.    Elapsed: 2:45:28.\n",
            "  Batch 7,000  of  7,606.    Elapsed: 2:46:25.\n",
            "  Batch 7,040  of  7,606.    Elapsed: 2:47:22.\n",
            "  Batch 7,080  of  7,606.    Elapsed: 2:48:19.\n",
            "  Batch 7,120  of  7,606.    Elapsed: 2:49:16.\n",
            "  Batch 7,160  of  7,606.    Elapsed: 2:50:13.\n",
            "  Batch 7,200  of  7,606.    Elapsed: 2:51:10.\n",
            "  Batch 7,240  of  7,606.    Elapsed: 2:52:07.\n",
            "  Batch 7,280  of  7,606.    Elapsed: 2:53:04.\n",
            "  Batch 7,320  of  7,606.    Elapsed: 2:54:01.\n",
            "  Batch 7,360  of  7,606.    Elapsed: 2:54:58.\n",
            "  Batch 7,400  of  7,606.    Elapsed: 2:55:55.\n",
            "  Batch 7,440  of  7,606.    Elapsed: 2:56:52.\n",
            "  Batch 7,480  of  7,606.    Elapsed: 2:57:49.\n",
            "  Batch 7,520  of  7,606.    Elapsed: 2:58:46.\n",
            "  Batch 7,560  of  7,606.    Elapsed: 2:59:43.\n",
            "  Batch 7,600  of  7,606.    Elapsed: 3:00:40.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 3:00:49\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.87\n",
            "  F1 score: 0.60\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.28      0.57      0.38     16168\n",
            "           1       0.92      0.78      0.84    105524\n",
            "\n",
            "    accuracy                           0.75    121692\n",
            "   macro avg       0.60      0.67      0.61    121692\n",
            "weighted avg       0.84      0.75      0.78    121692\n",
            "\n",
            "[[ 9187  6981]\n",
            " [23461 82063]]\n",
            "Mathews Correlation coefficient score for this epoch is : 0.26\n",
            "Model validation score: f1=0.844 auc=0.947\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3wV1b338c+PAKKACEKpCgL2gBIgAsYIokJBEK3GqlDhiELxFC+lPj099qm250ikUvFWe1R6LFYE8YaiteAVBCnikUJQRAERiihBHkUQKCJy+z1/7EnchJ1kJ9mzL9nf9+u1X+yZtWbPmiTkmzVrZo25OyIiIuXVS3UDREQkPSkgREQkJgWEiIjEpIAQEZGYFBAiIhJT/VQ3IFFatmzp7du3T3UzREQyyrJly75w91axyupMQLRv357i4uJUN0NEJKOY2ccVlekUk4iIxKSAEBGRmBQQIiISU50ZgxBJR/v27aOkpIQ9e/akuimS5Ro1akSbNm1o0KBB3NsoIERCVFJSQtOmTWnfvj1mlurmSJZyd7Zu3UpJSQkdOnSIe7vQTjGZ2RQz+9zM3q+g3MzsPjNbZ2YrzKxnVNlIM1sbvEaG1UaRsO3Zs4djjz1W4SApZWYce+yx1e7JhtmDmAo8ADxaQfn5QMfgdQbwP8AZZtYCGAfkAw4sM7NZ7v5laC0tahb1fkdou5HspHCQdFCTn8PQehDuvhDYVkmVi4FHPWIxcIyZHQecB8x1921BKMwFBofVzkPCIdayiEiWSuVVTCcAG6OWS4J1Fa0/jJmNMbNiMyvesmVLaA0VyVQbNmyga9euoXz2ggULuPDCCwGYNWsWEydODGU/kjoZPUjt7pOByQD5+fl68pFIihQWFlJYWJjqZkiCpbIHsQloG7XcJlhX0fpwlB9z0BiE1DH79+/niiuuoHPnzgwZMoTdu3czfvx4Tj/9dLp27cqYMWMofbLkfffdR25uLnl5eQwbNgyAr776itGjR1NQUECPHj3461//etg+pk6dytixYwEYNWoUN9xwA2eeeSYnnXQSM2fOLKt31113cfrpp5OXl8e4ceOScPRSG6nsQcwCxprZU0QGqXe4+2YzexX4nZk1D+oNAm4OtSUDxsG8W+E3n4W6G5HL//TWYesuzDuOK3u35+u9Bxj1yJLDyoec1oah+W3Z9tVernts2SFlM67pXeU+16xZw8MPP0yfPn0YPXo0f/zjHxk7diy33HILAFdeeSUvvPACF110ERMnTuSjjz7iiCOOYPv27QBMmDCB/v37M2XKFLZv305BQQHnnntupfvcvHkzixYt4oMPPqCwsJAhQ4YwZ84c1q5dy5IlS3B3CgsLWbhwIeecc06VxyCpEeZlrk8CbwEnm1mJmV1tZtea2bVBlZeA9cA64CHgegB33wb8FlgavMYH68Iz79bIvxO+G+puRFKhbdu29OnTB4ARI0awaNEiXn/9dc444wy6devG/PnzWblyJQB5eXlcccUVPPbYY9SvH/n7cc6cOUycOJHu3bvTr18/9uzZwyeffFLpPn/4wx9Sr149cnNz+eyzz8o+Z86cOfTo0YOePXvywQcfsHbt2hCPXGortB6Euw+votyBn1ZQNgWYEka7DlN0TPSeI8tF25Oya8k+lf3Ff2TDnErLWzRuGFePobzylzeaGddffz3FxcW0bduWoqKisuvjX3zxRRYuXMjs2bOZMGEC7733Hu7Os88+y8knn3zI55T+4o/liCOOKHtfevrK3bn55pu55pprqn0Mkhqai4nyY9sa65a65ZNPPuGttyKntp544gnOOussAFq2bMmuXbvKxggOHjzIxo0b+f73v88dd9zBjh072LVrF+eddx73339/2S/6d955p0btOO+885gyZQq7du0CYNOmTXz++ee1PTwJUUZfxSQiVTv55JOZNGkSo0ePJjc3l+uuu44vv/ySrl278t3vfpfTTz8dgAMHDjBixAh27NiBu3PDDTdwzDHH8F//9V/8/Oc/Jy8vj4MHD9KhQwdeeOGFardj0KBBrF69mt69I72gJk2a8Nhjj/Gd73wnoccriWOlfxVkuvz8fK/RA4Ni3RinK5kkQVavXk3nzp1T3QwRIPbPo5ktc/f8WPV1iilWGOhuahERBYTCQEQkNgWEiIjEpIAQEZGYFBCNmlddR0QkCykgbtoQe31RM41PiEhWU0BURSEhGS4nJ4fu3bvTtWtXLrroorI5lirSr18/anTJeGDDhg088cQTNd4+Edq3b88XX3xR6zq1UZuv44IFC/jf//3fsuUHH3yQRx+t6Nlr4VFAxGvjkm97FepdSAY58sgjWb58Oe+//z4tWrRg0qRJoe1r//79aREQma58QFx77bVcddVVSW+HAiIeRc3g4YGx1yssJNE2LoE37on8m2C9e/dm06bI7PnLly+nV69e5OXlcckll/Dll98+1Xf69OllvY4lSyLtqGja76lTp1JYWEj//v0ZMGAAN910E2+88Qbdu3fn3nvvZcOGDZx99tn07NmTnj17HvKLr9SGDRs45ZRTGDVqFJ06deKKK67gtddeo0+fPnTs2LGsDdu2beOHP/wheXl59OrVixUrVgCwdetWBg0aRJcuXfi3f/s3om8AfuyxxygoKKB79+5cc801HDhwoNKv0Zw5c+jduzc9e/Zk6NCh7Nq1i1deeYWhQ4eW1Yl+WNJ1111Hfn4+Xbp0qXAK8yZNmpS9nzlzJqNGjQJg9uzZnHHGGfTo0YNzzz2Xzz77jA0bNvDggw9y77330r17d9544w2Kioq4++67K/2+9evXj1/96lcUFBTQqVMn3njjjUqPMx6aagPg+NPg02VV16tMaUjoLmypyMs3wf97r/I63+yEz94HPwhWD1p3hSOOrrj+d7vB+fE9ye3AgQPMmzePq6++GoCrrrqK+++/n759+3LLLbdw66238oc//AGA3bt3s3z5chYuXMjo0aN5//33K532++2332bFihW0aNGCBQsWcPfdd5dNx7F7927mzp1Lo0aNWLt2LcOHD4956mXdunU888wzTJkyhdNPP50nnniCRYsWMWvWLH73u9/x/PPPM27cOHr06MHzzz/P/Pnzueqqq1i+fDm33norZ511FrfccgsvvvgiDz/8MBC5c3jGjBm8+eabNGjQgOuvv57HH3+8wr/Gv/jiC2677TZee+01GjduzB133MHvf/97fv3rXzNmzBi++uorGjduzIwZM8qelzFhwgRatGjBgQMHGDBgACtWrCAvLy+u78lZZ53F4sWLMTP+/Oc/c+edd3LPPfdw7bXX0qRJE2688UYA5s2bV7ZNZd+3/fv3s2TJEl566SVuvfVWXnvttbjaUREFBMCY+YnrBUR/jsJCqmvPjkg4QOTfPTsqD4g4fP3113Tv3p1NmzbRuXNnBg4cyI4dO9i+fTt9+/YFYOTIkYf8hTx8eGQy5nPOOYedO3eyfft25syZw6xZs8r+ko2e9nvgwIG0aNEi5v737dvH2LFjWb58OTk5OXz44Ycx63Xo0IFu3boB0KVLFwYMGICZ0a1bNzZs2ADAokWLePbZZwHo378/W7duZefOnSxcuJDnnnsOgB/84Ac0bx65OnHevHksW7asbL6pr7/+utK5nxYvXsyqVavKpkffu3cvvXv3pn79+gwePJjZs2czZMgQXnzxRe68804Ann76aSZPnsz+/fvZvHkzq1atijsgSkpKuPzyy9m8eTN79+6lQ4cOldav6vt26aWXAnDaaaeVfc1qQwERpopCR8GRneL5S3/jEphWCAf2Qk5DuOzP0LagVrstHYPYvXs35513HpMmTWLkyJGVbhNrivCKpv3++9//TuPGjSv8rHvvvZfWrVvz7rvvcvDgQRo1ahSzXvQU4fXq1StbrlevHvv376+0vRVxd0aOHMntt98ed/2BAwfy5JNPHlY2bNgwHnjgAVq0aEF+fj5Nmzblo48+4u6772bp0qU0b96cUaNGlU2dHi366xld/rOf/Yxf/OIXFBYWsmDBAoqKiqp/kFFKv2Y5OTk1/ppF0xhEKmjcQirStgBGzoL+v4n8W8twiHbUUUdx3333cc8999C4cWOaN29edp56+vTpZX+VAsyYMQOI/MXerFkzmjVrFve0302bNuWf//xn2fKOHTs47rjjqFevHtOnT69yDKAyZ599No8//jgQGQdo2bIlRx99NOecc07ZwPjLL79cdl5+wIABzJw5s2xa8W3btvHxxx9X+Pm9evXizTffZN26dUBk3KW0x9O3b1/efvttHnroobLTSzt37qRx48Y0a9aMzz77jJdffjnm57Zu3ZrVq1dz8OBB/vKXvxzytTnhhBMAmDZtWtn68l/DUs2aNav0+5Zo6kGkknoYEkvbgoQGQ7QePXqQl5fHk08+ybRp07j22mvZvXs3J510Eo888khZvUaNGtGjRw/27dvHlCmRZ3fFO+13Xl4eOTk5nHrqqYwaNYrrr7+eyy67jEcffZTBgwdX2tuoSlFREaNHjyYvL4+jjjqq7JfquHHjGD58OF26dOHMM8/kxBNPBCA3N5fbbruNQYMGcfDgQRo0aMCkSZNo165dzM9v1aoVU6dOZfjw4XzzzTcA3HbbbXTq1ImcnBwuvPBCpk6dWrbfU089lR49enDKKacc8uS+8iZOnMiFF15Iq1atyM/PL3smRlFREUOHDqV58+b079+fjz76CICLLrqIIUOG8Ne//pX777//kM+q7PuWaJruu9RdneCrSp5JHf1LO2l//ZuebpfhNN23pJPqTvetHkSpX374bUg0bn1oWJT/i76yv/ATGh6uQW8RSRkFRLRfxr66olpKf4mH0cvQw41EJIkUEGGJ9Yv7gQL4Yk2C99NMIZHm3P2wq4JEkq0mwwmhXsVkZoPNbI2ZrTOzm2KUtzOzeWa2wswWmFmbqLI7zWylma02s/usLvwPG7sknF/muioqbTVq1IitW7fW6D+nSKK4O1u3bq3wEuOKhNaDMLMcYBIwECgBlprZLHdfFVXtbuBRd59mZv2B24ErzexMoA9QerfJIqAvsCCs9iZVRSFR21/yups77bRp04aSkhK2bNmS6qZIlmvUqBFt2rSpumKUME8xFQDr3H09gJk9BVwMRAdELvCL4P3rwPPBewcaAQ0BAxoAlVxiVEck6vnY5bdRYKRMgwYNqrw7ViRdhRkQJwAbo5ZLgDPK1XkXuBT4b+ASoKmZHevub5nZ68BmIgHxgLuvLr8DMxsDjAHKrnuucxJxea2uhBKRGkj1IPWNwANmNgpYCGwCDpjZvwCdgdL+0FwzO9vdD5me0N0nA5Mhch9E0lqdKokOi8o+X0SyXpgBsQloG7XcJlhXxt0/JdKDwMyaAJe5+3Yz+wmw2N13BWUvA72B2s9fW1eEcTmtehoiEiXMq5iWAh3NrIOZNQSGAbOiK5hZSzMrbcPNwJTg/SdAXzOrb2YNiAxQH3aKSYj8IteVUSISgtB6EO6+38zGAq8COcAUd19pZuOBYnefBfQDbjczJ3KK6afB5jOB/sB7RAasX3H32WG1tU447G7vEKYvj7UfEamzNBdTNkh0T0AhIVJnaC6mbJfo+y40ViGSFRQQ2SwRp6UUFiJ1lh4YJN+q7YC3BrZF6hT1IORwte1ZlNbv9iO47KHEtElEkk49CKlaTXsV7z2tXoVIBlMPQuJT27u4NVYhknEUEFJ9tb2LW2EhkhF0iklqrnRQu7YD2yKSltSDkMSoTa9Cd2uLpCUFhCRWImecVVCIpJQCQsKTqIFtBYVISiggJDlqExYa1BZJCQ1SS/Lpbm2RjKAehKRGou7WVo9CJDTqQUh6qOnlsupRiIRGASHpRUEhkjZ0iknSU2lIPPuTyJxOcW+neypEEkVPlJPMUdsegsJC5DB6opzUDZoDSiSpNAYhmae28z+BxitE4qCAkMylJ+CJhCrUU0xmNhj4byAH+LO7TyxX3g6YArQCtgEj3L0kKDsR+DPQFnDgAnffEGZ7JUPpngqRUIQ2SG1mOcCHwECgBFgKDHf3VVF1ngFecPdpZtYf+LG7XxmULQAmuPtcM2sCHHT33RXtT4PUElON5oBSUEj2qGyQOsxTTAXAOndf7+57gaeAi8vVyQXmB+9fLy03s1ygvrvPBXD3XZWFg0iFanIaSqeeRIBwA+IEYGPUckmwLtq7wKXB+0uApmZ2LNAJ2G5mz5nZO2Z2V9AjOYSZjTGzYjMr3rJlSwiHIHWGgkKk2lJ9meuNwANmNgpYCGwCDhBp19lAD+ATYAYwCng4emN3nwxMhsgppmQ1WjJYTS6V1c13kqXCDIhNRAaYS7UJ1pVx908JehDBOMNl7r7dzEqA5e6+Pih7HuhFuYAQqbFEPQFPYSF1WJinmJYCHc2sg5k1BIYBs6IrmFlLMyttw81Ermgq3fYYM2sVLPcHViGSaLpUVqRCoQWEu+8HxgKvAquBp919pZmNN7PCoFo/YI2ZfQi0BiYE2x4gcvppnpm9BxjwUFhtFUlIUIjUMZqLSSQWzfskWUJzMYlUl26+E1FAiMSlps/UVlBIBtNcTCLVpXsqJEsoIERqSk++kzpOp5hEakOnnqQOU0CIJEpt79JWWEia0SkmkUSr6T0VOv0kaUYBIRIWBYVkOJ1iEglbTed90uknSTH1IESSpTbTeahHISmgHoRIsunKJ8kQ6kGIpJJuupM0poAQSQc1DYoHCsJpjwg6xSSSXqp7+umLNTr1JKGJqwdhZn3MbK6ZfWhm683sIzNbH3bjRLJa0Q5o2LQa9XXqSRIr3h7Ew8C/A8uIPDNaRJLh1yWRfzWYLSkQb0DscPeXQ22JiFSsNtN4KCikhuINiNfN7C7gOeCb0pXu/nYorRKR2BQUkkTxBsQZwb/Rj6VzoH9imyMicVFQSBLEFRDu/v2wGyIiNaAZZCVE8V7F1MzMfm9mxcHrHjOr8ifSzAab2RozW2dmN8Uob2dm88xshZktMLM25cqPNrMSM3sg/kMSyUK1mRhQpALx3ig3Bfgn8KPgtRN4pLINzCwHmAScD+QCw80st1y1u4FH3T0PGA/cXq78t8DCONsoIrozWxIo3oD4nruPc/f1wetW4KQqtikA1gX19wJPAReXq5MLzA/evx5dbmanAa2BOXG2UURKKSgkAeINiK/N7KzSBTPrA3xdxTYnABujlkuCddHeBS4N3l8CNDWzY82sHnAPcGOc7RORWGoaFCLEfxXTdcC0YNzBgG3AqATs/0bgATMbReRU0iYiN+JdD7zk7iVmVuHGZjYGGANw4oknJqA5InVUdafw0BVPApi7x1/Z7GgAd98ZR93eQJG7nxcs3xxsW36cobR+E+ADd29jZo8DZwMHgSZAQ+CP7n7YQHep/Px8Ly4ujvtYRLJatR9epKCoq8xsmbvnxyqrtAdhZiPc/TEz+0W59QC4++8r2Xwp0NHMOhDpGQwD/rXc57QEtrn7QeBmIoPhuPsVUXVGAfmVhYOIVFN1L49VjyIrVTUG0Tj4t2kFrwq5+35gLPAqsBp42t1Xmtl4MysMqvUD1pjZh0QGpCfU5CBEpIaqO0ah8YmsUq1TTOlMp5hEakmnnbJSZaeY4r1R7s7gprUGwY1tW8xsRGKbKSIpVZPeRFEzuKtTeG2SlIr3MtdBwcD0hcAG4F+AX4bVKBFJoeoGxVef6dRTHRVvQJQOZv8AeMbd1bcUqetq0qP47XfCa48kXbwB8YKZfQCcBswzs1bAnvCaJSJpozohceAb9SbqkLgCIrjE9Ewil5vuA77i8GkzRKSu0tVOWamq+yD6u/t8M7s0al10lefCapiIpKHq3D+hacUzXlVTbfQlMpneRTHKHAWESHaqyY12ComMo/sgRKT2qvXAIgVFOknEfRC/M7Njopabm9ltiWqgiGS4mtw/IWkv3quYznf37aUL7v4lcEE4TRKRjKTnT9Q58U73nWNmR7j7NwBmdiRwRHjNEpGMVN1pxaPr6dRT2om3B/E4kfsfrjazq4G5wLTwmiUiGU8PKsp4cQ9Sm9lg4Nxgca67vxpaq2pAg9QiaUwTAaatGj8PopzVwH53f83MjjKzpu7+z8Q0UUTqNF0Wm5HivYrpJ8BM4E/BqhOA58NqlIjUUdW5I1uD2CkX7xjET4E+wE4Ad18LaFYuEamZ6gaFpES8AfGNu+8tXTCz+kTupBYRqbnqhIRmik26eAPib2b2a+BIMxsIPAPMDq9ZIpI14u1NaKbYpIs3IH4FbAHeA64BXgL+M6xGiUgWqk5vQk+xS4oqr2IysxxgpbufAjwUfpNEJGvFe7VT6VPsdKVTqKrsQbj7AWCNmZ2YhPaIiGgAO03Ee4qpObDSzOaZ2azSV1UbmdlgM1tjZuvM7KYY5e2Cz1xhZgvMrE2wvruZvWVmK4Oyy6t3WCKS8eIdm9DlsKGJ605qM+sba727/62SbXKAD4GBQAmwFBju7qui6jwDvODu08ysP/Bjd7/SzDpFPt7XmtnxwDKgc/SEgeXpTmqROizuG+x0yqm6ajzdt5k1MrOfA0OBU4A33f1vpa8q9lsArHP39cElsk9x+GNKc4k8kAjg9dJyd/8wuNcCd/8U+BxoVcX+RKSuqk5vQhKmqlNM04B8IlcvnQ/cU43PPgHYGLVcEqyL9i5Q+jjTS4CmZnZsdAUzKwAaAv8ovwMzG2NmxWZWvGXLlmo0TUQykk45JVVVAZHr7iPc/U/AEODsBO//RqCvmb1D5PGmm4ADpYVmdhwwncipp4PlN3b3ye6e7+75rVqpgyGSFTRVR9JUFRD7St+4+/5qfvYmoG3UcptgXRl3/9TdL3X3HsBvgnXbAczsaOBF4Dfuvria+xaRukxTdSRFVQFxqpntDF7/BPJK35vZziq2XQp0NLMOZtYQGAYccuWTmbU0s9I23AxMCdY3BP4CPOruM6t7UCKSJdSbCFWlAeHuOe5+dPBq6u71o94fXcW2+4GxwKtEpgp/2t1Xmtl4MysMqvUjco/Fh0BrYEKw/kfAOcAoM1sevLrX/DBFpM6q7vOwJW5xPzAo3ekyVxHR5bDVV+PLXEVEMoouh00oBYSI1D26HDYhFBAiUjepN1FrCggRqdsUEjWmgBCRuk8hUSMKCBHJDgqJalNAiEj2iGdcQiFRRgEhItlHIREXBYSIZKd4QiLLg0IBISLZS+MSlVJAiEh2U0hUSAEhIqKQiEkBISIC1Zie45jw25ImFBAiIqXimp7Ds6Y3oYAQESkvnt7ExiXhtyPFFBAiIrFUFRIPD0xOO1JIASEiUpEsv6FOASEiUpmiHXD13ErK625IKCBERKrStqDy8qJmdXJMQgEhIhKPeMYkHr0kOW1JEgWEiEi8qgqJ9fPr1CmnUAPCzAab2RozW2dmN8Uob2dm88xshZktMLM2UWUjzWxt8BoZZjtFROKWRXddhxYQZpYDTALOB3KB4WaWW67a3cCj7p4HjAduD7ZtAYwDzgAKgHFm1jystoqIVEuWhESYPYgCYJ27r3f3vcBTwMXl6uQC84P3r0eVnwfMdfdt7v4lMBcYHGJbRUSqJ667rjNbmAFxArAxarkkWBftXeDS4P0lQFMzOzbObTGzMWZWbGbFW7ZsSVjDRUTiVllIZHgvItWD1DcCfc3sHaAvsAk4EO/G7j7Z3fPdPb9Vq1ZhtVFEpHJ1NCTCDIhNQNuo5TbBujLu/qm7X+ruPYDfBOu2x7OtiEjGyNCQCDMglgIdzayDmTUEhgGzoiuYWUszK23DzcCU4P2rwCAzax4MTg8K1omIpKc6OC1HaAHh7vuBsUR+sa8Gnnb3lWY23swKg2r9gDVm9iHQGpgQbLsN+C2RkFkKjA/WiYikrzo2aG3unuo2JER+fr4XFxenuhkiIpX3FtIsRMxsmbvnxypL9SC1iEjdU0cGrRUQIiLJliEhoYAQEQlDHRi0VkCIiIQlzcYbqksBISISpgwej1BAiIiELUNDQgEhIpJqaRoSCggRkWTIwPEIBYSISLJk2KkmBYSISDJlUE9CASEiki7SrBehgBARSbYMOdWkgBARSTdpEhIKCBGRVMiAqTgUECIiqZLmA9YKCBGRVErj8QgFhIhIqqVpSCggREQkJgWEiEg6SMNehAJCRCRdpFlIKCBERCSmUAPCzAab2RozW2dmN8UoP9HMXjezd8xshZldEKxvYGbTzOw9M1ttZjeH2U4RkbSRRr2I0ALCzHKAScD5QC4w3Mxyy1X7T+Bpd+8BDAP+GKwfChzh7t2A04BrzKx9WG0VEUkraXJ/RJg9iAJgnbuvd/e9wFPAxeXqOHB08L4Z8GnU+sZmVh84EtgL7AyxrSIimSGJvYgwA+IEYGPUckmwLloRMMLMSoCXgJ8F62cCXwGbgU+Au919W/kdmNkYMys2s+ItW7YkuPkiIimUBr2IVA9SDwemunsb4AJgupnVI9L7OAAcD3QA/sPMTiq/sbtPdvd8d89v1apVMtstIpI6SepFhBkQm4C2UcttgnXRrgaeBnD3t4BGQEvgX4FX3H2fu38OvAnkh9hWEZH0k+JeRJgBsRToaGYdzKwhkUHoWeXqfAIMADCzzkQCYkuwvn+wvjHQC/ggxLaKiGSWJPQiQgsId98PjAVeBVYTuVpppZmNN7PCoNp/AD8xs3eBJ4FR7u5Ern5qYmYriQTNI+6+Iqy2ioikrRRe9lo/zA9395eIDD5Hr7sl6v0qoE+M7XYRudRVRERO6g/r5yd9t6kepBYRkapc9ZeU7FYBISKSCSo61RTiaSYFhIiIxKSAEBHJdCH1IhQQIiKZIsn3RSggREQkJgWEiEgmSeJgtQJCRERiUkCIiEhMCggRkUyTpNNMCggREYlJASEiIjGFOllfJrn8T28dtu7CvOO4snd7vt57gFGPLDmsfMhpbRia35ZtX+3luseWHVY+olc7Ljr1eD7d/jX/PmP5YeU/Ofskzs1tzT+27OLXz713WPnP+nfkrI4tWfnpDsbPXnVY+f8dfDKntWvBso+3cecraw4rv+WiXLoc34xFa7/g/vlrDyv/3aXd+F6rJry26jMeemP9YeX3Xt6d4485ktnvfspjiz8+rPx/RpxGi8YNeaZ4IzOXlRxWPvXHBaE+VYEAAAaBSURBVBzZMIfpb23ghRWbDyufcU1vACYv/AfzVn9+SFmjBjlMG10AwH3z1vLmui8OKW9+VEMevPI0AO545QPe/vjLQ8qPa9aIPwzrAcCts1ey6tNDn1h7UqvG3H5pHgA3P7eC9Vu+OqQ89/ijGXdRFwB+/tQ7bN6x55Dynu2a86vBpwBw7fRlfLl77yHlff6lJTcM6AjAyClL2LPvwCHlAzp/hzHnfA/Qz55+9mr4s1e0g4NFzUL9K189CBGRDOYhfrZFHr+Q+fLz8724uDjVzRARSZ5Yg9LVvNvazJa5e8wndqoHISKSqcqHQYKn4tAYhIhIJgtxfib1IEREJCYFhIiIxKSAEBGRmBQQIiISkwJCRERiUkCIiEhMdeZGOTPbAhx+T378WgJfVFmrbsm2Y8624wUdc7aozTG3c/dWsQrqTEDUlpkVV3Q3YV2VbcecbccLOuZsEdYx6xSTiIjEpIAQEZGYFBDfmpzqBqRAth1zth0v6JizRSjHrDEIERGJST0IERGJSQEhIiIxZVVAmNlgM1tjZuvM7KYY5UeY2Yyg/O9m1j75rUysOI75F2a2ysxWmNk8M2uXinYmUlXHHFXvMjNzM8v4SyLjOWYz+1HwvV5pZk8ku42JFsfP9olm9rqZvRP8fF+QinYmiplNMbPPzez9CsrNzO4Lvh4rzKxnrXfq7lnxAnKAfwAnAQ2Bd4HccnWuBx4M3g8DZqS63Uk45u8DRwXvr8uGYw7qNQUWAouB/FS3Ownf547AO0DzYPk7qW53Eo55MnBd8D4X2JDqdtfymM8BegLvV1B+AfAyYEAv4O+13Wc29SAKgHXuvt7d9wJPAReXq3MxMC14PxMYYGaWxDYmWpXH7O6vu/vuYHEx0CbJbUy0eL7PAL8F7gD2xCjLNPEc80+ASe7+JYC7f57kNiZaPMfswNHB+2bAp0lsX8K5+0JgWyVVLgYe9YjFwDFmdlxt9plNAXECsDFquSRYF7OOu+8HdgDHJqV14YjnmKNdTeQvkExW5TEHXe+27v5iMhsWoni+z52ATmb2ppktNrPBSWtdOOI55iJghJmVAC8BP0tO01Kmuv/fq6RHjgoAZjYCyAf6protYTKzesDvgVEpbkqy1SdymqkfkV7iQjPr5u7bU9qqcA0Hprr7PWbWG5huZl3d/WCqG5YpsqkHsQloG7XcJlgXs46Z1SfSLd2alNaFI55jxszOBX4DFLr7N0lqW1iqOuamQFdggZltIHKudlaGD1TH830uAWa5+z53/wj4kEhgZKp4jvlq4GkAd38LaERkUru6Kq7/79WRTQGxFOhoZh3MrCGRQehZ5erMAkYG74cA8z0Y/clQVR6zmfUA/kQkHDL9vDRUcczuvsPdW7p7e3dvT2TcpdDdi1PT3ISI52f7eSK9B8ysJZFTTuuT2cgEi+eYPwEGAJhZZyIBsSWprUyuWcBVwdVMvYAd7r65Nh+YNaeY3H2/mY0FXiVyBcQUd19pZuOBYnefBTxMpBu6jshg0LDUtbj24jzmu4AmwDPBePwn7l6YskbXUpzHXKfEecyvAoPMbBVwAPilu2ds7zjOY/4P4CEz+3ciA9ajMvkPPjN7kkjItwzGVcYBDQDc/UEi4ywXAOuA3cCPa73PDP56iYhIiLLpFJOIiFSDAkJERGJSQIiISEwKCBERiUkBISIiMSkgRKrBzA6Y2XIze9/MZpvZMQn+/A3BfQqY2a5EfrZIdSkgRKrna3fv7u5didwr89NUN0gkLAoIkZp7i2AyNDP7npm9YmbLzOwNMzslWN/azP5iZu8GrzOD9c8HdVea2ZgUHoNIhbLmTmqRRDKzHCLTODwcrJoMXOvua83sDOCPQH/gPuBv7n5JsE2ToP5od99mZkcCS83s2Uy+s1nqJgWESPUcaWbLifQcVgNzzawJcCbfTlcCcETwb3/gKgB3P0BkCnmAG8zskuB9WyIT5ykgJK0oIESq52t3725mRxGZB+inwFRgu7t3j+cDzKwfcC7Q2913m9kCIhPJiaQVjUGI1EDwFL4biEwItxv4yMyGQtmzgU8Nqs4j8ihXzCzHzJoRmUb+yyAcTiEy5bhI2lFAiNSQu78DrCDyYJorgKvN7F1gJd8+/vL/AN83s/eAZUSejfwKUN/MVgMTiUw5LpJ2NJuriIjEpB6EiIjEpIAQEZGYFBAiIhKTAkJERGJSQIiISEwKCBERiUkBISIiMf1/uOYWBMonPSsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Validation Loss: 0.54\n",
            "  Validation took: 0:14:18\n",
            "\n",
            "Training complete!\n",
            "Total training took 9:45:18 (h:mm:ss)\n"
          ]
        }
      ],
      "source": [
        " #Using pos weight of loss_fn\n",
        "from sklearn.metrics import classification_report,confusion_matrix,f1_score,auc,precision_recall_curve,plot_precision_recall_curve,matthews_corrcoef\n",
        "import random\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "scaler = GradScaler()\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    roberta_model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        roberta_model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        with autocast():\n",
        "            loss, logits = roberta_model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "            b_labels1=torch.nn.functional.one_hot(b_labels, num_classes=2)\n",
        "            #Calculating weights\n",
        "            #positive=torch.sum(b_labels1, dim=0)\n",
        "           # negative=len(b_labels1)-positive\n",
        "            #negative\n",
        "            #pos_weight  = positive / negative\n",
        "            #criterion.pos_weight = pos_weight\n",
        "            loss1 = loss_fn1(logits,b_labels1).to(device)\n",
        "           # print(\"loss:\",loss1)\n",
        "            loss1 = loss1 / gradient_accumulations\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss1.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        scaler.scale(loss1).backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "       # torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm*scaler.get_scale())\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        if ((step + 1) % gradient_accumulations == 0):\n",
        "             scaler.step(optimizer)\n",
        "       # Updates the scale for next iteration.\n",
        "             scaler.update()\n",
        "        # Update the learning rate.\n",
        "             scheduler.step()       \n",
        "             optimizer.zero_grad()\n",
        "            # roberta_model.zero_grad()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    roberta_model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    total_f1_score =0\n",
        "    predlist =[]\n",
        "    lbllist =[]\n",
        "    total_logits =[]\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            (loss, logits) = roberta_model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            #Converting the labels to one hot to sync with same shape as logits\n",
        "            b_labels1=torch.nn.functional.one_hot(b_labels, num_classes=2)\n",
        "            loss1 = loss_fn(logits, b_labels1)\n",
        "       # print(\"loss1:\",loss1)\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss1.item()\n",
        "\n",
        "         #Converting for predictions by applying sigmoid to logits\n",
        "        pred_logits_sigmoid=torch.sigmoid(logits)\n",
        "        y_pred=torch.round(pred_logits_sigmoid)\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits_pred = y_pred.detach().cpu().numpy()\n",
        "        label_ids1 = b_labels.to('cpu').numpy()\n",
        "        logits=logits.detach().cpu().numpy()\n",
        "        #For confusion matrix and classification report to work we need same dimensions.\n",
        "        label_ids = b_labels1.to('cpu').numpy()\n",
        "        pred_logits_sigmoid = pred_logits_sigmoid.detach().cpu().numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids1)\n",
        "\n",
        "         #print(predictions)\n",
        "        predictions=np.argmax(logits_pred, axis=1)\n",
        "        y_test=np.argmax(label_ids,axis=1)\n",
        "        predlist.extend(predictions)\n",
        "        lbllist.extend(y_test)\n",
        "        #Accumulating the sigmoid positive logits for precision recall curve\n",
        "        total_logits.extend(pred_logits_sigmoid[:,1])\n",
        "        total_f1_score += f1_score(predictions,y_test, average = 'macro')\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    #f1 score\n",
        "\n",
        "    avg_f1_score =total_f1_score/len(validation_dataloader)\n",
        "    print(\"  F1 score: {0:.2f}\".format(avg_f1_score))\n",
        "\n",
        "     #classification report\n",
        "    print(classification_report(lbllist,predlist))  \n",
        "\n",
        "    #confusion matrix\n",
        "    cm = confusion_matrix(lbllist,predlist)\n",
        "    # constant for classes\n",
        "    print(cm)\n",
        "    #mcc score\n",
        "    print(\"Mathews Correlation coefficient score for this epoch is :\",round(matthews_corrcoef(lbllist, predlist),2))\n",
        "    #Precision recall curve plot\n",
        "    lr_precision, lr_recall, thresholds = precision_recall_curve(lbllist,total_logits)\n",
        "    lr_f1, lr_auc = f1_score( lbllist,predlist), auc(lr_recall, lr_precision)\n",
        "    print('Model validation score: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
        "    baseline = lbllist.count(1) / len(lbllist)\n",
        "    plt.plot([0, 1], [baseline, baseline], linestyle='--', label='baseline')\n",
        "    plt.plot(lr_recall, lr_precision, marker='.', label='Roberta model evaluation')\n",
        "    # axis labels\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    # show the legend\n",
        "    plt.legend()\n",
        "    # show the plot\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Gg2WU5bG01h",
        "outputId": "77cf1e94-c87c-4a65-af64-79ee6d2be01d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classes=np.unique(y_train)\n",
        "y_train[y_train==0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bkFUZgfsLL9",
        "outputId": "3c0d2f75-9f72-4477-e982-033150bdc1c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3.7842, 0.5761], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "#class weight computation method3\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "#compute the class weights\n",
        "y_train_indices = train_dataset.indices\n",
        "\n",
        "y_train = [target[i] for i in y_train_indices]\n",
        "\n",
        "class_wts = compute_class_weight(\"balanced\",classes= np.unique(y_train),y=y_train)\n",
        "#class_wts = dict(zip(np.unique(labels),class_wts))\n",
        "weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyL9rqq1GlER",
        "outputId": "c832edfb-c547-4b76-af1b-af2f0cfde09f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([     0,      1,      2, ..., 608455, 608456, 608457])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "np.unique(train_dataset.indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GctZMMYh3Brs"
      },
      "outputs": [],
      "source": [
        "#Tried to use cross entropy with computed weights for imbalance check\n",
        "weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "# loss function\n",
        "cross_entropy = torch.nn.CrossEntropyLoss(weight=weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iged2rKuSBf"
      },
      "outputs": [],
      "source": [
        "#For validation data loader\n",
        "cross_entropy1 =torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "p297s4FquSin",
        "outputId": "10871af3-aab4-4aad-e165-052bdb22ff32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 8 ========\n",
            "Training...\n",
            "  Batch    40  of  7,606.    Elapsed: 0:00:57.\n",
            "  Batch    80  of  7,606.    Elapsed: 0:01:54.\n",
            "  Batch   120  of  7,606.    Elapsed: 0:02:51.\n",
            "  Batch   160  of  7,606.    Elapsed: 0:03:48.\n",
            "  Batch   200  of  7,606.    Elapsed: 0:04:45.\n",
            "  Batch   240  of  7,606.    Elapsed: 0:05:42.\n",
            "  Batch   280  of  7,606.    Elapsed: 0:06:39.\n",
            "  Batch   320  of  7,606.    Elapsed: 0:07:36.\n",
            "  Batch   360  of  7,606.    Elapsed: 0:08:33.\n",
            "  Batch   400  of  7,606.    Elapsed: 0:09:30.\n",
            "  Batch   440  of  7,606.    Elapsed: 0:10:27.\n",
            "  Batch   480  of  7,606.    Elapsed: 0:11:24.\n",
            "  Batch   520  of  7,606.    Elapsed: 0:12:21.\n",
            "  Batch   560  of  7,606.    Elapsed: 0:13:18.\n",
            "  Batch   600  of  7,606.    Elapsed: 0:14:15.\n",
            "  Batch   640  of  7,606.    Elapsed: 0:15:12.\n",
            "  Batch   680  of  7,606.    Elapsed: 0:16:09.\n",
            "  Batch   720  of  7,606.    Elapsed: 0:17:06.\n",
            "  Batch   760  of  7,606.    Elapsed: 0:18:03.\n",
            "  Batch   800  of  7,606.    Elapsed: 0:19:00.\n",
            "  Batch   840  of  7,606.    Elapsed: 0:19:57.\n",
            "  Batch   880  of  7,606.    Elapsed: 0:20:54.\n",
            "  Batch   920  of  7,606.    Elapsed: 0:21:51.\n",
            "  Batch   960  of  7,606.    Elapsed: 0:22:48.\n",
            "  Batch 1,000  of  7,606.    Elapsed: 0:23:45.\n",
            "  Batch 1,040  of  7,606.    Elapsed: 0:24:42.\n",
            "  Batch 1,080  of  7,606.    Elapsed: 0:25:39.\n",
            "  Batch 1,120  of  7,606.    Elapsed: 0:26:36.\n",
            "  Batch 1,160  of  7,606.    Elapsed: 0:27:33.\n",
            "  Batch 1,200  of  7,606.    Elapsed: 0:28:30.\n",
            "  Batch 1,240  of  7,606.    Elapsed: 0:29:27.\n",
            "  Batch 1,280  of  7,606.    Elapsed: 0:30:24.\n",
            "  Batch 1,320  of  7,606.    Elapsed: 0:31:21.\n",
            "  Batch 1,360  of  7,606.    Elapsed: 0:32:18.\n",
            "  Batch 1,400  of  7,606.    Elapsed: 0:33:15.\n",
            "  Batch 1,440  of  7,606.    Elapsed: 0:34:12.\n",
            "  Batch 1,480  of  7,606.    Elapsed: 0:35:09.\n",
            "  Batch 1,520  of  7,606.    Elapsed: 0:36:06.\n",
            "  Batch 1,560  of  7,606.    Elapsed: 0:37:03.\n",
            "  Batch 1,600  of  7,606.    Elapsed: 0:38:00.\n",
            "  Batch 1,640  of  7,606.    Elapsed: 0:38:57.\n",
            "  Batch 1,680  of  7,606.    Elapsed: 0:39:54.\n",
            "  Batch 1,720  of  7,606.    Elapsed: 0:40:51.\n",
            "  Batch 1,760  of  7,606.    Elapsed: 0:41:48.\n",
            "  Batch 1,800  of  7,606.    Elapsed: 0:42:45.\n",
            "  Batch 1,840  of  7,606.    Elapsed: 0:43:42.\n",
            "  Batch 1,880  of  7,606.    Elapsed: 0:44:39.\n",
            "  Batch 1,920  of  7,606.    Elapsed: 0:45:36.\n",
            "  Batch 1,960  of  7,606.    Elapsed: 0:46:33.\n",
            "  Batch 2,000  of  7,606.    Elapsed: 0:47:30.\n",
            "  Batch 2,040  of  7,606.    Elapsed: 0:48:28.\n",
            "  Batch 2,080  of  7,606.    Elapsed: 0:49:25.\n",
            "  Batch 2,120  of  7,606.    Elapsed: 0:50:22.\n",
            "  Batch 2,160  of  7,606.    Elapsed: 0:51:19.\n",
            "  Batch 2,200  of  7,606.    Elapsed: 0:52:16.\n",
            "  Batch 2,240  of  7,606.    Elapsed: 0:53:13.\n",
            "  Batch 2,280  of  7,606.    Elapsed: 0:54:10.\n",
            "  Batch 2,320  of  7,606.    Elapsed: 0:55:07.\n",
            "  Batch 2,360  of  7,606.    Elapsed: 0:56:04.\n",
            "  Batch 2,400  of  7,606.    Elapsed: 0:57:01.\n",
            "  Batch 2,440  of  7,606.    Elapsed: 0:57:58.\n",
            "  Batch 2,480  of  7,606.    Elapsed: 0:58:55.\n",
            "  Batch 2,520  of  7,606.    Elapsed: 0:59:52.\n",
            "  Batch 2,560  of  7,606.    Elapsed: 1:00:49.\n",
            "  Batch 2,600  of  7,606.    Elapsed: 1:01:46.\n",
            "  Batch 2,640  of  7,606.    Elapsed: 1:02:43.\n",
            "  Batch 2,680  of  7,606.    Elapsed: 1:03:40.\n",
            "  Batch 2,720  of  7,606.    Elapsed: 1:04:37.\n",
            "  Batch 2,760  of  7,606.    Elapsed: 1:05:34.\n",
            "  Batch 2,800  of  7,606.    Elapsed: 1:06:31.\n",
            "  Batch 2,840  of  7,606.    Elapsed: 1:07:28.\n",
            "  Batch 2,880  of  7,606.    Elapsed: 1:08:25.\n",
            "  Batch 2,920  of  7,606.    Elapsed: 1:09:22.\n",
            "  Batch 2,960  of  7,606.    Elapsed: 1:10:19.\n",
            "  Batch 3,000  of  7,606.    Elapsed: 1:11:16.\n",
            "  Batch 3,040  of  7,606.    Elapsed: 1:12:13.\n",
            "  Batch 3,080  of  7,606.    Elapsed: 1:13:10.\n",
            "  Batch 3,120  of  7,606.    Elapsed: 1:14:07.\n",
            "  Batch 3,160  of  7,606.    Elapsed: 1:15:04.\n",
            "  Batch 3,200  of  7,606.    Elapsed: 1:16:01.\n",
            "  Batch 3,240  of  7,606.    Elapsed: 1:16:58.\n",
            "  Batch 3,280  of  7,606.    Elapsed: 1:17:55.\n",
            "  Batch 3,320  of  7,606.    Elapsed: 1:18:52.\n",
            "  Batch 3,360  of  7,606.    Elapsed: 1:19:49.\n",
            "  Batch 3,400  of  7,606.    Elapsed: 1:20:46.\n",
            "  Batch 3,440  of  7,606.    Elapsed: 1:21:43.\n",
            "  Batch 3,480  of  7,606.    Elapsed: 1:22:40.\n",
            "  Batch 3,520  of  7,606.    Elapsed: 1:23:37.\n",
            "  Batch 3,560  of  7,606.    Elapsed: 1:24:34.\n",
            "  Batch 3,600  of  7,606.    Elapsed: 1:25:31.\n",
            "  Batch 3,640  of  7,606.    Elapsed: 1:26:28.\n",
            "  Batch 3,680  of  7,606.    Elapsed: 1:27:25.\n",
            "  Batch 3,720  of  7,606.    Elapsed: 1:28:22.\n",
            "  Batch 3,760  of  7,606.    Elapsed: 1:29:19.\n",
            "  Batch 3,800  of  7,606.    Elapsed: 1:30:16.\n",
            "  Batch 3,840  of  7,606.    Elapsed: 1:31:13.\n",
            "  Batch 3,880  of  7,606.    Elapsed: 1:32:10.\n",
            "  Batch 3,920  of  7,606.    Elapsed: 1:33:07.\n",
            "  Batch 3,960  of  7,606.    Elapsed: 1:34:04.\n",
            "  Batch 4,000  of  7,606.    Elapsed: 1:35:01.\n",
            "  Batch 4,040  of  7,606.    Elapsed: 1:35:58.\n",
            "  Batch 4,080  of  7,606.    Elapsed: 1:36:55.\n",
            "  Batch 4,120  of  7,606.    Elapsed: 1:37:52.\n",
            "  Batch 4,160  of  7,606.    Elapsed: 1:38:49.\n",
            "  Batch 4,200  of  7,606.    Elapsed: 1:39:46.\n",
            "  Batch 4,240  of  7,606.    Elapsed: 1:40:43.\n",
            "  Batch 4,280  of  7,606.    Elapsed: 1:41:40.\n",
            "  Batch 4,320  of  7,606.    Elapsed: 1:42:37.\n",
            "  Batch 4,360  of  7,606.    Elapsed: 1:43:34.\n",
            "  Batch 4,400  of  7,606.    Elapsed: 1:44:31.\n",
            "  Batch 4,440  of  7,606.    Elapsed: 1:45:28.\n",
            "  Batch 4,480  of  7,606.    Elapsed: 1:46:25.\n",
            "  Batch 4,520  of  7,606.    Elapsed: 1:47:22.\n",
            "  Batch 4,560  of  7,606.    Elapsed: 1:48:19.\n",
            "  Batch 4,600  of  7,606.    Elapsed: 1:49:16.\n",
            "  Batch 4,640  of  7,606.    Elapsed: 1:50:13.\n",
            "  Batch 4,680  of  7,606.    Elapsed: 1:51:10.\n",
            "  Batch 4,720  of  7,606.    Elapsed: 1:52:07.\n",
            "  Batch 4,760  of  7,606.    Elapsed: 1:53:04.\n",
            "  Batch 4,800  of  7,606.    Elapsed: 1:54:01.\n",
            "  Batch 4,840  of  7,606.    Elapsed: 1:54:58.\n",
            "  Batch 4,880  of  7,606.    Elapsed: 1:55:55.\n",
            "  Batch 4,920  of  7,606.    Elapsed: 1:56:52.\n",
            "  Batch 4,960  of  7,606.    Elapsed: 1:57:49.\n",
            "  Batch 5,000  of  7,606.    Elapsed: 1:58:46.\n",
            "  Batch 5,040  of  7,606.    Elapsed: 1:59:43.\n",
            "  Batch 5,080  of  7,606.    Elapsed: 2:00:40.\n",
            "  Batch 5,120  of  7,606.    Elapsed: 2:01:37.\n",
            "  Batch 5,160  of  7,606.    Elapsed: 2:02:34.\n",
            "  Batch 5,200  of  7,606.    Elapsed: 2:03:31.\n",
            "  Batch 5,240  of  7,606.    Elapsed: 2:04:28.\n",
            "  Batch 5,280  of  7,606.    Elapsed: 2:05:25.\n",
            "  Batch 5,320  of  7,606.    Elapsed: 2:06:22.\n",
            "  Batch 5,360  of  7,606.    Elapsed: 2:07:19.\n",
            "  Batch 5,400  of  7,606.    Elapsed: 2:08:16.\n",
            "  Batch 5,440  of  7,606.    Elapsed: 2:09:13.\n",
            "  Batch 5,480  of  7,606.    Elapsed: 2:10:10.\n",
            "  Batch 5,520  of  7,606.    Elapsed: 2:11:07.\n",
            "  Batch 5,560  of  7,606.    Elapsed: 2:12:04.\n",
            "  Batch 5,600  of  7,606.    Elapsed: 2:13:01.\n",
            "  Batch 5,640  of  7,606.    Elapsed: 2:13:58.\n",
            "  Batch 5,680  of  7,606.    Elapsed: 2:14:55.\n",
            "  Batch 5,720  of  7,606.    Elapsed: 2:15:52.\n",
            "  Batch 5,760  of  7,606.    Elapsed: 2:16:49.\n",
            "  Batch 5,800  of  7,606.    Elapsed: 2:17:46.\n",
            "  Batch 5,840  of  7,606.    Elapsed: 2:18:43.\n",
            "  Batch 5,880  of  7,606.    Elapsed: 2:19:40.\n",
            "  Batch 5,920  of  7,606.    Elapsed: 2:20:37.\n",
            "  Batch 5,960  of  7,606.    Elapsed: 2:21:34.\n",
            "  Batch 6,000  of  7,606.    Elapsed: 2:22:31.\n",
            "  Batch 6,040  of  7,606.    Elapsed: 2:23:28.\n",
            "  Batch 6,080  of  7,606.    Elapsed: 2:24:25.\n",
            "  Batch 6,120  of  7,606.    Elapsed: 2:25:22.\n",
            "  Batch 6,160  of  7,606.    Elapsed: 2:26:19.\n",
            "  Batch 6,200  of  7,606.    Elapsed: 2:27:16.\n",
            "  Batch 6,240  of  7,606.    Elapsed: 2:28:13.\n",
            "  Batch 6,280  of  7,606.    Elapsed: 2:29:10.\n",
            "  Batch 6,320  of  7,606.    Elapsed: 2:30:07.\n",
            "  Batch 6,360  of  7,606.    Elapsed: 2:31:04.\n",
            "  Batch 6,400  of  7,606.    Elapsed: 2:32:01.\n",
            "  Batch 6,440  of  7,606.    Elapsed: 2:32:58.\n",
            "  Batch 6,480  of  7,606.    Elapsed: 2:33:55.\n",
            "  Batch 6,520  of  7,606.    Elapsed: 2:34:52.\n",
            "  Batch 6,560  of  7,606.    Elapsed: 2:35:49.\n",
            "  Batch 6,600  of  7,606.    Elapsed: 2:36:47.\n",
            "  Batch 6,640  of  7,606.    Elapsed: 2:37:44.\n",
            "  Batch 6,680  of  7,606.    Elapsed: 2:38:41.\n",
            "  Batch 6,720  of  7,606.    Elapsed: 2:39:38.\n",
            "  Batch 6,760  of  7,606.    Elapsed: 2:40:35.\n",
            "  Batch 6,800  of  7,606.    Elapsed: 2:41:32.\n",
            "  Batch 6,840  of  7,606.    Elapsed: 2:42:29.\n",
            "  Batch 6,880  of  7,606.    Elapsed: 2:43:26.\n",
            "  Batch 6,920  of  7,606.    Elapsed: 2:44:23.\n",
            "  Batch 6,960  of  7,606.    Elapsed: 2:45:20.\n",
            "  Batch 7,000  of  7,606.    Elapsed: 2:46:17.\n",
            "  Batch 7,040  of  7,606.    Elapsed: 2:47:14.\n",
            "  Batch 7,080  of  7,606.    Elapsed: 2:48:11.\n",
            "  Batch 7,120  of  7,606.    Elapsed: 2:49:08.\n",
            "  Batch 7,160  of  7,606.    Elapsed: 2:50:05.\n",
            "  Batch 7,200  of  7,606.    Elapsed: 2:51:02.\n",
            "  Batch 7,240  of  7,606.    Elapsed: 2:51:59.\n",
            "  Batch 7,280  of  7,606.    Elapsed: 2:52:56.\n",
            "  Batch 7,320  of  7,606.    Elapsed: 2:53:53.\n",
            "  Batch 7,360  of  7,606.    Elapsed: 2:54:50.\n",
            "  Batch 7,400  of  7,606.    Elapsed: 2:55:47.\n",
            "  Batch 7,440  of  7,606.    Elapsed: 2:56:44.\n",
            "  Batch 7,480  of  7,606.    Elapsed: 2:57:41.\n",
            "  Batch 7,520  of  7,606.    Elapsed: 2:58:38.\n",
            "  Batch 7,560  of  7,606.    Elapsed: 2:59:35.\n",
            "  Batch 7,600  of  7,606.    Elapsed: 3:00:32.\n",
            "\n",
            "  Average training loss: 0.07\n",
            "  Training epcoh took: 3:00:40\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.75\n",
            "  F1 score: 0.59\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.27      0.53      0.35     16123\n",
            "           1       0.92      0.77      0.84    105569\n",
            "\n",
            "    accuracy                           0.74    121692\n",
            "   macro avg       0.59      0.65      0.60    121692\n",
            "weighted avg       0.83      0.74      0.78    121692\n",
            "\n",
            "[[ 8596  7527]\n",
            " [23783 81786]]\n",
            "Mathews Correlation coefficient score for this epoch is : 0.24\n",
            "Model validation score: f1=0.839 auc=0.928\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Validation Loss: 0.46\n",
            "  Validation took: 0:14:17\n",
            "\n",
            "======== Epoch 2 / 8 ========\n",
            "Training...\n",
            "  Batch    40  of  7,606.    Elapsed: 0:00:57.\n",
            "  Batch    80  of  7,606.    Elapsed: 0:01:54.\n",
            "  Batch   120  of  7,606.    Elapsed: 0:02:51.\n",
            "  Batch   160  of  7,606.    Elapsed: 0:03:48.\n",
            "  Batch   200  of  7,606.    Elapsed: 0:04:45.\n",
            "  Batch   240  of  7,606.    Elapsed: 0:05:42.\n",
            "  Batch   280  of  7,606.    Elapsed: 0:06:39.\n",
            "  Batch   320  of  7,606.    Elapsed: 0:07:36.\n",
            "  Batch   360  of  7,606.    Elapsed: 0:08:33.\n",
            "  Batch   400  of  7,606.    Elapsed: 0:09:30.\n",
            "  Batch   440  of  7,606.    Elapsed: 0:10:27.\n",
            "  Batch   480  of  7,606.    Elapsed: 0:11:24.\n",
            "  Batch   520  of  7,606.    Elapsed: 0:12:21.\n",
            "  Batch   560  of  7,606.    Elapsed: 0:13:18.\n",
            "  Batch   600  of  7,606.    Elapsed: 0:14:15.\n",
            "  Batch   640  of  7,606.    Elapsed: 0:15:13.\n",
            "  Batch   680  of  7,606.    Elapsed: 0:16:10.\n",
            "  Batch   720  of  7,606.    Elapsed: 0:17:07.\n",
            "  Batch   760  of  7,606.    Elapsed: 0:18:04.\n",
            "  Batch   800  of  7,606.    Elapsed: 0:19:01.\n",
            "  Batch   840  of  7,606.    Elapsed: 0:19:58.\n",
            "  Batch   880  of  7,606.    Elapsed: 0:20:55.\n",
            "  Batch   920  of  7,606.    Elapsed: 0:21:52.\n",
            "  Batch   960  of  7,606.    Elapsed: 0:22:49.\n",
            "  Batch 1,000  of  7,606.    Elapsed: 0:23:46.\n",
            "  Batch 1,040  of  7,606.    Elapsed: 0:24:43.\n",
            "  Batch 1,080  of  7,606.    Elapsed: 0:25:40.\n",
            "  Batch 1,120  of  7,606.    Elapsed: 0:26:37.\n",
            "  Batch 1,160  of  7,606.    Elapsed: 0:27:34.\n",
            "  Batch 1,200  of  7,606.    Elapsed: 0:28:31.\n",
            "  Batch 1,240  of  7,606.    Elapsed: 0:29:28.\n",
            "  Batch 1,280  of  7,606.    Elapsed: 0:30:25.\n",
            "  Batch 1,320  of  7,606.    Elapsed: 0:31:22.\n",
            "  Batch 1,360  of  7,606.    Elapsed: 0:32:19.\n",
            "  Batch 1,400  of  7,606.    Elapsed: 0:33:16.\n",
            "  Batch 1,440  of  7,606.    Elapsed: 0:34:13.\n",
            "  Batch 1,480  of  7,606.    Elapsed: 0:35:10.\n",
            "  Batch 1,520  of  7,606.    Elapsed: 0:36:07.\n",
            "  Batch 1,560  of  7,606.    Elapsed: 0:37:04.\n",
            "  Batch 1,600  of  7,606.    Elapsed: 0:38:01.\n",
            "  Batch 1,640  of  7,606.    Elapsed: 0:38:58.\n",
            "  Batch 1,680  of  7,606.    Elapsed: 0:39:55.\n",
            "  Batch 1,720  of  7,606.    Elapsed: 0:40:52.\n",
            "  Batch 1,760  of  7,606.    Elapsed: 0:41:49.\n",
            "  Batch 1,800  of  7,606.    Elapsed: 0:42:46.\n",
            "  Batch 1,840  of  7,606.    Elapsed: 0:43:43.\n",
            "  Batch 1,880  of  7,606.    Elapsed: 0:44:40.\n",
            "  Batch 1,920  of  7,606.    Elapsed: 0:45:37.\n",
            "  Batch 1,960  of  7,606.    Elapsed: 0:46:34.\n",
            "  Batch 2,000  of  7,606.    Elapsed: 0:47:31.\n",
            "  Batch 2,040  of  7,606.    Elapsed: 0:48:28.\n",
            "  Batch 2,080  of  7,606.    Elapsed: 0:49:25.\n",
            "  Batch 2,120  of  7,606.    Elapsed: 0:50:22.\n",
            "  Batch 2,160  of  7,606.    Elapsed: 0:51:19.\n",
            "  Batch 2,200  of  7,606.    Elapsed: 0:52:16.\n",
            "  Batch 2,240  of  7,606.    Elapsed: 0:53:13.\n",
            "  Batch 2,280  of  7,606.    Elapsed: 0:54:10.\n",
            "  Batch 2,320  of  7,606.    Elapsed: 0:55:07.\n",
            "  Batch 2,360  of  7,606.    Elapsed: 0:56:04.\n",
            "  Batch 2,400  of  7,606.    Elapsed: 0:57:01.\n",
            "  Batch 2,440  of  7,606.    Elapsed: 0:57:58.\n",
            "  Batch 2,480  of  7,606.    Elapsed: 0:58:55.\n",
            "  Batch 2,520  of  7,606.    Elapsed: 0:59:52.\n",
            "  Batch 2,560  of  7,606.    Elapsed: 1:00:49.\n",
            "  Batch 2,600  of  7,606.    Elapsed: 1:01:46.\n",
            "  Batch 2,640  of  7,606.    Elapsed: 1:02:43.\n",
            "  Batch 2,680  of  7,606.    Elapsed: 1:03:40.\n",
            "  Batch 2,720  of  7,606.    Elapsed: 1:04:37.\n",
            "  Batch 2,760  of  7,606.    Elapsed: 1:05:34.\n",
            "  Batch 2,800  of  7,606.    Elapsed: 1:06:31.\n",
            "  Batch 2,840  of  7,606.    Elapsed: 1:07:28.\n",
            "  Batch 2,880  of  7,606.    Elapsed: 1:08:25.\n",
            "  Batch 2,920  of  7,606.    Elapsed: 1:09:22.\n",
            "  Batch 2,960  of  7,606.    Elapsed: 1:10:19.\n",
            "  Batch 3,000  of  7,606.    Elapsed: 1:11:16.\n",
            "  Batch 3,040  of  7,606.    Elapsed: 1:12:13.\n",
            "  Batch 3,080  of  7,606.    Elapsed: 1:13:10.\n",
            "  Batch 3,120  of  7,606.    Elapsed: 1:14:07.\n",
            "  Batch 3,160  of  7,606.    Elapsed: 1:15:04.\n",
            "  Batch 3,200  of  7,606.    Elapsed: 1:16:01.\n",
            "  Batch 3,240  of  7,606.    Elapsed: 1:16:58.\n",
            "  Batch 3,280  of  7,606.    Elapsed: 1:17:55.\n",
            "  Batch 3,320  of  7,606.    Elapsed: 1:18:52.\n",
            "  Batch 3,360  of  7,606.    Elapsed: 1:19:49.\n",
            "  Batch 3,400  of  7,606.    Elapsed: 1:20:46.\n",
            "  Batch 3,440  of  7,606.    Elapsed: 1:21:43.\n",
            "  Batch 3,480  of  7,606.    Elapsed: 1:22:40.\n",
            "  Batch 3,520  of  7,606.    Elapsed: 1:23:37.\n",
            "  Batch 3,560  of  7,606.    Elapsed: 1:24:34.\n",
            "  Batch 3,600  of  7,606.    Elapsed: 1:25:31.\n",
            "  Batch 3,640  of  7,606.    Elapsed: 1:26:28.\n",
            "  Batch 3,680  of  7,606.    Elapsed: 1:27:25.\n",
            "  Batch 3,720  of  7,606.    Elapsed: 1:28:22.\n",
            "  Batch 3,760  of  7,606.    Elapsed: 1:29:19.\n",
            "  Batch 3,800  of  7,606.    Elapsed: 1:30:16.\n",
            "  Batch 3,840  of  7,606.    Elapsed: 1:31:14.\n",
            "  Batch 3,880  of  7,606.    Elapsed: 1:32:11.\n",
            "  Batch 3,920  of  7,606.    Elapsed: 1:33:08.\n",
            "  Batch 3,960  of  7,606.    Elapsed: 1:34:05.\n",
            "  Batch 4,000  of  7,606.    Elapsed: 1:35:02.\n",
            "  Batch 4,040  of  7,606.    Elapsed: 1:35:59.\n",
            "  Batch 4,080  of  7,606.    Elapsed: 1:36:56.\n",
            "  Batch 4,120  of  7,606.    Elapsed: 1:37:53.\n",
            "  Batch 4,160  of  7,606.    Elapsed: 1:38:50.\n",
            "  Batch 4,200  of  7,606.    Elapsed: 1:39:47.\n",
            "  Batch 4,240  of  7,606.    Elapsed: 1:40:44.\n",
            "  Batch 4,280  of  7,606.    Elapsed: 1:41:41.\n",
            "  Batch 4,320  of  7,606.    Elapsed: 1:42:38.\n",
            "  Batch 4,360  of  7,606.    Elapsed: 1:43:35.\n",
            "  Batch 4,400  of  7,606.    Elapsed: 1:44:32.\n",
            "  Batch 4,440  of  7,606.    Elapsed: 1:45:29.\n",
            "  Batch 4,480  of  7,606.    Elapsed: 1:46:26.\n",
            "  Batch 4,520  of  7,606.    Elapsed: 1:47:23.\n",
            "  Batch 4,560  of  7,606.    Elapsed: 1:48:20.\n",
            "  Batch 4,600  of  7,606.    Elapsed: 1:49:17.\n",
            "  Batch 4,640  of  7,606.    Elapsed: 1:50:14.\n",
            "  Batch 4,680  of  7,606.    Elapsed: 1:51:11.\n",
            "  Batch 4,720  of  7,606.    Elapsed: 1:52:08.\n",
            "  Batch 4,760  of  7,606.    Elapsed: 1:53:05.\n",
            "  Batch 4,800  of  7,606.    Elapsed: 1:54:02.\n",
            "  Batch 4,840  of  7,606.    Elapsed: 1:54:59.\n",
            "  Batch 4,880  of  7,606.    Elapsed: 1:55:56.\n",
            "  Batch 4,920  of  7,606.    Elapsed: 1:56:53.\n",
            "  Batch 4,960  of  7,606.    Elapsed: 1:57:50.\n",
            "  Batch 5,000  of  7,606.    Elapsed: 1:58:47.\n",
            "  Batch 5,040  of  7,606.    Elapsed: 1:59:44.\n",
            "  Batch 5,080  of  7,606.    Elapsed: 2:00:41.\n",
            "  Batch 5,120  of  7,606.    Elapsed: 2:01:38.\n",
            "  Batch 5,160  of  7,606.    Elapsed: 2:02:35.\n",
            "  Batch 5,200  of  7,606.    Elapsed: 2:03:32.\n",
            "  Batch 5,240  of  7,606.    Elapsed: 2:04:29.\n",
            "  Batch 5,280  of  7,606.    Elapsed: 2:05:26.\n",
            "  Batch 5,320  of  7,606.    Elapsed: 2:06:23.\n",
            "  Batch 5,360  of  7,606.    Elapsed: 2:07:20.\n",
            "  Batch 5,400  of  7,606.    Elapsed: 2:08:17.\n",
            "  Batch 5,440  of  7,606.    Elapsed: 2:09:14.\n",
            "  Batch 5,480  of  7,606.    Elapsed: 2:10:11.\n",
            "  Batch 5,520  of  7,606.    Elapsed: 2:11:08.\n",
            "  Batch 5,560  of  7,606.    Elapsed: 2:12:05.\n",
            "  Batch 5,600  of  7,606.    Elapsed: 2:13:02.\n",
            "  Batch 5,640  of  7,606.    Elapsed: 2:13:59.\n",
            "  Batch 5,680  of  7,606.    Elapsed: 2:14:56.\n",
            "  Batch 5,720  of  7,606.    Elapsed: 2:15:54.\n",
            "  Batch 5,760  of  7,606.    Elapsed: 2:16:51.\n",
            "  Batch 5,800  of  7,606.    Elapsed: 2:17:48.\n",
            "  Batch 5,840  of  7,606.    Elapsed: 2:18:45.\n",
            "  Batch 5,880  of  7,606.    Elapsed: 2:19:42.\n",
            "  Batch 5,920  of  7,606.    Elapsed: 2:20:39.\n",
            "  Batch 5,960  of  7,606.    Elapsed: 2:21:36.\n",
            "  Batch 6,000  of  7,606.    Elapsed: 2:22:33.\n",
            "  Batch 6,040  of  7,606.    Elapsed: 2:23:30.\n",
            "  Batch 6,080  of  7,606.    Elapsed: 2:24:27.\n",
            "  Batch 6,120  of  7,606.    Elapsed: 2:25:24.\n",
            "  Batch 6,160  of  7,606.    Elapsed: 2:26:21.\n",
            "  Batch 6,200  of  7,606.    Elapsed: 2:27:18.\n",
            "  Batch 6,240  of  7,606.    Elapsed: 2:28:15.\n",
            "  Batch 6,280  of  7,606.    Elapsed: 2:29:12.\n",
            "  Batch 6,320  of  7,606.    Elapsed: 2:30:09.\n",
            "  Batch 6,360  of  7,606.    Elapsed: 2:31:06.\n",
            "  Batch 6,400  of  7,606.    Elapsed: 2:32:03.\n",
            "  Batch 6,440  of  7,606.    Elapsed: 2:33:00.\n",
            "  Batch 6,480  of  7,606.    Elapsed: 2:33:57.\n",
            "  Batch 6,520  of  7,606.    Elapsed: 2:34:54.\n",
            "  Batch 6,560  of  7,606.    Elapsed: 2:35:51.\n",
            "  Batch 6,600  of  7,606.    Elapsed: 2:36:48.\n",
            "  Batch 6,640  of  7,606.    Elapsed: 2:37:45.\n",
            "  Batch 6,680  of  7,606.    Elapsed: 2:38:42.\n",
            "  Batch 6,720  of  7,606.    Elapsed: 2:39:39.\n",
            "  Batch 6,760  of  7,606.    Elapsed: 2:40:36.\n",
            "  Batch 6,800  of  7,606.    Elapsed: 2:41:33.\n",
            "  Batch 6,840  of  7,606.    Elapsed: 2:42:30.\n",
            "  Batch 6,880  of  7,606.    Elapsed: 2:43:27.\n",
            "  Batch 6,920  of  7,606.    Elapsed: 2:44:24.\n",
            "  Batch 6,960  of  7,606.    Elapsed: 2:45:21.\n",
            "  Batch 7,000  of  7,606.    Elapsed: 2:46:18.\n",
            "  Batch 7,040  of  7,606.    Elapsed: 2:47:15.\n",
            "  Batch 7,080  of  7,606.    Elapsed: 2:48:12.\n",
            "  Batch 7,120  of  7,606.    Elapsed: 2:49:09.\n",
            "  Batch 7,160  of  7,606.    Elapsed: 2:50:06.\n",
            "  Batch 7,200  of  7,606.    Elapsed: 2:51:03.\n",
            "  Batch 7,240  of  7,606.    Elapsed: 2:52:00.\n",
            "  Batch 7,280  of  7,606.    Elapsed: 2:52:57.\n",
            "  Batch 7,320  of  7,606.    Elapsed: 2:53:54.\n",
            "  Batch 7,360  of  7,606.    Elapsed: 2:54:51.\n",
            "  Batch 7,400  of  7,606.    Elapsed: 2:55:48.\n",
            "  Batch 7,440  of  7,606.    Elapsed: 2:56:45.\n",
            "  Batch 7,480  of  7,606.    Elapsed: 2:57:42.\n",
            "  Batch 7,520  of  7,606.    Elapsed: 2:58:39.\n",
            "  Batch 7,560  of  7,606.    Elapsed: 2:59:36.\n",
            "  Batch 7,600  of  7,606.    Elapsed: 3:00:33.\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 3:00:42\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.52\n",
            "  F1 score: 0.43\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.18      0.89      0.30     16123\n",
            "           1       0.96      0.39      0.56    105569\n",
            "\n",
            "    accuracy                           0.46    121692\n",
            "   macro avg       0.57      0.64      0.43    121692\n",
            "weighted avg       0.86      0.46      0.52    121692\n",
            "\n",
            "[[14409  1714]\n",
            " [64066 41503]]\n",
            "Mathews Correlation coefficient score for this epoch is : 0.2\n",
            "Model validation score: f1=0.558 auc=0.945\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Validation Loss: 0.76\n",
            "  Validation took: 0:14:17\n",
            "\n",
            "======== Epoch 3 / 8 ========\n",
            "Training...\n",
            "  Batch    40  of  7,606.    Elapsed: 0:00:57.\n",
            "  Batch    80  of  7,606.    Elapsed: 0:01:54.\n",
            "  Batch   120  of  7,606.    Elapsed: 0:02:51.\n",
            "  Batch   160  of  7,606.    Elapsed: 0:03:48.\n",
            "  Batch   200  of  7,606.    Elapsed: 0:04:45.\n",
            "  Batch   240  of  7,606.    Elapsed: 0:05:42.\n",
            "  Batch   280  of  7,606.    Elapsed: 0:06:39.\n",
            "  Batch   320  of  7,606.    Elapsed: 0:07:36.\n",
            "  Batch   360  of  7,606.    Elapsed: 0:08:33.\n",
            "  Batch   400  of  7,606.    Elapsed: 0:09:30.\n",
            "  Batch   440  of  7,606.    Elapsed: 0:10:27.\n",
            "  Batch   480  of  7,606.    Elapsed: 0:11:24.\n",
            "  Batch   520  of  7,606.    Elapsed: 0:12:21.\n",
            "  Batch   560  of  7,606.    Elapsed: 0:13:18.\n",
            "  Batch   600  of  7,606.    Elapsed: 0:14:15.\n",
            "  Batch   640  of  7,606.    Elapsed: 0:15:12.\n",
            "  Batch   680  of  7,606.    Elapsed: 0:16:09.\n",
            "  Batch   720  of  7,606.    Elapsed: 0:17:06.\n",
            "  Batch   760  of  7,606.    Elapsed: 0:18:03.\n",
            "  Batch   800  of  7,606.    Elapsed: 0:19:00.\n",
            "  Batch   840  of  7,606.    Elapsed: 0:19:57.\n",
            "  Batch   880  of  7,606.    Elapsed: 0:20:54.\n",
            "  Batch   920  of  7,606.    Elapsed: 0:21:51.\n",
            "  Batch   960  of  7,606.    Elapsed: 0:22:48.\n",
            "  Batch 1,000  of  7,606.    Elapsed: 0:23:45.\n",
            "  Batch 1,040  of  7,606.    Elapsed: 0:24:42.\n",
            "  Batch 1,080  of  7,606.    Elapsed: 0:25:39.\n",
            "  Batch 1,120  of  7,606.    Elapsed: 0:26:36.\n",
            "  Batch 1,160  of  7,606.    Elapsed: 0:27:33.\n",
            "  Batch 1,200  of  7,606.    Elapsed: 0:28:30.\n",
            "  Batch 1,240  of  7,606.    Elapsed: 0:29:27.\n",
            "  Batch 1,280  of  7,606.    Elapsed: 0:30:24.\n",
            "  Batch 1,320  of  7,606.    Elapsed: 0:31:21.\n",
            "  Batch 1,360  of  7,606.    Elapsed: 0:32:18.\n",
            "  Batch 1,400  of  7,606.    Elapsed: 0:33:15.\n",
            "  Batch 1,440  of  7,606.    Elapsed: 0:34:12.\n",
            "  Batch 1,480  of  7,606.    Elapsed: 0:35:09.\n",
            "  Batch 1,520  of  7,606.    Elapsed: 0:36:06.\n",
            "  Batch 1,560  of  7,606.    Elapsed: 0:37:03.\n",
            "  Batch 1,600  of  7,606.    Elapsed: 0:38:00.\n",
            "  Batch 1,640  of  7,606.    Elapsed: 0:38:58.\n",
            "  Batch 1,680  of  7,606.    Elapsed: 0:39:55.\n",
            "  Batch 1,720  of  7,606.    Elapsed: 0:40:52.\n",
            "  Batch 1,760  of  7,606.    Elapsed: 0:41:49.\n",
            "  Batch 1,800  of  7,606.    Elapsed: 0:42:46.\n",
            "  Batch 1,840  of  7,606.    Elapsed: 0:43:43.\n",
            "  Batch 1,880  of  7,606.    Elapsed: 0:44:40.\n",
            "  Batch 1,920  of  7,606.    Elapsed: 0:45:37.\n",
            "  Batch 1,960  of  7,606.    Elapsed: 0:46:34.\n",
            "  Batch 2,000  of  7,606.    Elapsed: 0:47:31.\n",
            "  Batch 2,040  of  7,606.    Elapsed: 0:48:28.\n",
            "  Batch 2,080  of  7,606.    Elapsed: 0:49:25.\n",
            "  Batch 2,120  of  7,606.    Elapsed: 0:50:22.\n",
            "  Batch 2,160  of  7,606.    Elapsed: 0:51:19.\n",
            "  Batch 2,200  of  7,606.    Elapsed: 0:52:16.\n",
            "  Batch 2,240  of  7,606.    Elapsed: 0:53:13.\n",
            "  Batch 2,280  of  7,606.    Elapsed: 0:54:10.\n",
            "  Batch 2,320  of  7,606.    Elapsed: 0:55:07.\n",
            "  Batch 2,360  of  7,606.    Elapsed: 0:56:04.\n",
            "  Batch 2,400  of  7,606.    Elapsed: 0:57:01.\n",
            "  Batch 2,440  of  7,606.    Elapsed: 0:57:58.\n",
            "  Batch 2,480  of  7,606.    Elapsed: 0:58:55.\n",
            "  Batch 2,520  of  7,606.    Elapsed: 0:59:52.\n",
            "  Batch 2,560  of  7,606.    Elapsed: 1:00:49.\n",
            "  Batch 2,600  of  7,606.    Elapsed: 1:01:46.\n",
            "  Batch 2,640  of  7,606.    Elapsed: 1:02:43.\n",
            "  Batch 2,680  of  7,606.    Elapsed: 1:03:40.\n",
            "  Batch 2,720  of  7,606.    Elapsed: 1:04:37.\n",
            "  Batch 2,760  of  7,606.    Elapsed: 1:05:34.\n",
            "  Batch 2,800  of  7,606.    Elapsed: 1:06:31.\n",
            "  Batch 2,840  of  7,606.    Elapsed: 1:07:28.\n",
            "  Batch 2,880  of  7,606.    Elapsed: 1:08:25.\n",
            "  Batch 2,920  of  7,606.    Elapsed: 1:09:22.\n",
            "  Batch 2,960  of  7,606.    Elapsed: 1:10:19.\n",
            "  Batch 3,000  of  7,606.    Elapsed: 1:11:16.\n",
            "  Batch 3,040  of  7,606.    Elapsed: 1:12:13.\n",
            "  Batch 3,080  of  7,606.    Elapsed: 1:13:10.\n",
            "  Batch 3,120  of  7,606.    Elapsed: 1:14:07.\n",
            "  Batch 3,160  of  7,606.    Elapsed: 1:15:04.\n",
            "  Batch 3,200  of  7,606.    Elapsed: 1:16:01.\n",
            "  Batch 3,240  of  7,606.    Elapsed: 1:16:58.\n",
            "  Batch 3,280  of  7,606.    Elapsed: 1:17:55.\n",
            "  Batch 3,320  of  7,606.    Elapsed: 1:18:52.\n",
            "  Batch 3,360  of  7,606.    Elapsed: 1:19:49.\n",
            "  Batch 3,400  of  7,606.    Elapsed: 1:20:46.\n",
            "  Batch 3,440  of  7,606.    Elapsed: 1:21:43.\n",
            "  Batch 3,480  of  7,606.    Elapsed: 1:22:40.\n",
            "  Batch 3,520  of  7,606.    Elapsed: 1:23:37.\n",
            "  Batch 3,560  of  7,606.    Elapsed: 1:24:34.\n",
            "  Batch 3,600  of  7,606.    Elapsed: 1:25:31.\n",
            "  Batch 3,640  of  7,606.    Elapsed: 1:26:28.\n",
            "  Batch 3,680  of  7,606.    Elapsed: 1:27:25.\n",
            "  Batch 3,720  of  7,606.    Elapsed: 1:28:22.\n",
            "  Batch 3,760  of  7,606.    Elapsed: 1:29:19.\n",
            "  Batch 3,800  of  7,606.    Elapsed: 1:30:16.\n",
            "  Batch 3,840  of  7,606.    Elapsed: 1:31:13.\n",
            "  Batch 3,880  of  7,606.    Elapsed: 1:32:10.\n",
            "  Batch 3,920  of  7,606.    Elapsed: 1:33:07.\n",
            "  Batch 3,960  of  7,606.    Elapsed: 1:34:04.\n",
            "  Batch 4,000  of  7,606.    Elapsed: 1:35:01.\n",
            "  Batch 4,040  of  7,606.    Elapsed: 1:35:58.\n",
            "  Batch 4,080  of  7,606.    Elapsed: 1:36:55.\n",
            "  Batch 4,120  of  7,606.    Elapsed: 1:37:52.\n",
            "  Batch 4,160  of  7,606.    Elapsed: 1:38:49.\n",
            "  Batch 4,200  of  7,606.    Elapsed: 1:39:46.\n",
            "  Batch 4,240  of  7,606.    Elapsed: 1:40:43.\n",
            "  Batch 4,280  of  7,606.    Elapsed: 1:41:40.\n",
            "  Batch 4,320  of  7,606.    Elapsed: 1:42:37.\n",
            "  Batch 4,360  of  7,606.    Elapsed: 1:43:34.\n",
            "  Batch 4,400  of  7,606.    Elapsed: 1:44:31.\n",
            "  Batch 4,440  of  7,606.    Elapsed: 1:45:28.\n",
            "  Batch 4,480  of  7,606.    Elapsed: 1:46:25.\n",
            "  Batch 4,520  of  7,606.    Elapsed: 1:47:22.\n",
            "  Batch 4,560  of  7,606.    Elapsed: 1:48:19.\n",
            "  Batch 4,600  of  7,606.    Elapsed: 1:49:16.\n",
            "  Batch 4,640  of  7,606.    Elapsed: 1:50:13.\n",
            "  Batch 4,680  of  7,606.    Elapsed: 1:51:10.\n",
            "  Batch 4,720  of  7,606.    Elapsed: 1:52:07.\n",
            "  Batch 4,760  of  7,606.    Elapsed: 1:53:04.\n",
            "  Batch 4,800  of  7,606.    Elapsed: 1:54:01.\n",
            "  Batch 4,840  of  7,606.    Elapsed: 1:54:58.\n",
            "  Batch 4,880  of  7,606.    Elapsed: 1:55:55.\n",
            "  Batch 4,920  of  7,606.    Elapsed: 1:56:52.\n",
            "  Batch 4,960  of  7,606.    Elapsed: 1:57:49.\n",
            "  Batch 5,000  of  7,606.    Elapsed: 1:58:46.\n",
            "  Batch 5,040  of  7,606.    Elapsed: 1:59:43.\n",
            "  Batch 5,080  of  7,606.    Elapsed: 2:00:40.\n",
            "  Batch 5,120  of  7,606.    Elapsed: 2:01:37.\n",
            "  Batch 5,160  of  7,606.    Elapsed: 2:02:34.\n",
            "  Batch 5,200  of  7,606.    Elapsed: 2:03:31.\n",
            "  Batch 5,240  of  7,606.    Elapsed: 2:04:28.\n",
            "  Batch 5,280  of  7,606.    Elapsed: 2:05:25.\n",
            "  Batch 5,320  of  7,606.    Elapsed: 2:06:22.\n",
            "  Batch 5,360  of  7,606.    Elapsed: 2:07:19.\n",
            "  Batch 5,400  of  7,606.    Elapsed: 2:08:16.\n",
            "  Batch 5,440  of  7,606.    Elapsed: 2:09:13.\n",
            "  Batch 5,480  of  7,606.    Elapsed: 2:10:10.\n",
            "  Batch 5,520  of  7,606.    Elapsed: 2:11:07.\n",
            "  Batch 5,560  of  7,606.    Elapsed: 2:12:04.\n",
            "  Batch 5,600  of  7,606.    Elapsed: 2:13:01.\n",
            "  Batch 5,640  of  7,606.    Elapsed: 2:13:58.\n",
            "  Batch 5,680  of  7,606.    Elapsed: 2:14:55.\n",
            "  Batch 5,720  of  7,606.    Elapsed: 2:15:52.\n",
            "  Batch 5,760  of  7,606.    Elapsed: 2:16:49.\n",
            "  Batch 5,800  of  7,606.    Elapsed: 2:17:46.\n",
            "  Batch 5,840  of  7,606.    Elapsed: 2:18:43.\n",
            "  Batch 5,880  of  7,606.    Elapsed: 2:19:40.\n",
            "  Batch 5,920  of  7,606.    Elapsed: 2:20:37.\n",
            "  Batch 5,960  of  7,606.    Elapsed: 2:21:34.\n",
            "  Batch 6,000  of  7,606.    Elapsed: 2:22:31.\n",
            "  Batch 6,040  of  7,606.    Elapsed: 2:23:28.\n",
            "  Batch 6,080  of  7,606.    Elapsed: 2:24:25.\n",
            "  Batch 6,120  of  7,606.    Elapsed: 2:25:22.\n",
            "  Batch 6,160  of  7,606.    Elapsed: 2:26:19.\n",
            "  Batch 6,200  of  7,606.    Elapsed: 2:27:16.\n",
            "  Batch 6,240  of  7,606.    Elapsed: 2:28:13.\n",
            "  Batch 6,280  of  7,606.    Elapsed: 2:29:10.\n",
            "  Batch 6,320  of  7,606.    Elapsed: 2:30:07.\n",
            "  Batch 6,360  of  7,606.    Elapsed: 2:31:04.\n",
            "  Batch 6,400  of  7,606.    Elapsed: 2:32:01.\n",
            "  Batch 6,440  of  7,606.    Elapsed: 2:32:58.\n",
            "  Batch 6,480  of  7,606.    Elapsed: 2:33:55.\n",
            "  Batch 6,520  of  7,606.    Elapsed: 2:34:52.\n",
            "  Batch 6,560  of  7,606.    Elapsed: 2:35:49.\n",
            "  Batch 6,600  of  7,606.    Elapsed: 2:36:46.\n",
            "  Batch 6,640  of  7,606.    Elapsed: 2:37:43.\n",
            "  Batch 6,680  of  7,606.    Elapsed: 2:38:40.\n",
            "  Batch 6,720  of  7,606.    Elapsed: 2:39:37.\n",
            "  Batch 6,760  of  7,606.    Elapsed: 2:40:34.\n",
            "  Batch 6,800  of  7,606.    Elapsed: 2:41:31.\n",
            "  Batch 6,840  of  7,606.    Elapsed: 2:42:28.\n",
            "  Batch 6,880  of  7,606.    Elapsed: 2:43:25.\n",
            "  Batch 6,920  of  7,606.    Elapsed: 2:44:22.\n",
            "  Batch 6,960  of  7,606.    Elapsed: 2:45:19.\n",
            "  Batch 7,000  of  7,606.    Elapsed: 2:46:16.\n",
            "  Batch 7,040  of  7,606.    Elapsed: 2:47:13.\n",
            "  Batch 7,080  of  7,606.    Elapsed: 2:48:10.\n",
            "  Batch 7,120  of  7,606.    Elapsed: 2:49:07.\n",
            "  Batch 7,160  of  7,606.    Elapsed: 2:50:04.\n",
            "  Batch 7,200  of  7,606.    Elapsed: 2:51:01.\n",
            "  Batch 7,240  of  7,606.    Elapsed: 2:51:58.\n",
            "  Batch 7,280  of  7,606.    Elapsed: 2:52:55.\n",
            "  Batch 7,320  of  7,606.    Elapsed: 2:53:52.\n",
            "  Batch 7,360  of  7,606.    Elapsed: 2:54:49.\n",
            "  Batch 7,400  of  7,606.    Elapsed: 2:55:46.\n",
            "  Batch 7,440  of  7,606.    Elapsed: 2:56:43.\n",
            "  Batch 7,480  of  7,606.    Elapsed: 2:57:40.\n",
            "  Batch 7,520  of  7,606.    Elapsed: 2:58:37.\n",
            "  Batch 7,560  of  7,606.    Elapsed: 2:59:34.\n",
            "  Batch 7,600  of  7,606.    Elapsed: 3:00:31.\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 3:00:39\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.60\n",
            "  F1 score: 0.51\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.81      0.33     16123\n",
            "           1       0.95      0.54      0.69    105569\n",
            "\n",
            "    accuracy                           0.57    121692\n",
            "   macro avg       0.58      0.67      0.51    121692\n",
            "weighted avg       0.85      0.57      0.64    121692\n",
            "\n",
            "[[13063  3060]\n",
            " [48949 56620]]\n",
            "Mathews Correlation coefficient score for this epoch is : 0.24\n",
            "Model validation score: f1=0.685 auc=0.947\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Validation Loss: 0.68\n",
            "  Validation took: 0:14:17\n",
            "\n",
            "======== Epoch 4 / 8 ========\n",
            "Training...\n",
            "  Batch    40  of  7,606.    Elapsed: 0:00:57.\n",
            "  Batch    80  of  7,606.    Elapsed: 0:01:54.\n",
            "  Batch   120  of  7,606.    Elapsed: 0:02:51.\n",
            "  Batch   160  of  7,606.    Elapsed: 0:03:48.\n",
            "  Batch   200  of  7,606.    Elapsed: 0:04:45.\n",
            "  Batch   240  of  7,606.    Elapsed: 0:05:42.\n",
            "  Batch   280  of  7,606.    Elapsed: 0:06:39.\n",
            "  Batch   320  of  7,606.    Elapsed: 0:07:36.\n",
            "  Batch   360  of  7,606.    Elapsed: 0:08:33.\n",
            "  Batch   400  of  7,606.    Elapsed: 0:09:30.\n",
            "  Batch   440  of  7,606.    Elapsed: 0:10:27.\n",
            "  Batch   480  of  7,606.    Elapsed: 0:11:24.\n",
            "  Batch   520  of  7,606.    Elapsed: 0:12:21.\n",
            "  Batch   560  of  7,606.    Elapsed: 0:13:18.\n",
            "  Batch   600  of  7,606.    Elapsed: 0:14:15.\n",
            "  Batch   640  of  7,606.    Elapsed: 0:15:12.\n",
            "  Batch   680  of  7,606.    Elapsed: 0:16:09.\n",
            "  Batch   720  of  7,606.    Elapsed: 0:17:06.\n",
            "  Batch   760  of  7,606.    Elapsed: 0:18:03.\n",
            "  Batch   800  of  7,606.    Elapsed: 0:19:00.\n",
            "  Batch   840  of  7,606.    Elapsed: 0:19:57.\n",
            "  Batch   880  of  7,606.    Elapsed: 0:20:54.\n",
            "  Batch   920  of  7,606.    Elapsed: 0:21:51.\n",
            "  Batch   960  of  7,606.    Elapsed: 0:22:48.\n",
            "  Batch 1,000  of  7,606.    Elapsed: 0:23:45.\n",
            "  Batch 1,040  of  7,606.    Elapsed: 0:24:42.\n",
            "  Batch 1,080  of  7,606.    Elapsed: 0:25:39.\n",
            "  Batch 1,120  of  7,606.    Elapsed: 0:26:37.\n",
            "  Batch 1,160  of  7,606.    Elapsed: 0:27:34.\n",
            "  Batch 1,200  of  7,606.    Elapsed: 0:28:31.\n",
            "  Batch 1,240  of  7,606.    Elapsed: 0:29:28.\n",
            "  Batch 1,280  of  7,606.    Elapsed: 0:30:25.\n",
            "  Batch 1,320  of  7,606.    Elapsed: 0:31:22.\n",
            "  Batch 1,360  of  7,606.    Elapsed: 0:32:19.\n",
            "  Batch 1,400  of  7,606.    Elapsed: 0:33:16.\n",
            "  Batch 1,440  of  7,606.    Elapsed: 0:34:13.\n",
            "  Batch 1,480  of  7,606.    Elapsed: 0:35:10.\n",
            "  Batch 1,520  of  7,606.    Elapsed: 0:36:07.\n",
            "  Batch 1,560  of  7,606.    Elapsed: 0:37:04.\n",
            "  Batch 1,600  of  7,606.    Elapsed: 0:38:01.\n",
            "  Batch 1,640  of  7,606.    Elapsed: 0:38:58.\n",
            "  Batch 1,680  of  7,606.    Elapsed: 0:39:55.\n",
            "  Batch 1,720  of  7,606.    Elapsed: 0:40:52.\n",
            "  Batch 1,760  of  7,606.    Elapsed: 0:41:49.\n",
            "  Batch 1,800  of  7,606.    Elapsed: 0:42:46.\n",
            "  Batch 1,840  of  7,606.    Elapsed: 0:43:43.\n",
            "  Batch 1,880  of  7,606.    Elapsed: 0:44:40.\n",
            "  Batch 1,920  of  7,606.    Elapsed: 0:45:37.\n",
            "  Batch 1,960  of  7,606.    Elapsed: 0:46:34.\n",
            "  Batch 2,000  of  7,606.    Elapsed: 0:47:31.\n",
            "  Batch 2,040  of  7,606.    Elapsed: 0:48:28.\n",
            "  Batch 2,080  of  7,606.    Elapsed: 0:49:25.\n",
            "  Batch 2,120  of  7,606.    Elapsed: 0:50:22.\n",
            "  Batch 2,160  of  7,606.    Elapsed: 0:51:19.\n",
            "  Batch 2,200  of  7,606.    Elapsed: 0:52:16.\n",
            "  Batch 2,240  of  7,606.    Elapsed: 0:53:13.\n",
            "  Batch 2,280  of  7,606.    Elapsed: 0:54:10.\n",
            "  Batch 2,320  of  7,606.    Elapsed: 0:55:07.\n",
            "  Batch 2,360  of  7,606.    Elapsed: 0:56:04.\n",
            "  Batch 2,400  of  7,606.    Elapsed: 0:57:01.\n",
            "  Batch 2,440  of  7,606.    Elapsed: 0:57:58.\n",
            "  Batch 2,480  of  7,606.    Elapsed: 0:58:55.\n",
            "  Batch 2,520  of  7,606.    Elapsed: 0:59:52.\n",
            "  Batch 2,560  of  7,606.    Elapsed: 1:00:49.\n",
            "  Batch 2,600  of  7,606.    Elapsed: 1:01:46.\n",
            "  Batch 2,640  of  7,606.    Elapsed: 1:02:43.\n",
            "  Batch 2,680  of  7,606.    Elapsed: 1:03:40.\n",
            "  Batch 2,720  of  7,606.    Elapsed: 1:04:37.\n",
            "  Batch 2,760  of  7,606.    Elapsed: 1:05:34.\n",
            "  Batch 2,800  of  7,606.    Elapsed: 1:06:31.\n",
            "  Batch 2,840  of  7,606.    Elapsed: 1:07:28.\n",
            "  Batch 2,880  of  7,606.    Elapsed: 1:08:25.\n",
            "  Batch 2,920  of  7,606.    Elapsed: 1:09:22.\n",
            "  Batch 2,960  of  7,606.    Elapsed: 1:10:19.\n",
            "  Batch 3,000  of  7,606.    Elapsed: 1:11:16.\n",
            "  Batch 3,040  of  7,606.    Elapsed: 1:12:13.\n",
            "  Batch 3,080  of  7,606.    Elapsed: 1:13:10.\n",
            "  Batch 3,120  of  7,606.    Elapsed: 1:14:07.\n",
            "  Batch 3,160  of  7,606.    Elapsed: 1:15:04.\n",
            "  Batch 3,200  of  7,606.    Elapsed: 1:16:01.\n",
            "  Batch 3,240  of  7,606.    Elapsed: 1:16:58.\n",
            "  Batch 3,280  of  7,606.    Elapsed: 1:17:55.\n",
            "  Batch 3,320  of  7,606.    Elapsed: 1:18:52.\n",
            "  Batch 3,360  of  7,606.    Elapsed: 1:19:49.\n",
            "  Batch 3,400  of  7,606.    Elapsed: 1:20:46.\n",
            "  Batch 3,440  of  7,606.    Elapsed: 1:21:43.\n",
            "  Batch 3,480  of  7,606.    Elapsed: 1:22:40.\n",
            "  Batch 3,520  of  7,606.    Elapsed: 1:23:37.\n",
            "  Batch 3,560  of  7,606.    Elapsed: 1:24:34.\n",
            "  Batch 3,600  of  7,606.    Elapsed: 1:25:31.\n",
            "  Batch 3,640  of  7,606.    Elapsed: 1:26:28.\n",
            "  Batch 3,680  of  7,606.    Elapsed: 1:27:25.\n",
            "  Batch 3,720  of  7,606.    Elapsed: 1:28:22.\n",
            "  Batch 3,760  of  7,606.    Elapsed: 1:29:19.\n",
            "  Batch 3,800  of  7,606.    Elapsed: 1:30:16.\n",
            "  Batch 3,840  of  7,606.    Elapsed: 1:31:13.\n",
            "  Batch 3,880  of  7,606.    Elapsed: 1:32:10.\n",
            "  Batch 3,920  of  7,606.    Elapsed: 1:33:07.\n",
            "  Batch 3,960  of  7,606.    Elapsed: 1:34:04.\n",
            "  Batch 4,000  of  7,606.    Elapsed: 1:35:01.\n",
            "  Batch 4,040  of  7,606.    Elapsed: 1:35:58.\n",
            "  Batch 4,080  of  7,606.    Elapsed: 1:36:55.\n",
            "  Batch 4,120  of  7,606.    Elapsed: 1:37:52.\n",
            "  Batch 4,160  of  7,606.    Elapsed: 1:38:49.\n",
            "  Batch 4,200  of  7,606.    Elapsed: 1:39:46.\n",
            "  Batch 4,240  of  7,606.    Elapsed: 1:40:43.\n",
            "  Batch 4,280  of  7,606.    Elapsed: 1:41:40.\n",
            "  Batch 4,320  of  7,606.    Elapsed: 1:42:37.\n",
            "  Batch 4,360  of  7,606.    Elapsed: 1:43:34.\n",
            "  Batch 4,400  of  7,606.    Elapsed: 1:44:31.\n",
            "  Batch 4,440  of  7,606.    Elapsed: 1:45:28.\n",
            "  Batch 4,480  of  7,606.    Elapsed: 1:46:25.\n",
            "  Batch 4,520  of  7,606.    Elapsed: 1:47:22.\n",
            "  Batch 4,560  of  7,606.    Elapsed: 1:48:19.\n",
            "  Batch 4,600  of  7,606.    Elapsed: 1:49:16.\n",
            "  Batch 4,640  of  7,606.    Elapsed: 1:50:13.\n",
            "  Batch 4,680  of  7,606.    Elapsed: 1:51:10.\n",
            "  Batch 4,720  of  7,606.    Elapsed: 1:52:07.\n",
            "  Batch 4,760  of  7,606.    Elapsed: 1:53:04.\n",
            "  Batch 4,800  of  7,606.    Elapsed: 1:54:01.\n",
            "  Batch 4,840  of  7,606.    Elapsed: 1:54:58.\n",
            "  Batch 4,880  of  7,606.    Elapsed: 1:55:55.\n",
            "  Batch 4,920  of  7,606.    Elapsed: 1:56:52.\n",
            "  Batch 4,960  of  7,606.    Elapsed: 1:57:49.\n",
            "  Batch 5,000  of  7,606.    Elapsed: 1:58:46.\n",
            "  Batch 5,040  of  7,606.    Elapsed: 1:59:43.\n",
            "  Batch 5,080  of  7,606.    Elapsed: 2:00:40.\n",
            "  Batch 5,120  of  7,606.    Elapsed: 2:01:37.\n",
            "  Batch 5,160  of  7,606.    Elapsed: 2:02:34.\n",
            "  Batch 5,200  of  7,606.    Elapsed: 2:03:31.\n",
            "  Batch 5,240  of  7,606.    Elapsed: 2:04:28.\n",
            "  Batch 5,280  of  7,606.    Elapsed: 2:05:25.\n",
            "  Batch 5,320  of  7,606.    Elapsed: 2:06:22.\n",
            "  Batch 5,360  of  7,606.    Elapsed: 2:07:20.\n",
            "  Batch 5,400  of  7,606.    Elapsed: 2:08:17.\n",
            "  Batch 5,440  of  7,606.    Elapsed: 2:09:14.\n",
            "  Batch 5,480  of  7,606.    Elapsed: 2:10:11.\n",
            "  Batch 5,520  of  7,606.    Elapsed: 2:11:08.\n",
            "  Batch 5,560  of  7,606.    Elapsed: 2:12:05.\n",
            "  Batch 5,600  of  7,606.    Elapsed: 2:13:02.\n",
            "  Batch 5,640  of  7,606.    Elapsed: 2:13:59.\n",
            "  Batch 5,680  of  7,606.    Elapsed: 2:14:56.\n",
            "  Batch 5,720  of  7,606.    Elapsed: 2:15:53.\n",
            "  Batch 5,760  of  7,606.    Elapsed: 2:16:50.\n",
            "  Batch 5,800  of  7,606.    Elapsed: 2:17:47.\n",
            "  Batch 5,840  of  7,606.    Elapsed: 2:18:44.\n",
            "  Batch 5,880  of  7,606.    Elapsed: 2:19:41.\n",
            "  Batch 5,920  of  7,606.    Elapsed: 2:20:38.\n",
            "  Batch 5,960  of  7,606.    Elapsed: 2:21:35.\n",
            "  Batch 6,000  of  7,606.    Elapsed: 2:22:32.\n",
            "  Batch 6,040  of  7,606.    Elapsed: 2:23:29.\n",
            "  Batch 6,080  of  7,606.    Elapsed: 2:24:26.\n",
            "  Batch 6,120  of  7,606.    Elapsed: 2:25:23.\n",
            "  Batch 6,160  of  7,606.    Elapsed: 2:26:20.\n",
            "  Batch 6,200  of  7,606.    Elapsed: 2:27:17.\n",
            "  Batch 6,240  of  7,606.    Elapsed: 2:28:14.\n",
            "  Batch 6,280  of  7,606.    Elapsed: 2:29:11.\n",
            "  Batch 6,320  of  7,606.    Elapsed: 2:30:08.\n",
            "  Batch 6,360  of  7,606.    Elapsed: 2:31:05.\n",
            "  Batch 6,400  of  7,606.    Elapsed: 2:32:02.\n",
            "  Batch 6,440  of  7,606.    Elapsed: 2:32:59.\n",
            "  Batch 6,480  of  7,606.    Elapsed: 2:33:56.\n",
            "  Batch 6,520  of  7,606.    Elapsed: 2:34:53.\n",
            "  Batch 6,560  of  7,606.    Elapsed: 2:35:50.\n",
            "  Batch 6,600  of  7,606.    Elapsed: 2:36:47.\n",
            "  Batch 6,640  of  7,606.    Elapsed: 2:37:44.\n",
            "  Batch 6,680  of  7,606.    Elapsed: 2:38:41.\n",
            "  Batch 6,720  of  7,606.    Elapsed: 2:39:38.\n",
            "  Batch 6,760  of  7,606.    Elapsed: 2:40:35.\n",
            "  Batch 6,800  of  7,606.    Elapsed: 2:41:32.\n",
            "  Batch 6,840  of  7,606.    Elapsed: 2:42:29.\n",
            "  Batch 6,880  of  7,606.    Elapsed: 2:43:26.\n",
            "  Batch 6,920  of  7,606.    Elapsed: 2:44:23.\n",
            "  Batch 6,960  of  7,606.    Elapsed: 2:45:20.\n",
            "  Batch 7,000  of  7,606.    Elapsed: 2:46:17.\n",
            "  Batch 7,040  of  7,606.    Elapsed: 2:47:14.\n",
            "  Batch 7,080  of  7,606.    Elapsed: 2:48:11.\n",
            "  Batch 7,120  of  7,606.    Elapsed: 2:49:08.\n",
            "  Batch 7,160  of  7,606.    Elapsed: 2:50:05.\n",
            "  Batch 7,200  of  7,606.    Elapsed: 2:51:02.\n",
            "  Batch 7,240  of  7,606.    Elapsed: 2:51:59.\n",
            "  Batch 7,280  of  7,606.    Elapsed: 2:52:56.\n",
            "  Batch 7,320  of  7,606.    Elapsed: 2:53:53.\n",
            "  Batch 7,360  of  7,606.    Elapsed: 2:54:50.\n",
            "  Batch 7,400  of  7,606.    Elapsed: 2:55:47.\n",
            "  Batch 7,440  of  7,606.    Elapsed: 2:56:44.\n",
            "  Batch 7,480  of  7,606.    Elapsed: 2:57:41.\n",
            "  Batch 7,520  of  7,606.    Elapsed: 2:58:38.\n",
            "  Batch 7,560  of  7,606.    Elapsed: 2:59:35.\n",
            "  Batch 7,600  of  7,606.    Elapsed: 3:00:32.\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 3:00:40\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.72\n",
            "  F1 score: 0.57\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.68      0.37     16123\n",
            "           1       0.93      0.69      0.79    105569\n",
            "\n",
            "    accuracy                           0.69    121692\n",
            "   macro avg       0.59      0.68      0.58    121692\n",
            "weighted avg       0.84      0.69      0.74    121692\n",
            "\n",
            "[[11003  5120]\n",
            " [33029 72540]]\n",
            "Mathews Correlation coefficient score for this epoch is : 0.26\n",
            "Model validation score: f1=0.792 auc=0.949\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Validation Loss: 0.55\n",
            "  Validation took: 0:14:17\n",
            "\n",
            "======== Epoch 5 / 8 ========\n",
            "Training...\n",
            "  Batch    40  of  7,606.    Elapsed: 0:00:57.\n",
            "  Batch    80  of  7,606.    Elapsed: 0:01:54.\n",
            "  Batch   120  of  7,606.    Elapsed: 0:02:51.\n",
            "  Batch   160  of  7,606.    Elapsed: 0:03:48.\n",
            "  Batch   200  of  7,606.    Elapsed: 0:04:45.\n",
            "  Batch   240  of  7,606.    Elapsed: 0:05:42.\n",
            "  Batch   280  of  7,606.    Elapsed: 0:06:39.\n",
            "  Batch   320  of  7,606.    Elapsed: 0:07:36.\n",
            "  Batch   360  of  7,606.    Elapsed: 0:08:33.\n",
            "  Batch   400  of  7,606.    Elapsed: 0:09:30.\n",
            "  Batch   440  of  7,606.    Elapsed: 0:10:27.\n",
            "  Batch   480  of  7,606.    Elapsed: 0:11:24.\n",
            "  Batch   520  of  7,606.    Elapsed: 0:12:21.\n",
            "  Batch   560  of  7,606.    Elapsed: 0:13:18.\n",
            "  Batch   600  of  7,606.    Elapsed: 0:14:15.\n",
            "  Batch   640  of  7,606.    Elapsed: 0:15:12.\n",
            "  Batch   680  of  7,606.    Elapsed: 0:16:09.\n",
            "  Batch   720  of  7,606.    Elapsed: 0:17:06.\n",
            "  Batch   760  of  7,606.    Elapsed: 0:18:03.\n",
            "  Batch   800  of  7,606.    Elapsed: 0:19:00.\n",
            "  Batch   840  of  7,606.    Elapsed: 0:19:57.\n",
            "  Batch   880  of  7,606.    Elapsed: 0:20:54.\n",
            "  Batch   920  of  7,606.    Elapsed: 0:21:51.\n",
            "  Batch   960  of  7,606.    Elapsed: 0:22:48.\n",
            "  Batch 1,000  of  7,606.    Elapsed: 0:23:45.\n",
            "  Batch 1,040  of  7,606.    Elapsed: 0:24:42.\n",
            "  Batch 1,080  of  7,606.    Elapsed: 0:25:39.\n",
            "  Batch 1,120  of  7,606.    Elapsed: 0:26:36.\n",
            "  Batch 1,160  of  7,606.    Elapsed: 0:27:34.\n",
            "  Batch 1,200  of  7,606.    Elapsed: 0:28:31.\n",
            "  Batch 1,240  of  7,606.    Elapsed: 0:29:28.\n",
            "  Batch 1,280  of  7,606.    Elapsed: 0:30:25.\n",
            "  Batch 1,320  of  7,606.    Elapsed: 0:31:22.\n",
            "  Batch 1,360  of  7,606.    Elapsed: 0:32:19.\n",
            "  Batch 1,400  of  7,606.    Elapsed: 0:33:16.\n",
            "  Batch 1,440  of  7,606.    Elapsed: 0:34:13.\n",
            "  Batch 1,480  of  7,606.    Elapsed: 0:35:10.\n",
            "  Batch 1,520  of  7,606.    Elapsed: 0:36:07.\n",
            "  Batch 1,560  of  7,606.    Elapsed: 0:37:04.\n",
            "  Batch 1,600  of  7,606.    Elapsed: 0:38:01.\n",
            "  Batch 1,640  of  7,606.    Elapsed: 0:38:58.\n",
            "  Batch 1,680  of  7,606.    Elapsed: 0:39:55.\n",
            "  Batch 1,720  of  7,606.    Elapsed: 0:40:52.\n",
            "  Batch 1,760  of  7,606.    Elapsed: 0:41:49.\n",
            "  Batch 1,800  of  7,606.    Elapsed: 0:42:46.\n",
            "  Batch 1,840  of  7,606.    Elapsed: 0:43:43.\n",
            "  Batch 1,880  of  7,606.    Elapsed: 0:44:40.\n",
            "  Batch 1,920  of  7,606.    Elapsed: 0:45:37.\n",
            "  Batch 1,960  of  7,606.    Elapsed: 0:46:34.\n",
            "  Batch 2,000  of  7,606.    Elapsed: 0:47:31.\n",
            "  Batch 2,040  of  7,606.    Elapsed: 0:48:28.\n",
            "  Batch 2,080  of  7,606.    Elapsed: 0:49:25.\n",
            "  Batch 2,120  of  7,606.    Elapsed: 0:50:22.\n",
            "  Batch 2,160  of  7,606.    Elapsed: 0:51:19.\n",
            "  Batch 2,200  of  7,606.    Elapsed: 0:52:16.\n",
            "  Batch 2,240  of  7,606.    Elapsed: 0:53:13.\n",
            "  Batch 2,280  of  7,606.    Elapsed: 0:54:10.\n",
            "  Batch 2,320  of  7,606.    Elapsed: 0:55:07.\n",
            "  Batch 2,360  of  7,606.    Elapsed: 0:56:04.\n",
            "  Batch 2,400  of  7,606.    Elapsed: 0:57:01.\n",
            "  Batch 2,440  of  7,606.    Elapsed: 0:57:58.\n",
            "  Batch 2,480  of  7,606.    Elapsed: 0:58:55.\n",
            "  Batch 2,520  of  7,606.    Elapsed: 0:59:52.\n",
            "  Batch 2,560  of  7,606.    Elapsed: 1:00:49.\n",
            "  Batch 2,600  of  7,606.    Elapsed: 1:01:46.\n",
            "  Batch 2,640  of  7,606.    Elapsed: 1:02:43.\n",
            "  Batch 2,680  of  7,606.    Elapsed: 1:03:40.\n",
            "  Batch 2,720  of  7,606.    Elapsed: 1:04:37.\n",
            "  Batch 2,760  of  7,606.    Elapsed: 1:05:34.\n",
            "  Batch 2,800  of  7,606.    Elapsed: 1:06:31.\n",
            "  Batch 2,840  of  7,606.    Elapsed: 1:07:28.\n",
            "  Batch 2,880  of  7,606.    Elapsed: 1:08:25.\n",
            "  Batch 2,920  of  7,606.    Elapsed: 1:09:22.\n",
            "  Batch 2,960  of  7,606.    Elapsed: 1:10:19.\n",
            "  Batch 3,000  of  7,606.    Elapsed: 1:11:16.\n",
            "  Batch 3,040  of  7,606.    Elapsed: 1:12:13.\n",
            "  Batch 3,080  of  7,606.    Elapsed: 1:13:10.\n",
            "  Batch 3,120  of  7,606.    Elapsed: 1:14:07.\n",
            "  Batch 3,160  of  7,606.    Elapsed: 1:15:04.\n",
            "  Batch 3,200  of  7,606.    Elapsed: 1:16:01.\n",
            "  Batch 3,240  of  7,606.    Elapsed: 1:16:58.\n",
            "  Batch 3,280  of  7,606.    Elapsed: 1:17:55.\n",
            "  Batch 3,320  of  7,606.    Elapsed: 1:18:52.\n",
            "  Batch 3,360  of  7,606.    Elapsed: 1:19:49.\n",
            "  Batch 3,400  of  7,606.    Elapsed: 1:20:46.\n",
            "  Batch 3,440  of  7,606.    Elapsed: 1:21:43.\n",
            "  Batch 3,480  of  7,606.    Elapsed: 1:22:40.\n",
            "  Batch 3,520  of  7,606.    Elapsed: 1:23:37.\n",
            "  Batch 3,560  of  7,606.    Elapsed: 1:24:34.\n",
            "  Batch 3,600  of  7,606.    Elapsed: 1:25:31.\n",
            "  Batch 3,640  of  7,606.    Elapsed: 1:26:28.\n",
            "  Batch 3,680  of  7,606.    Elapsed: 1:27:25.\n",
            "  Batch 3,720  of  7,606.    Elapsed: 1:28:23.\n",
            "  Batch 3,760  of  7,606.    Elapsed: 1:29:20.\n",
            "  Batch 3,800  of  7,606.    Elapsed: 1:30:17.\n",
            "  Batch 3,840  of  7,606.    Elapsed: 1:31:14.\n",
            "  Batch 3,880  of  7,606.    Elapsed: 1:32:11.\n"
          ]
        }
      ],
      "source": [
        "#Using class weight of cross entropy loss\n",
        "from sklearn.metrics import classification_report,auc,confusion_matrix,f1_score,precision_recall_curve,plot_precision_recall_curve,matthews_corrcoef\n",
        "import random\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "scaler = GradScaler()\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    roberta_model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        roberta_model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        with autocast():\n",
        "            loss, logits = roberta_model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "            b_labels1=torch.nn.functional.one_hot(b_labels, num_classes=2)\n",
        "            #Calculating weights\n",
        "            #positive=torch.sum(b_labels1, dim=0)\n",
        "           # negative=len(b_labels1)-positive\n",
        "            #negative\n",
        "            #pos_weight  = positive / negative\n",
        "            #criterion.pos_weight = pos_weight\n",
        "            loss1 = cross_entropy(logits,b_labels).to(device)\n",
        "           # print(\"loss:\",loss1)\n",
        "            loss1 = loss1 / gradient_accumulations\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss1.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        scaler.scale(loss1).backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "       # torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm*scaler.get_scale())\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        if ((step + 1) % gradient_accumulations == 0):\n",
        "             scaler.step(optimizer)\n",
        "       # Updates the scale for next iteration.\n",
        "             scaler.update()\n",
        "        # Update the learning rate.\n",
        "             scheduler.step()       \n",
        "             optimizer.zero_grad()\n",
        "            # roberta_model.zero_grad()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    roberta_model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    total_f1_score =0\n",
        "    predlist=[]\n",
        "    lbllist=[]\n",
        "    total_logits=[]\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            (loss, logits) = roberta_model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            #Converting the labels to one hot to sync with same shape as logits\n",
        "            b_labels1=torch.nn.functional.one_hot(b_labels, num_classes=2)\n",
        "            loss1 = cross_entropy1(logits, b_labels)\n",
        "       # print(\"loss1:\",loss1)\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss1.item()\n",
        "\n",
        "         #Converting for predictions by applying sigmoid to logits\n",
        "        pred_logits_sigmoid=torch.sigmoid(logits)\n",
        "        y_pred=torch.round(pred_logits_sigmoid)\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits_pred = y_pred.detach().cpu().numpy()\n",
        "        label_ids1 = b_labels.to('cpu').numpy()\n",
        "        logits=logits.detach().cpu().numpy()\n",
        "        #For confusion matrix and classification report to work we need same dimensions.\n",
        "        label_ids = b_labels1.to('cpu').numpy()\n",
        "        pred_logits_sigmoid = pred_logits_sigmoid.detach().cpu().numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids1)\n",
        "\n",
        "         #print(predictions)\n",
        "        predictions=np.argmax(logits_pred, axis=1)\n",
        "        y_test=np.argmax(label_ids,axis=1)\n",
        "        predlist.extend(predictions)\n",
        "        lbllist.extend(y_test)\n",
        "        #Accumulating the sigmoid positive logits for precision recall curve\n",
        "        total_logits.extend(pred_logits_sigmoid[:,1])\n",
        "        total_f1_score += f1_score(predictions,y_test, average = 'macro')\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    #f1 score\n",
        "\n",
        "    avg_f1_score =total_f1_score/len(validation_dataloader)\n",
        "    print(\"  F1 score: {0:.2f}\".format(avg_f1_score))\n",
        "\n",
        "     #classification report\n",
        "    print(classification_report(lbllist, predlist))  \n",
        "\n",
        "    #confusion matrix\n",
        "    cm = confusion_matrix(lbllist,predlist)\n",
        "    # constant for classes\n",
        "    print(cm)\n",
        "    #mcc score\n",
        "    print(\"Mathews Correlation coefficient score for this epoch is :\",round(matthews_corrcoef(lbllist, predlist),2))\n",
        "    #Precision recall curve plot\n",
        "    lr_precision, lr_recall, thresholds = precision_recall_curve(lbllist,total_logits)\n",
        "    lr_f1, lr_auc = f1_score( lbllist,predlist), auc(lr_recall, lr_precision)\n",
        "    print('Model validation score: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
        "    baseline = lbllist.count(1) / len(lbllist)\n",
        "    plt.plot([0, 1], [baseline, baseline], linestyle='--', label='baseline')\n",
        "    plt.plot(lr_recall, lr_precision, marker='.', label='Roberta model evaluation')\n",
        "    # axis labels\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    # show the legend\n",
        "    plt.legend()\n",
        "    # show the plot\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuyWXQKcnqjk",
        "outputId": "ebbd3136-33ac-4612-bd7b-1693234f7787"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1]"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predlist=[]\n",
        "predlist.extend(predictions)\n",
        "predlist\n",
        "#predlist.(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06PH50GKgou1",
        "outputId": "84750f23-3deb-4cb7-eb1c-2a7a86df054c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 1],\n",
              "       [0, 0]])"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "pred=[]\n",
        "label =[]\n",
        "label.append(y_test)\n",
        "#label = [a.squeeze().tolist() for a in label]\n",
        "label1=np.argmax(label,axis=1)\n",
        "pred.append(predictions)\n",
        "#pred = [a.squeeze().tolist() for a in pred]\n",
        "pred=np.argmax(pred,axis=1)\n",
        "confusion_matrix(label1, pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLzESgGsE-LM"
      },
      "outputs": [],
      "source": [
        "del roberta_model\n",
        "del optimizer\n",
        "del scheduler\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DirM6N22kFDX"
      },
      "source": [
        "XLNET"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import XLNetConfig,AutoConfig\n",
        "\n",
        "configuration = XLNetConfig.from_pretrained('xlnet-base-cased',output_attentions=False,output_hidden_states=False,num_labels=2)\n",
        "configuration.hidden_dropout_prob = 0.2\n",
        "configuration.attention_probs_dropout_prob = 0.2"
      ],
      "metadata": {
        "id": "c7bK06XArKFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBf_YlMvkIHx",
        "outputId": "258e3fe1-a632-461f-c8bd-f28248e18b65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "WARNING:transformers.modeling_utils:Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Base model s loaded\n"
          ]
        }
      ],
      "source": [
        "#XLNET\n",
        "xlnet_model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", # 12-layer, 768-hidden, 12-heads, 125M parameters RoBERTa using the BERT-base architecture\n",
        "                                                                   # num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                                                                                    # You can increase this for multi-class tasks.   \n",
        "                                                                   # output_attentions = False, # Whether the model returns attentions weights.\n",
        "                                                                  #  output_hidden_states = False # Whether the model returns all hidden-states.\n",
        "                                                              config =configuration\n",
        "                                                                )\n",
        "xlnet_tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "xlnet_model.cuda()\n",
        "\n",
        "\n",
        "print(' Base model s loaded')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfJMRQAUlTfl",
        "outputId": "67570449-2b4e-4a4e-8ad7-0984365f54d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized XLNET:  ['▁drinks', '▁were', '▁bad', '▁', ',', '▁the', '▁hot', '▁chocolate', '▁was', '▁water', 'ed', '▁down', '▁and', '▁the', '▁la', 'tte', '▁had', '▁a', '▁burnt', '▁taste', '▁to', '▁it', '▁', '.', '▁the', '▁food', '▁was', '▁also', '▁poor', '▁quality', '▁', ',', '▁but', '▁the', '▁service', '▁was', '▁the', '▁worst', '▁part', '▁', ',', '▁their', '▁', 'cashier', '▁was', '▁very', '▁rude', '▁', '.']\n",
            "Token IDs XLNET:  [7841, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 775, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2151, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Print the text split into tokens.\n",
        "print('Tokenized XLNET: ', xlnet_tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the text mapped to token ids.\n",
        "print('Token IDs XLNET: ', xlnet_tokenizer.convert_tokens_to_ids(roberta_tokenizer.tokenize(sentences[0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7kvumvMlZaq",
        "outputId": "c0ccd681-0d4b-4ae6-9c34-9292b6d94aea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Will Your Hometown Be Taking In Obama’s Refugees? Here’s The List Of Cities Where They’re Being Transplanted 🏙️\n",
            "XLNET: ['▁Will', '▁Your', '▁Home', 'town', '▁Be', '▁Taking', '▁In', '▁Obama', '’', 's', '▁Refugees', '?', '▁Here', '’', 's', '▁The', '▁List', '▁Of', '▁Cities', '▁Where', '▁They', '’', 're', '▁Being', '▁Trans', 'plant', 'ed', '▁', '🏙️']\n"
          ]
        }
      ],
      "source": [
        "sequence = \"\"\"Will Your Hometown Be Taking In Obama’s Refugees? Here’s The List Of Cities Where They’re Being Transplanted 🏙️\"\"\"\n",
        "print(\"\"\"Will Your Hometown Be Taking In Obama’s Refugees? Here’s The List Of Cities Where They’re Being Transplanted 🏙️\"\"\")\n",
        "xlnet_tokenized_sequence=xlnet_tokenizer.tokenize(sequence)\n",
        "print(\"XLNET:\",xlnet_tokenized_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ThjNujalnC2",
        "outputId": "ac29329e-1ab5-4441-e9ee-67f18c0aae05"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('<pad>', 5)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "xlnet_tokenizer.pad_token,xlnet_tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTB5F8pklx_5"
      },
      "outputs": [],
      "source": [
        "token_lens = []\n",
        "\n",
        "for txt in sentences:\n",
        "    tokens = xlnet_tokenizer.encode(txt,truncation=True, max_length=512)\n",
        "    token_lens.append(len(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "4v-TZz0Suehl",
        "outputId": "a1dab50e-abe6-4013-c91d-80161eec972e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.histplot(token_lens)\n",
        "plt.xlim([0, 600]);\n",
        "plt.xlabel('Token count');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcfEjGZo6KSE",
        "outputId": "8e66ef94-888c-4e05-a647-9bf021c534a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  this was the worst experience i ve ever had a casual coffee light fare place . the server disappeared for minutes , just talking to his friend by the window as my girlfriend and i sat dumbfounded that this dude had the nerve to do that on the job . we re trying to make eye contact , but clearly getting paid to talk to his bud was more important to him . my girlfriend went up to the counter once the server disappeared into the back for another minutes what is this guy doing ? and asked if she should order food up there or something . the girl at the counter gives her a weird look and just says i ll get your server . when they arrive from the back , they look over at our table and have a laugh . yeah , leaving us hanging for half a goddamn hour at a place with only two other customers is not funny but in retrospect , your collective incompetence and false sense of entitlement certainly was . the food was okay . for a place called toast , i d figured the bread would be better , but it was just cold le bus . additionally , i m sure the andouille in my special was just a link of the pre packaged offering from trader joe s cut into four pieces . for unapologetic mediocrity will not be happening again . avoid this place like the plague . i almost didn t leave a tip , and honestly i shouldn t have . i felt the buyer s remorse all day . what a disgrace .\n",
            "Token IDs XLNET: tensor([   52,    30,    18,  2598,   656,    17,   150,    17,   189,   545,\n",
            "           54,    24, 10245,  2877,   697, 12040,   250,    17,     9,    18,\n",
            "         3441,  6239,    28,   641,    17,    19,   125,  1792,    22,    45,\n",
            "         1233,    37,    18,  2078,    34,    94,  8224,    21,    17,   150,\n",
            "         2162, 14240, 13538,    29,    52, 23196,    54,    18,  9629,    22,\n",
            "          112,    29,    31,    18,   625,    17,     9,    80,    17,    88,\n",
            "          619,    22,   144,  1715,  1056,    17,    19,    57,  2379,   723,\n",
            "         1373,    22,  1034,    22,    45,    17, 13352,    30,    70,   400,\n",
            "           22,   103,    17,     9,    94,  8224,   388,    76,    22,    18,\n",
            "         2610,   497,    18,  3441,  6239,    91,    18,   126,    28,   245,\n",
            "          641,   113,    27,    52,  2035,   690,    17,    82,    21,   442,\n",
            "          108,    85,   170,   374,   626,    76,   105,    49,   359,    17,\n",
            "            9,    18,  1615,    38,    18,  2610,  1849,    62,    24,  8189,\n",
            "          338,    21,   125,   349,    17,   150,    17,   215,   133,    73,\n",
            "         3441,    17,     9,    90,    63,  4623,    40,    18,   126,    17,\n",
            "           19,    63,   338,    95,    38,   120,  1324,    21,    47,    24,\n",
            "         5361,    17,     9, 11752,    17,    19,  1511,   211,  6020,    28,\n",
            "          455,    24, 30615,  1671,    38,    24,   250,    33,   114,    87,\n",
            "           86,  1391,    27,    50,  5787,    57,    25, 31601,    17,    19,\n",
            "           73,  5765, 29839,    21,  4417,  1186,    20, 25614,  2209,    30,\n",
            "           17,     9,    18,   626,    30,  4968,    17,     9,    28,    24,\n",
            "          250,   271, 15294,    17,    19,    17,   150,    17,    66,  8047,\n",
            "           18,  6111,    74,    39,   352,    17,    19,    57,    36,    30,\n",
            "          125,  1946,    17,   529,  2400,    17,     9, 21314,    17,    19,\n",
            "           17,   150,    17,    98,   512,    18,    21,  3498,  7587,    25,\n",
            "           94,   632,    30,   125,    24,  1730,    20,    18,     4,     3])\n",
            "labels: tensor([0, 0, 0,  ..., 1, 1, 1])\n"
          ]
        }
      ],
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "xlnet_input_ids =[]\n",
        "xlnet_attention_masks=[]\n",
        "sentence_ids = []\n",
        "counter = 0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    xlnet_encoded_dict = xlnet_tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 260,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt', \n",
        "                        truncation = True    # Return pytorch tensors.\n",
        "                   )\n",
        "    # Add the encoded sentence to the list.    \n",
        "    xlnet_input_ids.append(xlnet_encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    xlnet_attention_masks.append(xlnet_encoded_dict['attention_mask'])\n",
        "    \n",
        "    # collecting sentence_ids\n",
        "    sentence_ids.append(counter)\n",
        "    counter  = counter + 1\n",
        "    \n",
        "    \n",
        "    \n",
        "# Convert the lists into tensors.\n",
        "\n",
        "xlnet_input_ids = torch.cat(xlnet_input_ids, dim=0)\n",
        "xlnet_attention_masks = torch.cat(xlnet_attention_masks, dim=0)\n",
        "labels = torch.tensor(df_review_merge.label.values)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[1])\n",
        "print('Token IDs XLNET:', xlnet_input_ids[1])\n",
        "print('labels:', labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlvAh90Xpw34"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "# function to seed the script globally\n",
        "torch.manual_seed(1)\n",
        "xlnet_dataset = TensorDataset(xlnet_input_ids, xlnet_attention_masks, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNg983NXqDu0",
        "outputId": "7327a5c5-73e6-4350-be32-2a40cdd2e31f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "486,766 training samples\n",
            "121,692 validation samples\n"
          ]
        }
      ],
      "source": [
        "# Create a 80-20 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.8 * len(xlnet_dataset))\n",
        "val_size = len(xlnet_dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(xlnet_dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ldu4yTtcqWqM"
      },
      "outputs": [],
      "source": [
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXmjY2t9qeoU",
        "outputId": "4353cd4b-524a-47f6-c103-13fb3ecb565f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({1: 422366, 0: 64400})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "import collections as c\n",
        "train_classes = [df_review_merge.label[i] for i in train_dataset.indices]\n",
        "c.Counter(train_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uwAqFPhqnjH",
        "outputId": "81dc8e0f-a753-46b4-e865-be078c4f8799"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 64400 422366]\n",
            "[1.55279503e-05 2.36761482e-06]\n",
            "tensor([2.3676e-06, 2.3676e-06, 2.3676e-06,  ..., 2.3676e-06, 2.3676e-06,\n",
            "        2.3676e-06], dtype=torch.float64)\n",
            "38322\n",
            "<torch.utils.data.sampler.WeightedRandomSampler object at 0x7fec7e0d7310>\n"
          ]
        }
      ],
      "source": [
        "#Using weighted random sampler for class imbalance\n",
        "import numpy as np \n",
        "\n",
        "y_train_indices = train_dataset.indices\n",
        "\n",
        "y_train = [target[i] for i in y_train_indices]\n",
        "\n",
        "class_sample_count = np.array(\n",
        "    [len(np.where(y_train == t)[0]) for t in np.unique(y_train)])\n",
        "print(class_sample_count)\n",
        "weight = 1. / (class_sample_count)\n",
        "print(weight)\n",
        "samples_weight = np.array([weight[t] for t in y_train])\n",
        "samples_weight = torch.from_numpy(samples_weight)\n",
        "print(samples_weight)\n",
        "sampler = torch.utils.data.sampler.WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
        "print(next(iter(sampler)))\n",
        "print(sampler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5C4djohUqwmQ"
      },
      "outputs": [],
      "source": [
        "#Run if we are using weighted random sampler\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32 or 64.\n",
        "batch_size = 32\n",
        "gradient_accumulations=10\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = sampler, # Select batches randomly\n",
        "            batch_size = batch_size, # Trains with this batch size.\n",
        "            num_workers=2,\n",
        "            pin_memory = True\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size, # Evaluate with this batch size.\n",
        "            num_workers =2,\n",
        "            pin_memory = True\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJmvIxfaq524",
        "outputId": "20b343a6-6fb1-476e-d45d-a749aa743946"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels batch shape: <built-in method values of Tensor object at 0x7febff2c9e90>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([14, 18])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "#Testing the label counts in training data after applying weighted random sampler\n",
        "import collections as c\n",
        "train_labels = next(iter(train_dataloader))\n",
        "#print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels[0].values}\")\n",
        "#img = train_features[0].squeeze()\n",
        "label = train_labels[2]\n",
        "torch.bincount(label)\n",
        "#print(label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lSBpVVeOy28",
        "outputId": "9d760163-21ff-4029-ec43-d094eb3cd052"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15212"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "len(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmcVziFhrHe8",
        "outputId": "996550ea-df91-4362-f08b-6d4f1a6b05c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Xlnet model has 210 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "transformer.mask_emb                                     (1, 1, 768)\n",
            "transformer.word_embedding.weight                       (32000, 768)\n",
            "transformer.layer.0.rel_attn.q                          (768, 12, 64)\n",
            "transformer.layer.0.rel_attn.k                          (768, 12, 64)\n",
            "transformer.layer.0.rel_attn.v                          (768, 12, 64)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "transformer.layer.0.rel_attn.o                          (768, 12, 64)\n",
            "transformer.layer.0.rel_attn.r                          (768, 12, 64)\n",
            "transformer.layer.0.rel_attn.r_r_bias                       (12, 64)\n",
            "transformer.layer.0.rel_attn.r_s_bias                       (12, 64)\n",
            "transformer.layer.0.rel_attn.r_w_bias                       (12, 64)\n",
            "transformer.layer.0.rel_attn.seg_embed                   (2, 12, 64)\n",
            "transformer.layer.0.rel_attn.layer_norm.weight                (768,)\n",
            "transformer.layer.0.rel_attn.layer_norm.bias                  (768,)\n",
            "transformer.layer.0.ff.layer_norm.weight                      (768,)\n",
            "transformer.layer.0.ff.layer_norm.bias                        (768,)\n",
            "transformer.layer.0.ff.layer_1.weight                    (3072, 768)\n",
            "transformer.layer.0.ff.layer_1.bias                          (3072,)\n",
            "transformer.layer.0.ff.layer_2.weight                    (768, 3072)\n",
            "transformer.layer.0.ff.layer_2.bias                           (768,)\n",
            "transformer.layer.1.rel_attn.q                          (768, 12, 64)\n",
            "transformer.layer.1.rel_attn.k                          (768, 12, 64)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "sequence_summary.summary.weight                           (768, 768)\n",
            "sequence_summary.summary.bias                                 (768,)\n",
            "logits_proj.weight                                          (2, 768)\n",
            "logits_proj.bias                                                (2,)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(xlnet_model.named_parameters())\n",
        "\n",
        "print('The Xlnet model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NDzmXICrSgQ"
      },
      "outputs": [],
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in xlnet_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in xlnet_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5, eps = 1e-8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ry-Ux449rdN4"
      },
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "epochs = 1\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-w7dzl6Irudn",
        "outputId": "024152e5-70e0-4f57-a054-2eef6a1782a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 1 ========\n",
            "Training...\n",
            "  Batch    40  of  15,212.    Elapsed: 0:00:53.\n",
            "  Batch    80  of  15,212.    Elapsed: 0:01:45.\n",
            "  Batch   120  of  15,212.    Elapsed: 0:02:37.\n",
            "  Batch   160  of  15,212.    Elapsed: 0:03:28.\n",
            "  Batch   200  of  15,212.    Elapsed: 0:04:20.\n",
            "  Batch   240  of  15,212.    Elapsed: 0:05:12.\n",
            "  Batch   280  of  15,212.    Elapsed: 0:06:04.\n",
            "  Batch   320  of  15,212.    Elapsed: 0:06:56.\n",
            "  Batch   360  of  15,212.    Elapsed: 0:07:47.\n",
            "  Batch   400  of  15,212.    Elapsed: 0:08:39.\n",
            "  Batch   440  of  15,212.    Elapsed: 0:09:31.\n",
            "  Batch   480  of  15,212.    Elapsed: 0:10:23.\n",
            "  Batch   520  of  15,212.    Elapsed: 0:11:15.\n",
            "  Batch   560  of  15,212.    Elapsed: 0:12:06.\n",
            "  Batch   600  of  15,212.    Elapsed: 0:12:58.\n",
            "  Batch   640  of  15,212.    Elapsed: 0:13:50.\n",
            "  Batch   680  of  15,212.    Elapsed: 0:14:42.\n",
            "  Batch   720  of  15,212.    Elapsed: 0:15:34.\n",
            "  Batch   760  of  15,212.    Elapsed: 0:16:25.\n",
            "  Batch   800  of  15,212.    Elapsed: 0:17:17.\n",
            "  Batch   840  of  15,212.    Elapsed: 0:18:09.\n",
            "  Batch   880  of  15,212.    Elapsed: 0:19:01.\n",
            "  Batch   920  of  15,212.    Elapsed: 0:19:53.\n",
            "  Batch   960  of  15,212.    Elapsed: 0:20:44.\n",
            "  Batch 1,000  of  15,212.    Elapsed: 0:21:36.\n",
            "  Batch 1,040  of  15,212.    Elapsed: 0:22:28.\n",
            "  Batch 1,080  of  15,212.    Elapsed: 0:23:20.\n",
            "  Batch 1,120  of  15,212.    Elapsed: 0:24:12.\n",
            "  Batch 1,160  of  15,212.    Elapsed: 0:25:04.\n",
            "  Batch 1,200  of  15,212.    Elapsed: 0:25:55.\n",
            "  Batch 1,240  of  15,212.    Elapsed: 0:26:47.\n",
            "  Batch 1,280  of  15,212.    Elapsed: 0:27:39.\n",
            "  Batch 1,320  of  15,212.    Elapsed: 0:28:31.\n",
            "  Batch 1,360  of  15,212.    Elapsed: 0:29:23.\n",
            "  Batch 1,400  of  15,212.    Elapsed: 0:30:15.\n",
            "  Batch 1,440  of  15,212.    Elapsed: 0:31:06.\n",
            "  Batch 1,480  of  15,212.    Elapsed: 0:31:58.\n",
            "  Batch 1,520  of  15,212.    Elapsed: 0:32:50.\n",
            "  Batch 1,560  of  15,212.    Elapsed: 0:33:42.\n",
            "  Batch 1,600  of  15,212.    Elapsed: 0:34:34.\n",
            "  Batch 1,640  of  15,212.    Elapsed: 0:35:25.\n",
            "  Batch 1,680  of  15,212.    Elapsed: 0:36:17.\n",
            "  Batch 1,720  of  15,212.    Elapsed: 0:37:09.\n",
            "  Batch 1,760  of  15,212.    Elapsed: 0:38:01.\n",
            "  Batch 1,800  of  15,212.    Elapsed: 0:38:53.\n",
            "  Batch 1,840  of  15,212.    Elapsed: 0:39:44.\n",
            "  Batch 1,880  of  15,212.    Elapsed: 0:40:36.\n",
            "  Batch 1,920  of  15,212.    Elapsed: 0:41:28.\n",
            "  Batch 1,960  of  15,212.    Elapsed: 0:42:20.\n",
            "  Batch 2,000  of  15,212.    Elapsed: 0:43:12.\n",
            "  Batch 2,040  of  15,212.    Elapsed: 0:44:03.\n",
            "  Batch 2,080  of  15,212.    Elapsed: 0:44:55.\n",
            "  Batch 2,120  of  15,212.    Elapsed: 0:45:47.\n",
            "  Batch 2,160  of  15,212.    Elapsed: 0:46:39.\n",
            "  Batch 2,200  of  15,212.    Elapsed: 0:47:31.\n",
            "  Batch 2,240  of  15,212.    Elapsed: 0:48:22.\n",
            "  Batch 2,280  of  15,212.    Elapsed: 0:49:14.\n",
            "  Batch 2,320  of  15,212.    Elapsed: 0:50:06.\n",
            "  Batch 2,360  of  15,212.    Elapsed: 0:50:58.\n",
            "  Batch 2,400  of  15,212.    Elapsed: 0:51:50.\n",
            "  Batch 2,440  of  15,212.    Elapsed: 0:52:41.\n",
            "  Batch 2,480  of  15,212.    Elapsed: 0:53:33.\n",
            "  Batch 2,520  of  15,212.    Elapsed: 0:54:25.\n",
            "  Batch 2,560  of  15,212.    Elapsed: 0:55:17.\n",
            "  Batch 2,600  of  15,212.    Elapsed: 0:56:08.\n",
            "  Batch 2,640  of  15,212.    Elapsed: 0:57:00.\n",
            "  Batch 2,680  of  15,212.    Elapsed: 0:57:52.\n",
            "  Batch 2,720  of  15,212.    Elapsed: 0:58:44.\n",
            "  Batch 2,760  of  15,212.    Elapsed: 0:59:36.\n",
            "  Batch 2,800  of  15,212.    Elapsed: 1:00:27.\n",
            "  Batch 2,840  of  15,212.    Elapsed: 1:01:19.\n",
            "  Batch 2,880  of  15,212.    Elapsed: 1:02:11.\n",
            "  Batch 2,920  of  15,212.    Elapsed: 1:03:03.\n",
            "  Batch 2,960  of  15,212.    Elapsed: 1:03:54.\n",
            "  Batch 3,000  of  15,212.    Elapsed: 1:04:46.\n",
            "  Batch 3,040  of  15,212.    Elapsed: 1:05:38.\n",
            "  Batch 3,080  of  15,212.    Elapsed: 1:06:30.\n",
            "  Batch 3,120  of  15,212.    Elapsed: 1:07:22.\n",
            "  Batch 3,160  of  15,212.    Elapsed: 1:08:13.\n",
            "  Batch 3,200  of  15,212.    Elapsed: 1:09:05.\n",
            "  Batch 3,240  of  15,212.    Elapsed: 1:09:57.\n",
            "  Batch 3,280  of  15,212.    Elapsed: 1:10:49.\n",
            "  Batch 3,320  of  15,212.    Elapsed: 1:11:41.\n",
            "  Batch 3,360  of  15,212.    Elapsed: 1:12:32.\n",
            "  Batch 3,400  of  15,212.    Elapsed: 1:13:24.\n",
            "  Batch 3,440  of  15,212.    Elapsed: 1:14:16.\n",
            "  Batch 3,480  of  15,212.    Elapsed: 1:15:08.\n",
            "  Batch 3,520  of  15,212.    Elapsed: 1:15:59.\n",
            "  Batch 3,560  of  15,212.    Elapsed: 1:16:51.\n",
            "  Batch 3,600  of  15,212.    Elapsed: 1:17:43.\n",
            "  Batch 3,640  of  15,212.    Elapsed: 1:18:35.\n",
            "  Batch 3,680  of  15,212.    Elapsed: 1:19:27.\n",
            "  Batch 3,720  of  15,212.    Elapsed: 1:20:18.\n",
            "  Batch 3,760  of  15,212.    Elapsed: 1:21:10.\n",
            "  Batch 3,800  of  15,212.    Elapsed: 1:22:02.\n",
            "  Batch 3,840  of  15,212.    Elapsed: 1:22:54.\n",
            "  Batch 3,880  of  15,212.    Elapsed: 1:23:46.\n",
            "  Batch 3,920  of  15,212.    Elapsed: 1:24:37.\n",
            "  Batch 3,960  of  15,212.    Elapsed: 1:25:29.\n",
            "  Batch 4,000  of  15,212.    Elapsed: 1:26:21.\n",
            "  Batch 4,040  of  15,212.    Elapsed: 1:27:13.\n",
            "  Batch 4,080  of  15,212.    Elapsed: 1:28:05.\n",
            "  Batch 4,120  of  15,212.    Elapsed: 1:28:56.\n",
            "  Batch 4,160  of  15,212.    Elapsed: 1:29:48.\n",
            "  Batch 4,200  of  15,212.    Elapsed: 1:30:40.\n",
            "  Batch 4,240  of  15,212.    Elapsed: 1:31:32.\n",
            "  Batch 4,280  of  15,212.    Elapsed: 1:32:24.\n",
            "  Batch 4,320  of  15,212.    Elapsed: 1:33:15.\n",
            "  Batch 4,360  of  15,212.    Elapsed: 1:34:07.\n",
            "  Batch 4,400  of  15,212.    Elapsed: 1:34:59.\n",
            "  Batch 4,440  of  15,212.    Elapsed: 1:35:51.\n",
            "  Batch 4,480  of  15,212.    Elapsed: 1:36:43.\n",
            "  Batch 4,520  of  15,212.    Elapsed: 1:37:34.\n",
            "  Batch 4,560  of  15,212.    Elapsed: 1:38:26.\n",
            "  Batch 4,600  of  15,212.    Elapsed: 1:39:18.\n",
            "  Batch 4,640  of  15,212.    Elapsed: 1:40:10.\n",
            "  Batch 4,680  of  15,212.    Elapsed: 1:41:01.\n",
            "  Batch 4,720  of  15,212.    Elapsed: 1:41:53.\n",
            "  Batch 4,760  of  15,212.    Elapsed: 1:42:45.\n",
            "  Batch 4,800  of  15,212.    Elapsed: 1:43:37.\n",
            "  Batch 4,840  of  15,212.    Elapsed: 1:44:29.\n",
            "  Batch 4,880  of  15,212.    Elapsed: 1:45:20.\n",
            "  Batch 4,920  of  15,212.    Elapsed: 1:46:12.\n",
            "  Batch 4,960  of  15,212.    Elapsed: 1:47:04.\n",
            "  Batch 5,000  of  15,212.    Elapsed: 1:47:56.\n",
            "  Batch 5,040  of  15,212.    Elapsed: 1:48:48.\n",
            "  Batch 5,080  of  15,212.    Elapsed: 1:49:39.\n",
            "  Batch 5,120  of  15,212.    Elapsed: 1:50:31.\n",
            "  Batch 5,160  of  15,212.    Elapsed: 1:51:23.\n",
            "  Batch 5,200  of  15,212.    Elapsed: 1:52:15.\n",
            "  Batch 5,240  of  15,212.    Elapsed: 1:53:06.\n",
            "  Batch 5,280  of  15,212.    Elapsed: 1:53:58.\n",
            "  Batch 5,320  of  15,212.    Elapsed: 1:54:50.\n",
            "  Batch 5,360  of  15,212.    Elapsed: 1:55:42.\n",
            "  Batch 5,400  of  15,212.    Elapsed: 1:56:33.\n",
            "  Batch 5,440  of  15,212.    Elapsed: 1:57:25.\n",
            "  Batch 5,480  of  15,212.    Elapsed: 1:58:17.\n",
            "  Batch 5,520  of  15,212.    Elapsed: 1:59:09.\n",
            "  Batch 5,560  of  15,212.    Elapsed: 2:00:01.\n",
            "  Batch 5,600  of  15,212.    Elapsed: 2:00:53.\n",
            "  Batch 5,640  of  15,212.    Elapsed: 2:01:44.\n",
            "  Batch 5,680  of  15,212.    Elapsed: 2:02:36.\n",
            "  Batch 5,720  of  15,212.    Elapsed: 2:03:28.\n",
            "  Batch 5,760  of  15,212.    Elapsed: 2:04:20.\n",
            "  Batch 5,800  of  15,212.    Elapsed: 2:05:11.\n",
            "  Batch 5,840  of  15,212.    Elapsed: 2:06:03.\n",
            "  Batch 5,880  of  15,212.    Elapsed: 2:06:55.\n",
            "  Batch 5,920  of  15,212.    Elapsed: 2:07:47.\n",
            "  Batch 5,960  of  15,212.    Elapsed: 2:08:39.\n",
            "  Batch 6,000  of  15,212.    Elapsed: 2:09:30.\n",
            "  Batch 6,040  of  15,212.    Elapsed: 2:10:22.\n",
            "  Batch 6,080  of  15,212.    Elapsed: 2:11:14.\n",
            "  Batch 6,120  of  15,212.    Elapsed: 2:12:06.\n",
            "  Batch 6,160  of  15,212.    Elapsed: 2:12:58.\n",
            "  Batch 6,200  of  15,212.    Elapsed: 2:13:49.\n",
            "  Batch 6,240  of  15,212.    Elapsed: 2:14:41.\n",
            "  Batch 6,280  of  15,212.    Elapsed: 2:15:33.\n",
            "  Batch 6,320  of  15,212.    Elapsed: 2:16:25.\n",
            "  Batch 6,360  of  15,212.    Elapsed: 2:17:17.\n",
            "  Batch 6,400  of  15,212.    Elapsed: 2:18:08.\n",
            "  Batch 6,440  of  15,212.    Elapsed: 2:19:00.\n",
            "  Batch 6,480  of  15,212.    Elapsed: 2:19:52.\n",
            "  Batch 6,520  of  15,212.    Elapsed: 2:20:44.\n",
            "  Batch 6,560  of  15,212.    Elapsed: 2:21:36.\n",
            "  Batch 6,600  of  15,212.    Elapsed: 2:22:27.\n",
            "  Batch 6,640  of  15,212.    Elapsed: 2:23:19.\n",
            "  Batch 6,680  of  15,212.    Elapsed: 2:24:11.\n",
            "  Batch 6,720  of  15,212.    Elapsed: 2:25:03.\n",
            "  Batch 6,760  of  15,212.    Elapsed: 2:25:54.\n",
            "  Batch 6,800  of  15,212.    Elapsed: 2:26:46.\n",
            "  Batch 6,840  of  15,212.    Elapsed: 2:27:38.\n",
            "  Batch 6,880  of  15,212.    Elapsed: 2:28:30.\n",
            "  Batch 6,920  of  15,212.    Elapsed: 2:29:22.\n",
            "  Batch 6,960  of  15,212.    Elapsed: 2:30:14.\n",
            "  Batch 7,000  of  15,212.    Elapsed: 2:31:05.\n",
            "  Batch 7,040  of  15,212.    Elapsed: 2:31:57.\n",
            "  Batch 7,080  of  15,212.    Elapsed: 2:32:49.\n",
            "  Batch 7,120  of  15,212.    Elapsed: 2:33:41.\n",
            "  Batch 7,160  of  15,212.    Elapsed: 2:34:32.\n",
            "  Batch 7,200  of  15,212.    Elapsed: 2:35:24.\n",
            "  Batch 7,240  of  15,212.    Elapsed: 2:36:16.\n",
            "  Batch 7,280  of  15,212.    Elapsed: 2:37:08.\n",
            "  Batch 7,320  of  15,212.    Elapsed: 2:38:00.\n",
            "  Batch 7,360  of  15,212.    Elapsed: 2:38:51.\n",
            "  Batch 7,400  of  15,212.    Elapsed: 2:39:43.\n",
            "  Batch 7,440  of  15,212.    Elapsed: 2:40:35.\n",
            "  Batch 7,480  of  15,212.    Elapsed: 2:41:27.\n",
            "  Batch 7,520  of  15,212.    Elapsed: 2:42:19.\n",
            "  Batch 7,560  of  15,212.    Elapsed: 2:43:10.\n",
            "  Batch 7,600  of  15,212.    Elapsed: 2:44:02.\n",
            "  Batch 7,640  of  15,212.    Elapsed: 2:44:54.\n",
            "  Batch 7,680  of  15,212.    Elapsed: 2:45:46.\n",
            "  Batch 7,720  of  15,212.    Elapsed: 2:46:37.\n",
            "  Batch 7,760  of  15,212.    Elapsed: 2:47:29.\n",
            "  Batch 7,800  of  15,212.    Elapsed: 2:48:21.\n",
            "  Batch 7,840  of  15,212.    Elapsed: 2:49:13.\n",
            "  Batch 7,880  of  15,212.    Elapsed: 2:50:04.\n",
            "  Batch 7,920  of  15,212.    Elapsed: 2:50:56.\n",
            "  Batch 7,960  of  15,212.    Elapsed: 2:51:48.\n",
            "  Batch 8,000  of  15,212.    Elapsed: 2:52:40.\n",
            "  Batch 8,040  of  15,212.    Elapsed: 2:53:32.\n",
            "  Batch 8,080  of  15,212.    Elapsed: 2:54:23.\n",
            "  Batch 8,120  of  15,212.    Elapsed: 2:55:15.\n",
            "  Batch 8,160  of  15,212.    Elapsed: 2:56:07.\n",
            "  Batch 8,200  of  15,212.    Elapsed: 2:56:59.\n",
            "  Batch 8,240  of  15,212.    Elapsed: 2:57:51.\n",
            "  Batch 8,280  of  15,212.    Elapsed: 2:58:42.\n",
            "  Batch 8,320  of  15,212.    Elapsed: 2:59:34.\n",
            "  Batch 8,360  of  15,212.    Elapsed: 3:00:26.\n",
            "  Batch 8,400  of  15,212.    Elapsed: 3:01:18.\n",
            "  Batch 8,440  of  15,212.    Elapsed: 3:02:10.\n",
            "  Batch 8,480  of  15,212.    Elapsed: 3:03:01.\n",
            "  Batch 8,520  of  15,212.    Elapsed: 3:03:53.\n",
            "  Batch 8,560  of  15,212.    Elapsed: 3:04:45.\n",
            "  Batch 8,600  of  15,212.    Elapsed: 3:05:37.\n",
            "  Batch 8,640  of  15,212.    Elapsed: 3:06:29.\n",
            "  Batch 8,680  of  15,212.    Elapsed: 3:07:20.\n",
            "  Batch 8,720  of  15,212.    Elapsed: 3:08:12.\n",
            "  Batch 8,760  of  15,212.    Elapsed: 3:09:04.\n",
            "  Batch 8,800  of  15,212.    Elapsed: 3:09:56.\n",
            "  Batch 8,840  of  15,212.    Elapsed: 3:10:48.\n",
            "  Batch 8,880  of  15,212.    Elapsed: 3:11:39.\n",
            "  Batch 8,920  of  15,212.    Elapsed: 3:12:31.\n",
            "  Batch 8,960  of  15,212.    Elapsed: 3:13:23.\n",
            "  Batch 9,000  of  15,212.    Elapsed: 3:14:15.\n",
            "  Batch 9,040  of  15,212.    Elapsed: 3:15:06.\n",
            "  Batch 9,080  of  15,212.    Elapsed: 3:15:58.\n",
            "  Batch 9,120  of  15,212.    Elapsed: 3:16:50.\n",
            "  Batch 9,160  of  15,212.    Elapsed: 3:17:42.\n",
            "  Batch 9,200  of  15,212.    Elapsed: 3:18:34.\n",
            "  Batch 9,240  of  15,212.    Elapsed: 3:19:25.\n",
            "  Batch 9,280  of  15,212.    Elapsed: 3:20:17.\n",
            "  Batch 9,320  of  15,212.    Elapsed: 3:21:09.\n",
            "  Batch 9,360  of  15,212.    Elapsed: 3:22:01.\n",
            "  Batch 9,400  of  15,212.    Elapsed: 3:22:53.\n",
            "  Batch 9,440  of  15,212.    Elapsed: 3:23:44.\n",
            "  Batch 9,480  of  15,212.    Elapsed: 3:24:36.\n",
            "  Batch 9,520  of  15,212.    Elapsed: 3:25:28.\n",
            "  Batch 9,560  of  15,212.    Elapsed: 3:26:20.\n",
            "  Batch 9,600  of  15,212.    Elapsed: 3:27:12.\n",
            "  Batch 9,640  of  15,212.    Elapsed: 3:28:03.\n",
            "  Batch 9,680  of  15,212.    Elapsed: 3:28:55.\n",
            "  Batch 9,720  of  15,212.    Elapsed: 3:29:47.\n",
            "  Batch 9,760  of  15,212.    Elapsed: 3:30:39.\n",
            "  Batch 9,800  of  15,212.    Elapsed: 3:31:30.\n",
            "  Batch 9,840  of  15,212.    Elapsed: 3:32:22.\n",
            "  Batch 9,880  of  15,212.    Elapsed: 3:33:14.\n",
            "  Batch 9,920  of  15,212.    Elapsed: 3:34:06.\n",
            "  Batch 9,960  of  15,212.    Elapsed: 3:34:58.\n",
            "  Batch 10,000  of  15,212.    Elapsed: 3:35:49.\n",
            "  Batch 10,040  of  15,212.    Elapsed: 3:36:41.\n",
            "  Batch 10,080  of  15,212.    Elapsed: 3:37:33.\n",
            "  Batch 10,120  of  15,212.    Elapsed: 3:38:25.\n",
            "  Batch 10,160  of  15,212.    Elapsed: 3:39:16.\n",
            "  Batch 10,200  of  15,212.    Elapsed: 3:40:08.\n",
            "  Batch 10,240  of  15,212.    Elapsed: 3:41:00.\n",
            "  Batch 10,280  of  15,212.    Elapsed: 3:41:52.\n",
            "  Batch 10,320  of  15,212.    Elapsed: 3:42:44.\n",
            "  Batch 10,360  of  15,212.    Elapsed: 3:43:35.\n",
            "  Batch 10,400  of  15,212.    Elapsed: 3:44:27.\n",
            "  Batch 10,440  of  15,212.    Elapsed: 3:45:19.\n",
            "  Batch 10,480  of  15,212.    Elapsed: 3:46:11.\n",
            "  Batch 10,520  of  15,212.    Elapsed: 3:47:03.\n",
            "  Batch 10,560  of  15,212.    Elapsed: 3:47:54.\n",
            "  Batch 10,600  of  15,212.    Elapsed: 3:48:46.\n",
            "  Batch 10,640  of  15,212.    Elapsed: 3:49:38.\n",
            "  Batch 10,680  of  15,212.    Elapsed: 3:50:30.\n",
            "  Batch 10,720  of  15,212.    Elapsed: 3:51:21.\n",
            "  Batch 10,760  of  15,212.    Elapsed: 3:52:13.\n",
            "  Batch 10,800  of  15,212.    Elapsed: 3:53:05.\n",
            "  Batch 10,840  of  15,212.    Elapsed: 3:53:57.\n",
            "  Batch 10,880  of  15,212.    Elapsed: 3:54:49.\n",
            "  Batch 10,920  of  15,212.    Elapsed: 3:55:40.\n",
            "  Batch 10,960  of  15,212.    Elapsed: 3:56:32.\n",
            "  Batch 11,000  of  15,212.    Elapsed: 3:57:24.\n",
            "  Batch 11,040  of  15,212.    Elapsed: 3:58:16.\n",
            "  Batch 11,080  of  15,212.    Elapsed: 3:59:08.\n",
            "  Batch 11,120  of  15,212.    Elapsed: 3:59:59.\n",
            "  Batch 11,160  of  15,212.    Elapsed: 4:00:51.\n",
            "  Batch 11,200  of  15,212.    Elapsed: 4:01:43.\n",
            "  Batch 11,240  of  15,212.    Elapsed: 4:02:35.\n",
            "  Batch 11,280  of  15,212.    Elapsed: 4:03:26.\n",
            "  Batch 11,320  of  15,212.    Elapsed: 4:04:18.\n",
            "  Batch 11,360  of  15,212.    Elapsed: 4:05:10.\n",
            "  Batch 11,400  of  15,212.    Elapsed: 4:06:02.\n",
            "  Batch 11,440  of  15,212.    Elapsed: 4:06:54.\n",
            "  Batch 11,480  of  15,212.    Elapsed: 4:07:45.\n",
            "  Batch 11,520  of  15,212.    Elapsed: 4:08:37.\n",
            "  Batch 11,560  of  15,212.    Elapsed: 4:09:29.\n",
            "  Batch 11,600  of  15,212.    Elapsed: 4:10:21.\n",
            "  Batch 11,640  of  15,212.    Elapsed: 4:11:12.\n",
            "  Batch 11,680  of  15,212.    Elapsed: 4:12:04.\n",
            "  Batch 11,720  of  15,212.    Elapsed: 4:12:56.\n",
            "  Batch 11,760  of  15,212.    Elapsed: 4:13:48.\n",
            "  Batch 11,800  of  15,212.    Elapsed: 4:14:40.\n",
            "  Batch 11,840  of  15,212.    Elapsed: 4:15:31.\n",
            "  Batch 11,880  of  15,212.    Elapsed: 4:16:23.\n",
            "  Batch 11,920  of  15,212.    Elapsed: 4:17:15.\n",
            "  Batch 11,960  of  15,212.    Elapsed: 4:18:07.\n",
            "  Batch 12,000  of  15,212.    Elapsed: 4:18:59.\n",
            "  Batch 12,040  of  15,212.    Elapsed: 4:19:50.\n",
            "  Batch 12,080  of  15,212.    Elapsed: 4:20:42.\n",
            "  Batch 12,120  of  15,212.    Elapsed: 4:21:34.\n",
            "  Batch 12,160  of  15,212.    Elapsed: 4:22:26.\n",
            "  Batch 12,200  of  15,212.    Elapsed: 4:23:18.\n",
            "  Batch 12,240  of  15,212.    Elapsed: 4:24:09.\n",
            "  Batch 12,280  of  15,212.    Elapsed: 4:25:01.\n",
            "  Batch 12,320  of  15,212.    Elapsed: 4:25:53.\n",
            "  Batch 12,360  of  15,212.    Elapsed: 4:26:45.\n",
            "  Batch 12,400  of  15,212.    Elapsed: 4:27:37.\n",
            "  Batch 12,440  of  15,212.    Elapsed: 4:28:28.\n",
            "  Batch 12,480  of  15,212.    Elapsed: 4:29:20.\n",
            "  Batch 12,520  of  15,212.    Elapsed: 4:30:12.\n",
            "  Batch 12,560  of  15,212.    Elapsed: 4:31:04.\n",
            "  Batch 12,600  of  15,212.    Elapsed: 4:31:55.\n",
            "  Batch 12,640  of  15,212.    Elapsed: 4:32:47.\n",
            "  Batch 12,680  of  15,212.    Elapsed: 4:33:39.\n",
            "  Batch 12,720  of  15,212.    Elapsed: 4:34:31.\n",
            "  Batch 12,760  of  15,212.    Elapsed: 4:35:23.\n",
            "  Batch 12,800  of  15,212.    Elapsed: 4:36:14.\n",
            "  Batch 12,840  of  15,212.    Elapsed: 4:37:06.\n",
            "  Batch 12,880  of  15,212.    Elapsed: 4:37:58.\n",
            "  Batch 12,920  of  15,212.    Elapsed: 4:38:50.\n",
            "  Batch 12,960  of  15,212.    Elapsed: 4:39:41.\n",
            "  Batch 13,000  of  15,212.    Elapsed: 4:40:33.\n",
            "  Batch 13,040  of  15,212.    Elapsed: 4:41:25.\n",
            "  Batch 13,080  of  15,212.    Elapsed: 4:42:17.\n",
            "  Batch 13,120  of  15,212.    Elapsed: 4:43:08.\n",
            "  Batch 13,160  of  15,212.    Elapsed: 4:44:00.\n",
            "  Batch 13,200  of  15,212.    Elapsed: 4:44:52.\n",
            "  Batch 13,240  of  15,212.    Elapsed: 4:45:44.\n",
            "  Batch 13,280  of  15,212.    Elapsed: 4:46:35.\n",
            "  Batch 13,320  of  15,212.    Elapsed: 4:47:27.\n",
            "  Batch 13,360  of  15,212.    Elapsed: 4:48:19.\n",
            "  Batch 13,400  of  15,212.    Elapsed: 4:49:11.\n",
            "  Batch 13,440  of  15,212.    Elapsed: 4:50:02.\n",
            "  Batch 13,480  of  15,212.    Elapsed: 4:50:54.\n",
            "  Batch 13,520  of  15,212.    Elapsed: 4:51:46.\n",
            "  Batch 13,560  of  15,212.    Elapsed: 4:52:38.\n",
            "  Batch 13,600  of  15,212.    Elapsed: 4:53:30.\n",
            "  Batch 13,640  of  15,212.    Elapsed: 4:54:21.\n",
            "  Batch 13,680  of  15,212.    Elapsed: 4:55:13.\n",
            "  Batch 13,720  of  15,212.    Elapsed: 4:56:05.\n",
            "  Batch 13,760  of  15,212.    Elapsed: 4:56:57.\n",
            "  Batch 13,800  of  15,212.    Elapsed: 4:57:49.\n",
            "  Batch 13,840  of  15,212.    Elapsed: 4:58:40.\n",
            "  Batch 13,880  of  15,212.    Elapsed: 4:59:32.\n",
            "  Batch 13,920  of  15,212.    Elapsed: 5:00:24.\n",
            "  Batch 13,960  of  15,212.    Elapsed: 5:01:16.\n",
            "  Batch 14,000  of  15,212.    Elapsed: 5:02:08.\n",
            "  Batch 14,040  of  15,212.    Elapsed: 5:02:59.\n",
            "  Batch 14,080  of  15,212.    Elapsed: 5:03:51.\n",
            "  Batch 14,120  of  15,212.    Elapsed: 5:04:43.\n",
            "  Batch 14,160  of  15,212.    Elapsed: 5:05:35.\n",
            "  Batch 14,200  of  15,212.    Elapsed: 5:06:27.\n",
            "  Batch 14,240  of  15,212.    Elapsed: 5:07:18.\n",
            "  Batch 14,280  of  15,212.    Elapsed: 5:08:10.\n",
            "  Batch 14,320  of  15,212.    Elapsed: 5:09:02.\n",
            "  Batch 14,360  of  15,212.    Elapsed: 5:09:54.\n",
            "  Batch 14,400  of  15,212.    Elapsed: 5:10:46.\n",
            "  Batch 14,440  of  15,212.    Elapsed: 5:11:37.\n",
            "  Batch 14,480  of  15,212.    Elapsed: 5:12:29.\n",
            "  Batch 14,520  of  15,212.    Elapsed: 5:13:21.\n",
            "  Batch 14,560  of  15,212.    Elapsed: 5:14:13.\n",
            "  Batch 14,600  of  15,212.    Elapsed: 5:15:04.\n",
            "  Batch 14,640  of  15,212.    Elapsed: 5:15:56.\n",
            "  Batch 14,680  of  15,212.    Elapsed: 5:16:48.\n",
            "  Batch 14,720  of  15,212.    Elapsed: 5:17:40.\n",
            "  Batch 14,760  of  15,212.    Elapsed: 5:18:32.\n",
            "  Batch 14,800  of  15,212.    Elapsed: 5:19:23.\n",
            "  Batch 14,840  of  15,212.    Elapsed: 5:20:15.\n",
            "  Batch 14,880  of  15,212.    Elapsed: 5:21:07.\n",
            "  Batch 14,920  of  15,212.    Elapsed: 5:21:59.\n",
            "  Batch 14,960  of  15,212.    Elapsed: 5:22:50.\n",
            "  Batch 15,000  of  15,212.    Elapsed: 5:23:42.\n",
            "  Batch 15,040  of  15,212.    Elapsed: 5:24:34.\n",
            "  Batch 15,080  of  15,212.    Elapsed: 5:25:26.\n",
            "  Batch 15,120  of  15,212.    Elapsed: 5:26:18.\n",
            "  Batch 15,160  of  15,212.    Elapsed: 5:27:09.\n",
            "  Batch 15,200  of  15,212.    Elapsed: 5:28:01.\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 5:28:16\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.72\n",
            "  F1 score: 0.60\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.63      0.37     16039\n",
            "           1       0.93      0.73      0.82    105653\n",
            "\n",
            "    accuracy                           0.72    121692\n",
            "   macro avg       0.60      0.68      0.59    121692\n",
            "weighted avg       0.84      0.72      0.76    121692\n",
            "\n",
            "[[10140  5899]\n",
            " [28495 77158]]\n",
            "Mathews Correlation coefficient score for this epoch is : 0.26\n",
            "Model validation score: f1=0.818 auc=0.947\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Validation Loss: 0.55\n",
            "  Validation took: 1:20:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 6:48:16 (h:mm:ss)\n"
          ]
        }
      ],
      "source": [
        "#using weighted random sampler\n",
        "from sklearn.metrics import classification_report,auc,confusion_matrix,f1_score,precision_recall_curve,plot_precision_recall_curve,matthews_corrcoef\n",
        "import random\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "scaler = GradScaler()\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    xlnet_model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        xlnet_model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        with autocast():\n",
        "            loss, logits = xlnet_model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "            b_labels1=torch.nn.functional.one_hot(b_labels, num_classes=2)\n",
        "            #Calculating weights\n",
        "            #positive=torch.sum(b_labels1, dim=0)\n",
        "           # negative=len(b_labels1)-positive\n",
        "            #negative\n",
        "            #pos_weight  = positive / negative\n",
        "            #criterion.pos_weight = pos_weight\n",
        "            loss1 = loss_fn(logits,b_labels1).to(device)\n",
        "           # print(\"loss:\",loss1)\n",
        "            loss1 = loss1 / gradient_accumulations\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss1.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        scaler.scale(loss1).backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "       # torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm*scaler.get_scale())\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        if ((step + 1) % gradient_accumulations == 0):\n",
        "             scaler.step(optimizer)\n",
        "       # Updates the scale for next iteration.\n",
        "             scaler.update()\n",
        "        # Update the learning rate.\n",
        "             scheduler.step()       \n",
        "             optimizer.zero_grad()\n",
        "            # xlnet_model.zero_grad()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    xlnet_model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    total_f1_score =0\n",
        "    predlist =[]\n",
        "    lbllist =[]\n",
        "    total_logits=[]\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            (loss, logits) = xlnet_model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            #Converting the labels to one hot to sync with same shape as logits\n",
        "            b_labels1=torch.nn.functional.one_hot(b_labels, num_classes=2)\n",
        "            loss1 = loss_fn(logits, b_labels1)\n",
        "       # print(\"loss1:\",loss1)\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss1.item()\n",
        "\n",
        "         #Converting for predictions by applying sigmoid to logits\n",
        "        pred_logits_sigmoid=torch.sigmoid(logits)\n",
        "        y_pred=torch.round(pred_logits_sigmoid)\n",
        "        \n",
        "        # Move logits and labels to CPU\n",
        "        logits_pred = y_pred.detach().cpu().numpy()\n",
        "        label_ids1 = b_labels.to('cpu').numpy()\n",
        "        logits=logits.detach().cpu().numpy()\n",
        "        #For confusion matrix and classification report to work we need same dimensions.\n",
        "        label_ids = b_labels1.to('cpu').numpy()\n",
        "        pred_logits_sigmoid=pred_logits_sigmoid.detach().cpu().numpy()\n",
        "     \n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids1)\n",
        "\n",
        "         #print(predictions)\n",
        "       # predictions=np.argmax(logits_pred, axis=1)\n",
        "        predictions =np.argmax(logits_pred,axis=1)\n",
        "        y_test=np.argmax(label_ids,axis=1)\n",
        "        predlist.extend(predictions)\n",
        "        lbllist.extend(y_test)\n",
        "        #Accumulating the sigmoid positive logits for precision recall curve\n",
        "        total_logits.extend(pred_logits_sigmoid[:,1])\n",
        "        #total_logits.extend(pred_logits_sigmoid)\n",
        "        total_f1_score += f1_score(predlist,lbllist, average = 'macro')\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    #f1 score\n",
        "\n",
        "    avg_f1_score =total_f1_score/len(validation_dataloader)\n",
        "    print(\"  F1 score: {0:.2f}\".format(avg_f1_score))\n",
        "\n",
        "     #classification report\n",
        "    print(classification_report(lbllist, predlist))  \n",
        "\n",
        "    #confusion matrix\n",
        "    cm = confusion_matrix(lbllist,predlist)\n",
        "    # constant for classes\n",
        "    print(cm)\n",
        "    #mcc score\n",
        "    print(\"Mathews Correlation coefficient score for this epoch is :\",round(matthews_corrcoef(lbllist, predlist),2))\n",
        "    #Precision recall curve plot\n",
        "    lr_precision, lr_recall, thresholds = precision_recall_curve(lbllist,total_logits)\n",
        "    lr_f1, lr_auc = f1_score( lbllist,predlist), auc(lr_recall, lr_precision)\n",
        "    print('Model validation score: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
        "    baseline = lbllist.count(1) / len(lbllist)\n",
        "    plt.plot([0, 1], [baseline, baseline], linestyle='--', label='baseline')\n",
        "    plt.plot(lr_recall, lr_precision, marker='.', label='Xlnet model evaluation')\n",
        "    # axis labels\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    # show the legend\n",
        "    plt.legend()\n",
        "    # show the plot\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gslb6tZrsNe_"
      },
      "outputs": [],
      "source": [
        "del xlnet_model\n",
        "del optimizer\n",
        "del scheduler\n",
        "torch.cuda.empty_cache()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}